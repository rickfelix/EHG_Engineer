# EHG_Engineer Environment Configuration Example
# Copy this file to .env and add your actual values

# Supabase Configuration (Required)
NEXT_PUBLIC_SUPABASE_URL=your-supabase-project-url
NEXT_PUBLIC_SUPABASE_ANON_KEY=your-supabase-anon-key

# Supabase Service Role Key (Optional - Server-Side Scripts Only)
# ⚠️  SECURITY WARNING: This key bypasses ALL RLS policies
# - NEVER expose to client-side code
# - NEVER commit to version control
# - Only use in server-side scripts
#
# USE THIS KEY FOR:
# - Reading protected data (handoffs, sub-agent results) in scripts
# - Server-side operations requiring authenticated access
# - Bypassing RLS policies for admin operations
#
# GET YOUR KEY FROM: Supabase Dashboard > Project Settings > API > service_role key
# SCRIPTS: Use createSupabaseServiceClient() from scripts/lib/supabase-connection.js
#
SUPABASE_SERVICE_ROLE_KEY=your-service-role-key-here

# EHG Application Database (Consolidated Database)
# All EHG data has been consolidated into this single database
EHG_SUPABASE_URL=https://dedlbzhpgkmetvhbkyzq.supabase.co
EHG_SUPABASE_ANON_KEY=your-ehg-anon-key
EHG_SUPABASE_SERVICE_ROLE_KEY=your-ehg-service-role-key

# Project Configuration
LEO_PROTOCOL_VERSION=3.1.5.9
PROJECT_NAME=EHG_Engineer

# ==============================================================================
# LLM PROVIDER CONFIGURATION
# ==============================================================================
# At least one API key is required for AI features to function
# Priority: Google Gemini (primary) → OpenAI (secondary) → Ollama (local)

# Google Gemini API Key (Primary — used for all LLM, embedding, and vision tasks)
# Get your key from: https://aistudio.google.com/apikey
GEMINI_API_KEY=your-gemini-api-key

# OpenAI API Key (Secondary fallback — also needed for voice/WebRTC features)
# Get your key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-...

# ==============================================================================
# LOCAL LLM CONFIGURATION (Ollama - Haiku Replacement)
# ==============================================================================
# Use local LLM via Ollama instead of cloud Haiku for cost savings
# Requires Ollama running locally: ollama serve
# Falls back to cloud Haiku automatically if Ollama unavailable

# Enable local LLM for Haiku-class tasks (classification, fast ops)
# USE_LOCAL_LLM=true

# Ollama server URL (default: http://localhost:11434)
# OLLAMA_BASE_URL=http://localhost:11434

# Local model to use (default: qwen3-coder:30b - benchmarked best for Haiku tasks)
# OLLAMA_MODEL=qwen3-coder:30b

# Disable automatic fallback to cloud Haiku (default: true = fallback enabled)
# OLLAMA_FALLBACK_ENABLED=true

# Request timeout in milliseconds (default: 30000)
# OLLAMA_TIMEOUT_MS=30000

# Vision QA Configuration (Optional)
AI_PROVIDER=google              # 'google', 'openai', or 'anthropic' (default: google)
AI_MODEL=auto                   # 'auto' for automatic model selection (recommended)

# Optional: Set specific model if you don't want auto-selection
# AI_MODEL=gemini-3.1-pro-preview    # Most accurate (default for validation/generation)
# AI_MODEL=gemini-3-flash-preview    # Balanced performance/cost (default for fast ops)
# AI_MODEL=gpt-5.2                   # OpenAI fallback
# AI_MODEL=claude-sonnet-4-20250514  # Anthropic option

# Optional: Default cost limits for Vision QA
# VISION_QA_DEFAULT_COST_LIMIT=2.00
# VISION_QA_MAX_ITERATIONS=30
# VISION_QA_CONSENSUS_RUNS=1

# EVA Periodic Vision Scoring Scheduler (SD-LEO-INFRA-VISION-PERIODIC-SCORER-001)
# Set to 'true' to enable automated periodic vision scoring via eva-master-scheduler.js
# When enabled: start the scheduler with: node scripts/eva-scheduler.js start
# Without this flag, the scheduler silently skips vision scoring even when running.
# VISION_PERIODIC_SCORING_ENABLED=true

# Development Environment (Optional)
NODE_ENV=development
DEBUG=vision-qa:*              # Enable debug logging for Vision QA

# RLS Policy Verification (Security Infrastructure)
# Connection string for rls_auditor role - read-only access to pg_policies
# Format: postgresql://rls_auditor:[PASSWORD]@[HOST]:[PORT]/[DATABASE]
# Security: SELECT-only on pg_policies and information_schema.tables
# Rotation: Every 90 days (documented in database/migrations/create-rls-auditor-role.sql)
# Usage: scripts/verify-rls-policies.js and GitHub Actions workflow
SUPABASE_RLS_AUDITOR_URL=postgresql://rls_auditor:your-rls-auditor-password@your-host.pooler.supabase.com:5432/postgres

# ==============================================================================
# SOVEREIGN ALERT SYSTEM (Industrial Hardening v3.0)
# ==============================================================================
# Push notification service for critical system events
# At least one channel should be configured for production

# Discord Webhook (Recommended for team visibility)
# Create: Discord Server Settings > Integrations > Webhooks > New Webhook
# Copy the webhook URL here
DISCORD_ALERT_WEBHOOK=https://discord.com/api/webhooks/YOUR_WEBHOOK_ID/YOUR_WEBHOOK_TOKEN

# Email Alerts via Resend (Optional - for EMERGENCY severity only)
# Sign up: https://resend.com
# Create API key in dashboard
RESEND_API_KEY=re_...

# Email address to receive emergency alerts
SOVEREIGN_ALERT_EMAIL=chairman@yourdomain.com

# Telegram Bot notifications (SD-MAN-INFRA-TELEGRAM-ADAPTER-VISION-001)
# 1. Message @BotFather on Telegram, send /newbot, follow prompts → get token
# 2. Add bot to your channel/chat, then get chat ID via: https://api.telegram.org/bot<TOKEN>/getUpdates
TELEGRAM_BOT_TOKEN=
TELEGRAM_CHAT_ID=

# EVA Idea Processing Pipeline (Optional)
# Todoist API Token: https://todoist.com/app/settings/integrations/developer
TODOIST_API_TOKEN=your-todoist-api-token

# Google OAuth (for YouTube integration)
# Create credentials: https://console.cloud.google.com/apis/credentials
GOOGLE_CLIENT_ID=your-google-client-id.apps.googleusercontent.com
GOOGLE_CLIENT_SECRET=your-google-client-secret