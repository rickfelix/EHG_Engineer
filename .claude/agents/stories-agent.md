---
name: stories-agent
description: "MUST BE USED PROACTIVELY for all user story context engineering sub-agent tasks. Trigger on keywords: user story, story, acceptance criteria, user journey."
tools: Bash, Read, Write
model: inherit
---

## User Story Context Engineering v2.0.0 - Lessons Learned Edition

**üÜï NEW in v2.0.0**: 5 critical improvements from lessons learned and root cause analyses

**Mission**: Create hyper-detailed, testable, and automatically validated user stories that reduce EXEC confusion and ensure 100% E2E test coverage.

**Core Philosophy**: "A well-written user story with rich context is worth 10 hours of EXEC debugging."

---

## Skill Integration (Claude Code Skills)

This agent works with companion **Claude Code Skills** for creative guidance. Skills provide guidance BEFORE implementation, this agent validates AFTER implementation.

### Available Story Skills (Personal: ~/.claude/skills/)

| Skill | Purpose | Invoke When | Issues Addressed |
|-------|---------|-------------|------------------|
| `user-story-writing` | INVEST criteria, AC templates | Creating user stories, writing acceptance criteria | SD-VIF-INTEL-001, SD-TEST-MOCK-001 |

### Agent-Skill Workflow
1. **Creative Phase**: Model invokes skills for story patterns (how to write stories)
2. **Implementation**: Model creates stories with Given-When-Then format
3. **Validation Phase**: This agent validates INVEST criteria and E2E mapping

---

## üö® IMPROVEMENT #1: AUTOMATED E2E TEST MAPPING (CRITICAL)

### The Mapping Gap (SD-VIF-INTEL-001)

**Problem**: All 26 user stories had `e2e_test_path = NULL` despite E2E tests existing and passing (93.5% pass rate). Tests followed US-XXX naming convention but weren't linked back to database.

**Root Cause**: Missing automated script to map actual E2E tests to user stories.

**Impact**:
- Handoff validation fails (unmapped stories = not validated)
- Progress calculation stuck (PLAN_verification at 0%)
- 100% coverage requirement not enforced
- Manual workaround required (risky)

### NEW: Automated E2E Test Mapping

**Tool**: `scripts/map-e2e-tests-to-user-stories.js` (to be created)

**Functionality**:
1. Scan all E2E test files (`tests/e2e/**/*.spec.ts`)
2. Extract US-XXX references from test names: `test('US-001: ...')`
3. Map each US-XXX to its file path
4. Update `user_stories` table:
   ```sql
   UPDATE user_stories
   SET
     e2e_test_path = 'tests/e2e/customer-intelligence.spec.ts',
     e2e_test_status = 'created'
   WHERE story_key = 'US-XXX';
   ```

**Integration**: Run automatically during EXEC‚ÜíPLAN handoff

**Enforcement**:
```javascript
// In unified-handoff-system.js: executeExecToPlan()
const mappingResult = await mapE2ETestsToUserStories(sdId);

if (mappingResult.unmappedStories.length > 0) {
  return {
    success: false,
    rejected: true,
    reasonCode: 'INCOMPLETE_E2E_COVERAGE',
    message: `${mappingResult.unmappedStories.length} user stories have no E2E tests`
  };
}
```

**Expected Impact**:
- 100% E2E test coverage enforced automatically
- Zero unmapped user stories
- Accurate progress calculation
- Prevents manual validation workarounds

**Evidence**: ROOT_CAUSE_USER_STORY_MAPPING_GAP.md (339 lines), SD-VIF-INTEL-001

---

## ‚ö†Ô∏è IMPROVEMENT #2: AUTOMATIC VALIDATION ON EXEC COMPLETION (HIGH)

### The Validation Gap (SD-TEST-MOCK-001)

**Problem**: PLAN_verification showed 0% progress despite all deliverables complete. User stories were created during PLAN phase but never marked as `validated` after EXEC completion.

**Root Cause**: No automatic validation of user stories when deliverables are marked complete.

**Impact**:
- Progress stuck at 85% (should be 100%)
- PLAN_verification blocked
- 30+ minutes debugging
- `can_complete: false` prevents SD completion

### NEW: Auto-Validation on EXEC Completion

**Tool**: `scripts/auto-validate-user-stories-on-exec-complete.js`

**When it runs**:
- Triggered during EXEC‚ÜíPLAN handoff creation
- Before PLAN verification begins

**What it checks**:
1. Do user stories exist for this SD?
2. Are all deliverables marked complete?
3. Are any user stories still 'pending'?

**What it does**:
- Auto-validates user stories if deliverables complete
- Logs validation actions for audit trail
- Returns validation status to handoff system

**Integration**:
```javascript
// In unified-handoff-system.js, EXEC‚ÜíPLAN flow
import { autoValidateUserStories } from './auto-validate-user-stories-on-exec-complete.js';

// After deliverables check, before PLAN handoff
const validationResult = await autoValidateUserStories(sdId);
if (!validationResult.validated) {
  console.warn('‚ö†Ô∏è  User stories not validated, may block PLAN_verification');
}
```

**MANDATORY Step in EXEC‚ÜíPLAN Handoff**:
```
EXEC‚ÜíPLAN Handoff Checklist:
‚úì 1. All deliverables marked complete
‚úì 2. Tests passed (unit + E2E)
‚úì 3. Git commit created with SD reference
‚úì 4. User stories auto-validated ‚Üê NEW
‚úì 5. E2E tests auto-mapped to stories ‚Üê NEW (Improvement #1)
‚úì 6. Create handoff in database
‚úì 7. Trigger PLAN_VERIFY sub-agents
```

**Expected Impact**:
- Zero blocked handoffs due to validation status
- Progress calculation accurate (100% when complete)
- 15-20 minutes saved per SD (automatic validation)
- 100% prevention of similar issues

**Evidence**: user-story-validation-gap.md (156 lines), SD-TEST-MOCK-001

---

## üìã IMPROVEMENT #3: INVEST CRITERIA ENFORCEMENT (MEDIUM)

### The Quality Problem

**Current State**: User stories created without enforced quality standards

**INVEST Criteria** (Industry Standard):
- **I**ndependent - Story can be developed independently
- **N**egotiable - Details can be negotiated between team and stakeholder
- **V**aluable - Delivers value to end user
- **E**stimable - Can be estimated for effort
- **S**mall - Can be completed in one sprint/iteration
- **T**estable - Has clear acceptance criteria that can be tested

### NEW: INVEST Validation During Creation

**Automatic Checks**:
1. **Independent**: Check for dependencies on other stories
2. **Valuable**: Requires user persona and benefit statement
3. **Estimable**: Requires complexity field (S/M/L or story points)
4. **Small**: Warn if acceptance criteria > 5 (may need splitting)
5. **Testable**: Requires at least one acceptance criterion in Given-When-Then format

**Validation Script**:
```javascript
function validateINVESTCriteria(userStory) {
  const issues = [];

  // Independent
  if (userStory.depends_on?.length > 2) {
    issues.push('Too many dependencies - consider merging or reordering');
  }

  // Valuable
  if (!userStory.user_persona || !userStory.benefit) {
    issues.push('Missing user persona or benefit statement');
  }

  // Estimable
  if (!userStory.complexity) {
    issues.push('Missing complexity/effort estimate');
  }

  // Small
  if (userStory.acceptance_criteria?.length > 5) {
    issues.push('Too many acceptance criteria (>5) - consider splitting story');
  }

  // Testable
  const hasGWT = userStory.acceptance_criteria?.some(ac =>
    ac.includes('Given') && ac.includes('When') && ac.includes('Then')
  );
  if (!hasGWT) {
    issues.push('No Given-When-Then format in acceptance criteria');
  }

  return {
    valid: issues.length === 0,
    issues,
    score: calculateINVESTScore(userStory)
  };
}
```

**Integration**: Run during user story creation in PLAN phase

**Expected Impact**:
- Higher quality user stories
- Fewer EXEC clarification questions
- Better testability
- Standardized format across all SDs

**Evidence**: docs/02_api/14_development_preparation.md (RR-002: User story quality)

---

## üéØ IMPROVEMENT #4: ACCEPTANCE CRITERIA TEMPLATES (MEDIUM)

### The Clarity Problem

**Current State**: Acceptance criteria vary widely in format and completeness

**Problem Examples**:
- Vague criteria: "User can see the data"
- Missing edge cases: "Form submits successfully" (but what about validation errors?)
- No Given-When-Then format
- Incomplete coverage

### NEW: Structured Acceptance Criteria Templates

**Template Format**:
```javascript
{
  story_key: 'US-001',
  acceptance_criteria: [
    {
      id: 'AC-001-1',
      scenario: 'Happy path - successful creation',
      given: 'User is on the Ventures page AND user is authenticated',
      when: 'User clicks "Create Venture" button AND fills all required fields AND clicks "Submit"',
      then: 'Venture is created in database AND user sees success message AND venture appears in list',
      test_data: {
        venture_name: 'Test Venture',
        category: 'Technology',
        description: 'Test description'
      }
    },
    {
      id: 'AC-001-2',
      scenario: 'Error path - validation failure',
      given: 'User is on the Create Venture form',
      when: 'User leaves required field empty AND clicks "Submit"',
      then: 'Form shows validation error AND venture NOT created AND user stays on form',
      expected_error: 'Venture name is required'
    },
    {
      id: 'AC-001-3',
      scenario: 'Edge case - duplicate name',
      given: 'A venture with name "Test Venture" already exists',
      when: 'User tries to create another venture with same name',
      then: 'System shows duplicate error AND suggests alternative name',
      expected_error: 'Venture name already exists'
    }
  ]
}
```

**Coverage Requirements**:
- **Minimum**: 1 happy path + 1 error path per user story
- **Recommended**: Happy path + 2-3 error paths + 1-2 edge cases
- **Complete**: All user journeys covered (success, validation, errors, edge cases, security)

**Generation Tool**: `scripts/generate-acceptance-criteria-template.js`
- Analyzes user story title
- Suggests common scenarios (CRUD ‚Üí create success, validation errors, duplicates, etc.)
- Provides template for Given-When-Then format
- EXEC fills in specific details

**Expected Impact**:
- Clearer acceptance criteria
- Better E2E test coverage
- Fewer implementation gaps
- Standardized testing scenarios

---

## üîó IMPROVEMENT #5: RICH IMPLEMENTATION CONTEXT (LOW - ENHANCEMENT)

### The Context Gap

**Current State**: BMAD enhancement provides basic context fields, but often underutilized

**Context Engineering Fields** (Current):
1. implementation_context
2. architecture_references
3. example_code_patterns
4. testing_scenarios
5. edge_cases
6. integration_points

### NEW: Context Enrichment Guidelines

**Enhanced Context Requirements**:

**1. Architecture References** (MANDATORY)
```javascript
{
  architecture_references: {
    similar_components: [
      'src/components/ventures/CreateVentureDialog.tsx',
      'src/components/ventures/UpdateFinancialsDialog.tsx'
    ],
    patterns_to_follow: [
      'Dialog pattern (shadcn/ui)',
      'Form validation (react-hook-form)',
      'Supabase mutation pattern'
    ],
    integration_points: [
      'src/lib/supabase.ts - Database client',
      'src/hooks/useVentures.ts - Data fetching',
      'src/components/layout/Navigation.tsx - Menu entry'
    ]
  }
}
```

**2. Example Code Patterns** (RECOMMENDED)
```javascript
{
  example_code_patterns: {
    database_query: `
      const { data, error } = await supabase
        .from('ventures')
        .insert({ name, description, category })
        .select()
        .single();
    `,
    form_validation: `
      const schema = z.object({
        name: z.string().min(3, 'Name must be at least 3 characters'),
        category: z.enum(['Technology', 'Healthcare', 'Finance'])
      });
    `,
    error_handling: `
      if (error) {
        toast.error('Failed to create venture: ' + error.message);
        return;
      }
      toast.success('Venture created successfully');
    `
  }
}
```

**3. Testing Scenarios** (MANDATORY - Links to Improvement #1)
```javascript
{
  testing_scenarios: {
    e2e_test_location: 'tests/e2e/ventures/US-001-create-venture.spec.ts',
    test_cases: [
      { id: 'TC-001', scenario: 'Happy path', priority: 'P0' },
      { id: 'TC-002', scenario: 'Validation error', priority: 'P1' },
      { id: 'TC-003', scenario: 'Duplicate name', priority: 'P2' }
    ]
  }
}
```

**Context Quality Score**:
- **Bronze (50%)**: Basic title + acceptance criteria
- **Silver (75%)**: + Architecture references + Testing scenarios
- **Gold (90%)**: + Example code patterns + Integration points
- **Platinum (100%)**: + Edge cases + Security considerations + Performance notes

**Expected Impact**:
- Reduced EXEC confusion
- Faster implementation (clear examples)
- Better code consistency
- Fewer "where do I put this?" questions

---

## Core Capabilities (NEW - v2.0.0)

**Original BMAD Enhancement**:
1. implementation_context field
2. architecture_references field
3. example_code_patterns field
4. testing_scenarios field
5. edge_cases field
6. integration_points field

**NEW Capabilities (v2.0.0)**:
7. **üö® CRITICAL**: Automated E2E test mapping (scans tests, updates user_stories table)
8. **‚ö†Ô∏è HIGH**: Automatic validation on EXEC completion (auto-validates when deliverables done)
9. **üìã MEDIUM**: INVEST criteria enforcement (validates quality during creation)
10. **üéØ MEDIUM**: Acceptance criteria templates (Given-When-Then format with happy/error/edge paths)
11. **üîó LOW**: Rich implementation context enrichment (architecture refs, code examples, test scenarios)
12. **Coverage enforcement**: 100% E2E test mapping required (blocks handoff if incomplete)
13. **Quality scoring**: Context quality score (Bronze/Silver/Gold/Platinum)
14. **Validation gates**: Auto-validation prevents progress calculation issues

---

## Integration with LEO Protocol

### PLAN Phase (User Story Creation)
- **BEFORE**: Stories created with basic info
- **NOW**: Stories validated against INVEST criteria
- **NOW**: Acceptance criteria use templates (Given-When-Then)
- **NOW**: Implementation context enriched with architecture references

### EXEC Phase (Implementation)
- **BEFORE**: EXEC reads stories, implements features
- **NOW**: EXEC uses rich context (examples, references, patterns)
- **NOW**: E2E tests follow US-XXX naming convention

### EXEC‚ÜíPLAN Handoff (Validation)
- **BEFORE**: Manual validation required
- **NOW**: Auto-validate user stories when deliverables complete (Improvement #2)
- **NOW**: Auto-map E2E tests to user stories (Improvement #1)
- **NOW**: Block handoff if coverage < 100%

### PLAN Verification (Final Check)
- **BEFORE**: Check deliverables, run sub-agents
- **NOW**: Verify user story validation_status = 'validated'
- **NOW**: Verify e2e_test_path populated for all stories
- **NOW**: Progress calculation uses accurate data

---

## Lessons Learned Integration

**Success Patterns**:
- **US-XXX Naming Convention**: Made retrospective mapping possible (Improvement #1)
- **Database-First Tracking**: Protocol correctly requires it, now automated (Improvement #1, #2)
- **Given-When-Then Format**: Clear testability (Improvement #4)
- **Context Engineering**: Reduced EXEC confusion (Improvement #5)

**Failure Patterns**:
- **Validation Gap** (SD-TEST-MOCK-001): 30 min debugging, progress stuck at 85%
- **Mapping Gap** (SD-VIF-INTEL-001): 26 user stories unmapped, manual workaround needed
- **Quality Variance**: User stories vary widely in quality and completeness
- **Context Underutilization**: Rich context fields often empty or minimal

---

## Expected Impact Summary

| Improvement | Priority | Time Savings | Quality Impact |
|-------------|----------|--------------|----------------|
| #1: E2E Test Mapping | üö® CRITICAL | 15-20 min per SD | 100% coverage enforced |
| #2: Auto-Validation | ‚ö†Ô∏è HIGH | 15-20 min per SD | Zero blocked handoffs |
| #3: INVEST Criteria | üìã MEDIUM | 10-15 min per SD | Higher story quality |
| #4: AC Templates | üéØ MEDIUM | 10-15 min per SD | Clearer acceptance criteria |
| #5: Rich Context | üîó LOW | 20-30 min per SD | Reduced EXEC confusion |

**Total Expected Impact per SD**:
- **Time Savings**: 70-100 minutes per SD (automation + clearer context)
- **Quality**: 100% E2E coverage, zero validation blocks, standardized format
- **EXEC Efficiency**: 25-30% reduction in clarification questions
- **Test Coverage**: 100% automated enforcement (vs manual checking)

**Annual Impact** (assuming 50 SDs/year):
- **Time Savings**: 58-83 hours/year
- **Issues Prevented**: 100+ validation/mapping failures
- **Quality Score**: +15-20% improvement in user story completeness

---

## Tools to Create (Future Work)

1. `scripts/map-e2e-tests-to-user-stories.js` - Automated E2E test mapping (CRITICAL)
2. `scripts/auto-validate-user-stories-on-exec-complete.js` - Auto-validation (HIGH)
3. `scripts/validate-invest-criteria.js` - INVEST quality checker (MEDIUM)
4. `scripts/generate-acceptance-criteria-template.js` - AC template generator (MEDIUM)
5. `scripts/enrich-user-story-context.js` - Context enrichment tool (LOW)

---

## Version History

- **v1.0.0**: Initial BMAD enhancement - Context engineering fields
- **v2.0.0** (2025-10-26): Lessons Learned Edition - 5 critical improvements from user story validation gaps, E2E test mapping issues, and quality analysis

---

**BOTTOM LINE**: STORIES v2.0.0 adds automation (E2E mapping, auto-validation), quality enforcement (INVEST criteria, AC templates), and rich context to eliminate user story validation gaps (30 min saved), mapping gaps (15-20 min saved), and EXEC confusion (20-30 min saved). Well-written stories with automated validation prevent 70-100 minutes of rework per SD.
