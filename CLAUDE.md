# CLAUDE.md - LEO Protocol Workflow Guide for AI Agents

## ‚ö†Ô∏è DO NOT EDIT THIS FILE DIRECTLY

**This file is AUTO-GENERATED from the database.**

## To Make Changes:
1. **For dynamic content** (agents, sub-agents, triggers): Update database tables directly
2. **For static sections** (guides, examples, instructions): Add/update in `leo_protocol_sections` table
3. **Regenerate file**: Run `node scripts/generate-claude-md-from-db.js`

**Any direct edits to this file will be lost on next regeneration!**

See documentation for table structure: `database/schema/007_leo_protocol_schema_fixed.sql`


## Session Prologue (Short)

1. **Follow LEAD‚ÜíPLAN‚ÜíEXEC** - Target ‚â•85% gate pass rate
2. **Use sub-agents** - Architect, QA, Reviewer - summarize outputs
3. **Database-first** - No markdown files as source of truth
4. **Small PRs** - Target ‚â§100 lines, max 400 with justification
5. **7-element handoffs** - Required for all phase transitions
6. **Priority-first** - Use `npm run prio:top3` to justify work

*For copy-paste version: see `templates/session-prologue.md` (generate via `npm run session:prologue`)*


## üèóÔ∏è Application Architecture - CRITICAL CONTEXT

### Two Distinct Applications:
1. **EHG_Engineer** (Management Dashboard) - WHERE YOU ARE NOW
   - **Path**: `/mnt/c/_EHG/EHG_Engineer/`
   - **Purpose**: LEO Protocol dashboard for managing Strategic Directives & PRDs
   - **Database**: dedlbzhpgkmetvhbkyzq (Supabase)
   - **GitHub**: https://github.com/rickfelix/EHG_Engineer.git
   - **Port**: 3000-3001
   - **Role**: MANAGEMENT TOOL ONLY - no customer features here!

2. **EHG** (Business Application) - IMPLEMENTATION TARGET
   - **Path**: `/mnt/c/_EHG/ehg/`
   - **Purpose**: The actual customer-facing business application
   - **Database**: liapbndqlqxdcgpwntbv (Supabase)
   - **GitHub**: https://github.com/rickfelix/ehg.git
   - **Built with**: Vite + React + Shadcn + TypeScript
   - **Role**: WHERE ALL FEATURES GET IMPLEMENTED

### ‚ö†Ô∏è CRITICAL: During EXEC Phase Implementation
1. **Read PRD** from EHG_Engineer database
2. **Navigate** to `/mnt/c/_EHG/ehg/` for implementation
3. **Make code changes** in EHG application (NOT in EHG_Engineer!)
4. **Push changes** to EHG's GitHub repo: `rickfelix/ehg.git`
5. **Track progress** in EHG_Engineer dashboard

### üîÑ Workflow Relationship
```
EHG_Engineer (Management)          EHG App (Implementation)
‚îú‚îÄ‚îÄ Strategic Directives     ‚Üí     Features implemented here
‚îú‚îÄ‚îÄ PRDs                     ‚Üí     Code changes made here
‚îú‚îÄ‚îÄ Progress Tracking        ‚Üê     Results verified from here
‚îî‚îÄ‚îÄ Dashboard Views          ‚Üê     No changes here!
```


## ‚ö†Ô∏è DYNAMICALLY GENERATED FROM DATABASE
**Last Generated**: 2025-10-11 11:51:03 AM
**Source**: Supabase Database (not files)
**Auto-Update**: Run `node scripts/generate-claude-md-from-db.js` anytime

## üü¢ CURRENT LEO PROTOCOL VERSION: vv4.2.0_story_gates

**CRITICAL**: This is the ACTIVE version from database
**ID**: leo-v4-2-0-story-gates
**Status**: ACTIVE
**Title**: LEO Protocol v4.2.0 - Story Gates & Automated Release Control

### üìÖ Protocol Management

**Database-First Architecture**:
- Protocol stored in `leo_protocols` table
- Sub-agents in `leo_sub_agents` table
- Handoffs in `leo_handoff_templates` table
- Single source of truth - no file conflicts

**To update protocol version**:
```sql
-- Only via database operations
UPDATE leo_protocols SET status = 'active' WHERE version = 'new_version';
UPDATE leo_protocols SET status = 'superseded' WHERE version != 'new_version';
```

## Agent Responsibilities


### Implementation Agent (EXEC)
- **Responsibilities**: Implementation based on PRD. **CRITICAL: Implementations happen in /mnt/c/_EHG/ehg/ (EHG app), NOT in EHG_Engineer!** Always cd to target app before coding. **SIMPLICITY IN EXECUTION**: Implement the simplest solution that meets requirements. Avoid over-engineering. Use proven patterns and existing libraries.
- **Planning**: 0%
- **Implementation**: 30%
- **Verification**: 0%
- **Approval**: 0%
- **Total**: 30%

### Strategic Leadership Agent (LEAD)
- **Responsibilities**: Strategic planning, business objectives, final approval. **SIMPLICITY FIRST (PRE-APPROVAL ONLY)**: During initial SD review, challenge complexity and favor simple solutions. Ask "What's the simplest solution?" and "Why not just configure existing tools?" Apply 80/20 rule BEFORE approval. Once SD is approved, LEAD commits to full scope and verifies completion only - scope reduction post-approval is prohibited without explicit human authorization and creating new SDs for deferred work.
- **üõ°Ô∏è HUMAN APPROVAL REQUIRED**: LEAD MUST request human approval before changing SD status/priority. Use standardized over-engineering rubric for evaluations. NEVER override user selections without explicit permission.
- **üìã Over-Engineering Evaluation**: Use `scripts/lead-over-engineering-rubric.js` for standardized assessments. Present findings to human for approval before any changes.
- **üîç MANDATORY BACKLOG REVIEW**: When evaluating any Strategic Directive, LEAD MUST query `sd_backlog_map` table to review all linked backlog items. This is step 3 of the 5-step SD evaluation checklist (see below). Backlog items contain critical scope details not present in SD metadata.
- **üö´ PROHIBITED**: Autonomous SD status changes, user selection overrides, subjective over-engineering calls without rubric, **skipping backlog review before scope decisions**. **ü§ñ MANDATORY SUB-AGENT AUTOMATION**: Before approving any SD as complete, LEAD MUST run automated sub-agent validation. This automatically executes all required sub-agents (Continuous Improvement Coach for retrospectives, DevOps Platform Architect for CI/CD verification) and validates completion requirements. Failure to run this script will result in missed retrospectives and incomplete protocol execution. **‚úÖ APPROVAL CHECKLIST**: LEAD may only approve an SD after: (1) Running sub-agent validation successfully, (2) Verifying output shows "‚úÖ SD READY FOR COMPLETION", (3) Reviewing any warnings, (4) Obtaining human approval for status change.

**5-STEP SD EVALUATION CHECKLIST** (Mandatory for LEAD):
1. Query `strategic_directives_v2` for SD metadata (title, status, priority, progress, scope)
2. Query `product_requirements_v2` for existing PRD (if any)
3. **Query `sd_backlog_map` for linked backlog items** ‚Üê CRITICAL: Contains detailed requirements
4. Search codebase for existing infrastructure (services, components, routes)
5. Identify gaps between backlog requirements and existing code

**Backlog Review Requirements**:
- Review `backlog_title`, `item_description`, `extras.Description_1` for each item
- Assess priority alignment: `priority` field (High/Medium/Low) vs `description_raw` (Must Have/Nice to Have)
- Check completion status: `completion_status` (NOT_STARTED/IN_PROGRESS/COMPLETED)
- Evaluate scope match between backlog items and existing codebase
- Flag scope mismatches for LEAD decision (implement backlog vs use existing code)
- **Planning**: 20%
- **Implementation**: 0%
- **Verification**: 0%
- **Approval**: 15%
- **Total**: 35%

### Technical Planning Agent (PLAN)
- **Responsibilities**: Technical design, PRD creation with comprehensive test plans, pre-automation validation, acceptance testing. **PRAGMATIC ENGINEERING**: Use boring technology that works reliably. Prefer configuration over code, simple solutions over complex architectures. Filter sub-agent recommendations through simplicity lens. **If PRD seems over-engineered during creation, escalate to LEAD for scope reduction BEFORE proceeding to EXEC.**
- **üîç MANDATORY BACKLOG REVIEW**: When creating PRD, PLAN MUST query `sd_backlog_map` table to ensure all backlog items are addressed in the PRD. This is step 3 of the 5-step SD evaluation checklist. Backlog items define the actual requirements to be implemented.
- **üîç Supervisor Mode**: Final "done done" verification with all sub-agents
- **üîç CI/CD VERIFICATION**: After EXEC completion, wait 2-3 minutes for GitHub CI/CD pipelines to complete, then trigger DevOps Platform Architect to verify no pipeline failures exist before final approval.

**5-STEP SD EVALUATION CHECKLIST** (Mandatory for PLAN):
1. Query `strategic_directives_v2` for SD metadata
2. Query `product_requirements_v2` for existing PRD (if creating new PRD)
3. **Query `sd_backlog_map` for linked backlog items** ‚Üê CRITICAL: These define what to build
4. Search codebase for existing infrastructure
5. Map backlog items to PRD sections (objectives, features, acceptance criteria)

**Backlog-to-PRD Mapping**:
- Each backlog item should map to at least one PRD objective
- Backlog `extras.Description_1` provides detailed feature descriptions
- Priority from backlog (`priority` + `description_raw`) informs PRD must-haves
- Existing infrastructure may satisfy some backlog items (document in PRD)
- Gap analysis: What backlog items require new implementation vs configuration?
- **üîç Supervisor Mode**: Final "done done" verification with all sub-agents
- **Planning**: 20%
- **Implementation**: 0%
- **Verification**: 15%
- **Approval**: 0%
- **Total**: 35%

## üö® EXEC Agent Implementation Requirements

### MANDATORY Pre-Implementation Verification
Before writing ANY code, EXEC MUST:

0. **APPLICATION CHECK** ‚ö†Ô∏è CRITICAL FIRST STEP
   - Confirm target app: `/mnt/c/_EHG/ehg/` (NOT EHG_Engineer!)
   - Verify: `cd /mnt/c/_EHG/ehg && pwd` should show `/mnt/c/_EHG/ehg`
   - Check GitHub: `git remote -v` should show `rickfelix/ehg.git`
   - If you're in EHG_Engineer, you're in the WRONG place for implementation!

1. **URL Verification** ‚úÖ
   - Navigate to the EXACT URL specified in the PRD
   - Confirm the page loads and is accessible
   - Take a screenshot for evidence
   - Document: "Verified: [URL] is accessible"

2. **Component Identification** üéØ
   - Identify the exact file path of the target component
   - Confirm component exists at specified location
   - Document: "Target component: [full/path/to/component.tsx]"

3. **Application Context** üìÅ
   - Verify correct application directory
   - Confirm port number matches PRD
   - Document: "Application: [/path/to/app] on port [XXXX]"

4. **Visual Confirmation** üì∏
   - Screenshot current state BEFORE changes
   - Identify exact location for new features
   - Document: "Current state captured, changes will go at [location]"

### Implementation Checklist Template
```markdown
## EXEC Pre-Implementation Checklist
- [ ] URL verified: [exact URL from PRD]
- [ ] Page accessible: [YES/NO]
- [ ] Component identified: [path/to/component]
- [ ] Application path: [/full/path/to/app]
- [ ] Port confirmed: [port number]
- [ ] Screenshot taken: [timestamp]
- [ ] Target location confirmed: [where changes go]
```

### Common Mistakes to AVOID
- ‚ùå Assuming component location based on naming similarity
- ‚ùå Implementing without navigating to the URL first
- ‚ùå Ignoring port numbers in URLs
- ‚ùå Pattern matching without verification
- ‚ùå Starting to code before completing checklist
- ‚ùå Not restarting dev servers after changes
- ‚ùå **CRITICAL**: Creating files for PRDs, handoffs, or documentation


## üîÑ Git Commit Guidelines

### Commit Format (MANDATORY)
All commits MUST follow Conventional Commits format with SD-ID:
```
<type>(<SD-ID>): <subject>

<body>

<footer>
```

### Required Elements
- **Type**: feat|fix|docs|style|refactor|test|chore|perf|ci|revert
- **Scope**: MUST include Strategic Directive ID (e.g., SD-2025-001)
- **Subject**: Imperative mood, no period, max 72 chars
- **Body**: Explain "why" not "what", wrap at 72 chars
- **Footer**: Breaking changes, co-authorship, AI attribution

### Commit Timing (During EXEC Phase)
1. **After completing each checklist item**
2. **Before context switches** (end of session, meetings, breaks)
3. **At logical breakpoints** (feature complete, tests passing)
4. **Frequency**: Min 1/session, Max 10/checklist item

### Branch Strategy by Application
- **EHG_Engineer changes** (dashboard/tooling): Use `eng/` prefix
  - Example: `eng/dashboard-update`, `eng/leo-protocol-v4`
- **EHG app features** (customer features): Use standard prefixes
  - Example: `feature/SD-2025-001-voice-api`, `fix/SD-2025-002-auth-bug`
- **CRITICAL**: Verify you're in the correct directory before branching!
  - `pwd` should show `/mnt/c/_EHG/ehg` for feature implementation
  - `pwd` should show `/mnt/c/_EHG/EHG_Engineer` for dashboard changes
- **Main branch**: NO direct commits during EXEC phase
- **Merges**: Only via approved Pull Requests after LEAD approval

### AI Attribution
When AI generates code, include in footer:
```bash
git commit -m "feat(SD-2025-001): Implement retry logic

ü§ñ Generated with [Claude Code](https://claude.ai/code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

### Quick Reference
- Format: `<type>(<SD-ID>): <subject>`
- Size: <100 lines ideal, <200 max
- Files: 1-3 ideal, 10 max per commit
- Validation: Commits without proper format fail CI

**Full Guidelines**: See `docs/03_protocols_and_standards/leo_git_commit_guidelines_v4.2.0.md`


## PR Size Guidelines

**Philosophy**: Balance AI capability with human review capacity. Modern AI can handle larger changes, but humans still need to review them.

**Three Tiers**:

1. **‚â§100 lines (Sweet Spot)** - No justification needed
   - Simple bug fixes
   - Single feature additions
   - Configuration changes
   - Documentation updates

2. **101-200 lines (Acceptable)** - Brief justification in PR description
   - Multi-component features
   - Refactoring with tests
   - Database migrations with updates
   - Example: "Adds authentication UI (3 components) + tests"

3. **201-400 lines (Requires Strong Justification)** - Detailed rationale required
   - Complex features that cannot be reasonably split
   - Large refactorings with extensive test coverage
   - Third-party integrations with configuration
   - Must explain why splitting would create more risk/complexity
   - Example: "OAuth integration requires provider config, UI flows, session management, and error handling as atomic unit"

**Over 400 lines**: Generally prohibited. Split into multiple PRs unless exceptional circumstances (emergency hotfix, external dependency forcing bundled changes).

**Key Principle**: If you can split it without creating incomplete/broken intermediate states, you should split it.


## üìä Communication & Context

### Context Economy Rules

**Core Principles**:
- **Response Budget**: ‚â§500 tokens default (unless complexity requires more)
- **Summarize > Paste**: Reference paths/links instead of full content
- **Fetch-on-Demand**: Name files first, retrieve only needed parts
- **Running Summaries**: Keep condensed handoff/PR descriptions

### Best Practices

**Efficient Context Usage**:
- **Quote selectively**: Show only relevant lines with context
- **Use file:line references**: `src/component.js:42-58` instead of full file
- **Batch related reads**: Minimize round-trips when exploring
- **Archive verbosity**: Move details to handoffs/database, not conversation

### Examples

| ‚ùå Inefficient | ‚úÖ Efficient |
|----------------|--------------|
| Paste entire 500-line file | Quote lines 42-58 with `...` markers |
| Read file multiple times | Batch read relevant sections once |
| Repeat full error in response | Summarize error + reference line |
| Include all test output | Show failed tests + counts only |

### üîÑ MANDATORY: Server Restart Protocol
After ANY code changes:
1. **Kill the dev server**: `kill [PID]` or Ctrl+C
2. **Restart the server**: `npm run dev` or appropriate command
3. **Wait for ready message**: Confirm server is fully started
4. **Hard refresh browser**: Ctrl+Shift+R / Cmd+Shift+R
5. **Verify changes are live**: Test the new functionality

**WHY**: Dev servers may cache components, especially new files. Hot reload is NOT always reliable.


## Parallel Execution

**When to Use**: Modern AI supports parallel tool execution for independent operations. Use conservatively.

**Safe for Parallel Execution**:
- ‚úÖ Reading multiple independent files for analysis
- ‚úÖ Running multiple independent database queries
- ‚úÖ Executing multiple read-only Git commands (status, log, diff)
- ‚úÖ Multiple WebFetch calls to different URLs
- ‚úÖ Batch file searches (multiple Glob operations)

**NOT Safe for Parallel Execution**:
- ‚ùå Write operations (Edit, Write tools)
- ‚ùå Database mutations (INSERT, UPDATE, DELETE)
- ‚ùå Any operations where order matters
- ‚ùå Operations that depend on each other's results
- ‚ùå Git operations that modify state (commit, push, merge)

**Critical Constraint**: Context sharing between parallel operations is limited. Each operation receives the same initial context but cannot see other parallel operations' results until they all complete.

**Example Use Case**:
```
"Read the following 3 files for analysis:"
- Read src/component.tsx
- Read src/types.ts
- Read tests/component.test.tsx
```

**Anti-Pattern**:
```
"Read file A, then based on what you find, read file B"
(Must be sequential - second read depends on first)
```


## Progress Calculation

```
Total = EXEC: 30% + LEAD: 35% + PLAN: 35% = 100%
```

## üéØ LEAD Agent Operations

### Finding Active Strategic Directives

As the LEAD agent, you have immediate access to strategic directives:

**Quick Command**:
```bash
node scripts/query-active-sds.js
```

**Direct Database Query**:
```javascript
const { data: activeSDs } = await supabase
  .from('strategic_directives_v2')
  .select('*')
  .in('status', ['draft', 'in_progress', 'active', 'pending_approval'])
  .order('priority', { ascending: false })
  .order('created_at', { ascending: false });
```

### LEAD Decision Matrix

| SD Status | LEAD Action | Command/Script |
|-----------|-------------|----------------|
| **Draft** | Review & approve | `node scripts/lead-approve-sdip.js` |
| **Pending Approval** | Final review | `node scripts/conduct-lead-approval-assessment.js` |
| **Active** | Create LEAD‚ÜíPLAN handoff | `node scripts/create-exec-to-plan-handoff.js` |
| **In Progress** | Monitor execution | `node scripts/debug-dashboard-progress.js` |

### Key LEAD Responsibilities

1. **Strategic Direction**
   - Define business objectives
   - Set priorities (CRITICAL: 90+, HIGH: 70-89, MEDIUM: 50-69, LOW: 30-49)
   - Approve strategic directives

2. **Handoff Creation**
   - LEAD‚ÜíPLAN: Strategic objectives to technical planning
   - Must include all 7 mandatory elements
   - Use `node scripts/unified-handoff-system.js`

3. **Progress Monitoring**
   - Review SD progress: `node scripts/test-progress-calculation.js`
   - Check phase completion: `node scripts/complete-lead-phase.js`
   - Final approval: `node scripts/start-lead-approval.js`

### Strategic Directive Lifecycle

```mermaid
graph LR
    Draft --> LEAD_Review
    LEAD_Review --> Active
    Active --> LEAD_PLAN_Handoff
    LEAD_PLAN_Handoff --> In_Progress
    In_Progress --> Pending_Approval
    Pending_Approval --> LEAD_Final_Approval
    LEAD_Final_Approval --> Completed
```


## üìã Directive Submission Review Process

**CRITICAL**: Directive submissions contain essential context not present in SDs. Always review linked submissions before making strategic decisions.

#### Step-by-Step Review Workflow

1. **Query Pending Submissions**:
```javascript
// Find submissions needing LEAD review
const { data: pendingSubmissions } = await supabase
  .from('directive_submissions')
  .select('*')
  .in('status', ['pending_review', 'completed'])
  .is('gate_status->resulting_sd_id', null)
  .order('created_at', { ascending: false });
```

2. **Get Full Submission Context**:
```javascript
// Retrieve submission with linked SD (if exists)
const { data: submission } = await supabase
  .from('directive_submissions')
  .select(`
    *,
    linked_sd:strategic_directives_v2!gate_status->resulting_sd_id(*)
  `)
  .eq('id', submission_id)
  .single();
```

3. **Review Checklist**:
- [ ] **Original Input**: Review chairman_input for true intent
- [ ] **Intent Clarity**: Verify intent_summary captures essence
- [ ] **Visual Context**: Check screenshot_url if provided
- [ ] **Strategic Alignment**: Assess fit with organizational goals
- [ ] **Priority Assessment**: Determine business impact (Critical/High/Medium/Low)
- [ ] **Scope Validation**: Ensure scope is achievable and clear
- [ ] **Duplicate Check**: Verify no existing SDs cover this need
- [ ] **Gate Progression**: Confirm all validation gates passed

4. **Decision Matrix**:

| Submission State | Gate Status | LEAD Action | Command |
|-----------------|-------------|-------------|---------|
| Completed + No SD | Gates passed | Create SD | `node scripts/create-sd-from-submission.js` |
| Completed + SD exists | Linked to SD | Verify & handoff | `node scripts/unified-handoff-system.js` |
| Pending | Gates incomplete | Monitor | `node scripts/check-submission-status.js` |
| Failed validation | Gates failed | Archive/remediate | `node scripts/archive-submission.js` |

5. **Quick Review Command**:
```bash
node scripts/lead-review-submissions.js
```

#### Critical Context Elements

When reviewing submissions, pay special attention to:
- **Chairman Input**: The unfiltered original request
- **Screenshot URL**: Visual context for UI/UX requests
- **SDIP ID**: For tracking through the processing pipeline
- **Processing History**: Number of steps and iterations
- **Metadata**: Additional context from processing

#### Linking Submissions to Strategic Directives

When creating an SD from a submission:
1. Include submission_id in SD metadata
2. Reference key context in SD description
3. Preserve chairman's original intent
4. Map submission scope to SD objectives




## Database Operations - One Table at a Time

### REQUIRED: Database Operations Only

**‚ö†Ô∏è CRITICAL: One Table at a Time**
- When manipulating Supabase tables, **ALWAYS operate on ONE table at a time**
- Batch operations across multiple tables often fail or cause inconsistencies
- Complete each table operation fully before moving to the next table
- Verify success after each table operation before proceeding

**Strategic Directives**:
- ‚úÖ Create in `strategic_directives_v2` table
- ‚úÖ Use `scripts/create-strategic-directive.js` or dashboard
- ‚úÖ ALL SD data must be in database, not files
- ‚úÖ **One SD insertion at a time** - verify before next

**PRDs (Product Requirements)**:
- ‚úÖ Create in `product_requirements_v2` table
- ‚úÖ Use `scripts/add-prd-to-database.js`
- ‚úÖ Link to SD via `strategic_directive_id` foreign key
- ‚úÖ **One PRD insertion at a time** - verify before next

**Retrospectives**:
- ‚úÖ Create in `retrospectives` table
- ‚úÖ Use `scripts/generate-comprehensive-retrospective.js`
- ‚úÖ Trigger: Continuous Improvement Coach sub-agent
- ‚úÖ Link to SD via `sd_id` foreign key
- ‚úÖ **One retrospective at a time** - verify before next

**Handoffs**:
- ‚úÖ Store in handoff tracking tables
- ‚úÖ 7-element structure required
- ‚úÖ Link to SD and phase
- ‚úÖ **One handoff at a time** - verify before next

**Progress & Verification**:
- ‚úÖ Update database fields directly
- ‚úÖ Store verification results in database
- ‚úÖ Track in real-time via dashboard
- ‚úÖ **One record update at a time** - verify before next

## Multi-Application Testing Architecture

## üß™ Multi-Application Testing Architecture

### Critical Context: Two Independent Test Suites

The EHG ecosystem consists of two separate applications with **independent test suites**:

#### 1. EHG_Engineer Application (Management Dashboard)
- **Test Location**: `/mnt/c/_EHG/EHG_Engineer/tests/`
- **Test Framework**: Vitest + Jest
- **Coverage Target**: 50% minimum for management features
- **Test Types**: Unit tests, integration tests for LEO Protocol
- **Run Command**: `npm run test` (from EHG_Engineer directory)
- **Purpose**: Testing SD management, PRD tracking, dashboard functionality

#### 2. EHG Application (Business Application)
- **Test Location**: `/mnt/c/_EHG/ehg/tests/`
- **Test Framework**: Vitest (unit), Playwright (E2E)
- **Coverage Targets**:
  - Unit: 50% minimum
  - E2E: Comprehensive user flow coverage
  - A11y: WCAG 2.1 AA compliance
- **Test Types**:
  - `tests/unit/` - Vitest unit tests
  - `tests/integration/` - Integration tests
  - `tests/e2e/` - Playwright E2E tests
  - `tests/a11y/` - Accessibility tests
  - `tests/security/` - Security tests
  - `tests/performance/` - Performance tests
- **Run Commands**:
  - `npm run test:unit` - Unit tests with coverage
  - `npm run test:integration` - Integration tests
  - `npm run test:e2e` - Playwright E2E tests
  - `npm run test:a11y` - Accessibility tests
- **Purpose**: Testing customer-facing features, business logic, UX

### MANDATORY: Test Context Switching

**Before running ANY tests**, determine target application:

1. **Read SD Description** - Which application is mentioned?
2. **Navigate to Correct Directory**:
   ```bash
   # For EHG_Engineer tests:
   cd /mnt/c/_EHG/EHG_Engineer && npm run test

   # For EHG application tests:
   cd /mnt/c/_EHG/ehg && npm run test:unit
   ```

3. **Verify Test Location**:
   - Wrong: "No tests found" error
   - Right: Tests execute in correct context

### Coverage Reporting Per Application

**Coverage metrics are INDEPENDENT** - report separately:

- **EHG_Engineer Coverage**: Dashboard/management features only
- **EHG Application Coverage**: Customer-facing features only

**DO NOT** combine coverage metrics across applications!

### Common Mistakes to Avoid

‚ùå **Assuming test location based on current directory**
‚úÖ **Always verify target application from SD context**

‚ùå **Running tests in wrong application directory**
‚úÖ **Navigate to correct app before test execution**

‚ùå **Combining coverage metrics across applications**
‚úÖ **Report coverage per application separately**

‚ùå **Claiming "zero test coverage" without checking both apps**
‚úÖ **Verify test suite in correct application first**

### Sub-Agent Context: QA Engineering Director

When QA sub-agent is triggered:
1. Determine target application from SD context
2. Navigate to correct test directory
3. Run appropriate test suite for that application
4. Report coverage for that application only
5. Document test location in handoffs

### Example: SD-QUALITY-001 Learning

**Original Claim**: "362,538 LOC with 6 test files (0.001% coverage)"

**Reality Check**:
- EHG application has 63 test files (not 6!)
- E2E, integration, a11y tests extensive
- **Actual gap**: Unit test coverage for business logic
- Lesson: Always verify test suite in correct application

**Corrected Scope**: "Unit test coverage gap in EHG application business logic"

## LEAD Over-Engineering Evaluation Process

### üõ°Ô∏è LEAD Over-Engineering Evaluation Process

**MANDATORY**: LEAD agents MUST use the standardized rubric before making any SD status/priority changes.

#### Step-by-Step Evaluation Process

1. **Execute Rubric Evaluation**:
   ```bash
   node scripts/lead-over-engineering-rubric.js --sd-id [SD_ID]
   ```

2. **Review 6-Dimension Scores** (1-5 scale each):
   - **Technical Complexity vs Business Value**: Complexity-to-value ratio
   - **Resource Intensity vs Urgency**: Development effort vs business urgency  
   - **Strategic Priority Alignment**: Alignment with Stage 1/EVA/GTM priorities
   - **Market Timing & Opportunity Window**: Market opportunity timing
   - **Implementation & Business Risk**: Risk vs reward assessment
   - **Return on Investment Projection**: Expected ROI evaluation

3. **Check Over-Engineering Thresholds**:
   - Total Score ‚â§15/30 = Over-engineered
   - Complexity ‚â§2 = Problematic
   - Strategic Alignment ‚â§2 = Concerning  
   - Risk Assessment ‚â§2 = Dangerous

4. **Present Findings to Human**:
   ```bash
   node scripts/lead-human-approval-system.js --sd-id [SD_ID] --evaluation [RESULTS]
   ```

5. **Request Explicit Approval**: Show scores, reasoning, and consequences

6. **Execute Only After Approval**: NEVER make autonomous changes

#### Available Scripts for LEAD Agents
- `scripts/lead-over-engineering-rubric.js` - Standardized 6-dimension evaluation
- `scripts/lead-human-approval-system.js` - Human approval workflow
- `scripts/enhanced-priority-rubric.js` - Priority rebalancing tools

#### Prohibited Actions
- ‚ùå Autonomous SD status/priority changes  
- ‚ùå Overriding user selections without permission
- ‚ùå Subjective over-engineering calls without rubric
- ‚ùå Making changes before human approval

## 6-Step SD Evaluation Checklist

## 5-Step SD Evaluation Checklist

**MANDATORY**: All agents (LEAD, PLAN) MUST complete these steps when evaluating a Strategic Directive:

### Step 1: Query SD Metadata
```javascript
const { data: sd } = await supabase
  .from('strategic_directives_v2')
  .select('*')
  .eq('id', 'SD-XXX')
  .single();
```

**Extract**: title, status, priority, progress, current_phase, scope, category, target_application

### Step 2: Check for Existing PRD
```javascript
const { data: prd } = await supabase
  .from('product_requirements_v2')
  .select('*')
  .eq('directive_id', 'SD-XXX');
```

**If exists**: Review PRD objectives, features, acceptance criteria
**If missing**: PRD creation required (PLAN responsibility)

### Step 3: Query Backlog Items ‚úÖ CRITICAL
```javascript
const { data: backlogItems } = await supabase
  .from('sd_backlog_map')
  .select('*')
  .eq('sd_id', 'SD-XXX')
  .order('priority', { ascending: false })
  .order('sequence_no', { ascending: true });
```

**Review for EACH item**:
- `backlog_title`: Short description
- `item_description`: Additional context
- `extras.Description_1`: **Detailed feature description** (MOST IMPORTANT)
- `priority`: High/Medium/Low
- `description_raw`: Must Have/Nice to Have/Future
- `completion_status`: NOT_STARTED/IN_PROGRESS/COMPLETED
- `phase`: Discovery/Planning/Development/Launch
- `stage_number`: Sequence in overall backlog
- `extras.Page Category_1`: Feature category
- `extras.Category`: Business category

**Why Critical**: Backlog items contain the ACTUAL requirements. SD metadata may be generic; backlog items have specifics.

### Step 4: Search Codebase for Existing Infrastructure
```bash
# Search for related services
find . -name "*service*.ts" -o -name "*Service.ts" | grep -i [feature-name]

# Search for UI components
find . -name "*.tsx" -o -name "*.jsx" | grep -i [feature-name]

# Check routing
grep -r "/[route-name]" src/App.tsx src/routes/
```

**Document**:
- Existing files (paths, line counts, capabilities)
- Mock data vs real data
- Database tables expected vs existing
- UI components complete vs partial

### Step 5: Gap Analysis
**Compare**: Backlog requirements vs Existing infrastructure

**Identify**:
1. ‚úÖ **Satisfied**: Backlog items fully met by existing code
2. ‚ö†Ô∏è **Partial**: Existing code needs integration/configuration
3. ‚ùå **Missing**: Backlog items require new implementation
4. üîÑ **Mismatch**: Existing code does different things than backlog requests

**Output**: Scope recommendation
- **Option A**: Implement all backlog items (high effort)
- **Option B**: Connect/configure existing infrastructure (low effort)
- **Option C**: Hybrid (phase existing code, defer new features)

### Step 6: Execute QA Smoke Tests ‚úÖ NEW

**Evidence**: SD-EXPORT-001 - "5-step checklist comprehensive but missing testing"

**CRITICAL**: Before approving ANY SD as complete, verify tests have been executed.

```javascript
// For LEAD or PLAN agents evaluating SD completion
const { data: testResults } = await supabase
  .from('sub_agent_execution_results')
  .select('*')
  .eq('sd_id', sd_id)
  .eq('sub_agent_code', 'TESTING')
  .order('created_at', { ascending: false })
  .limit(1)
  .single();

if (!testResults || testResults.verdict !== 'PASS') {
  throw new Error('Cannot approve SD without passing test evidence');
}
```

**Checklist**:
- [ ] Unit tests executed and passed
- [ ] E2E tests executed and passed
- [ ] Test evidence documented (screenshots, reports)
- [ ] Coverage meets minimum threshold (50% unit, 100% user story coverage for E2E)
- [ ] QA Engineering Director sub-agent executed
- [ ] Test results stored in database

**Why Added**:
- **SD-EXPORT-001**: "Done-done definition ignored" - tests existed but weren't run before approval
- **Impact**: Prevents 30-minute gap between claiming complete and discovering test failures

---

## Common Mistakes to Avoid

‚ùå **Skipping backlog review**: Leads to scope misunderstandings
‚ùå **Assuming SD description = full scope**: Backlog has the details
‚ùå **Not checking completion_status**: May duplicate completed work
‚ùå **Ignoring priority conflicts**: `priority: High` but `description_raw: Nice to Have`
‚ùå **Missing extras.Description_1**: This field has the most detailed requirements
‚ùå **Approving SD without test evidence**: Step 6 MANDATORY (added based on retrospectives)

## Example: SD-041 Analysis

**Step 1**: SD-041 = "Knowledge Base: Consolidated", status: active, priority: high, 30% complete
**Step 2**: No PRD exists
**Step 3**: 2 backlog items found:
  - Item #62: "Define Cloning Process for Venture Ideation" (Low priority, Must Have)
  - Item #290: "AI-Powered Knowledge Base & Help Docs" (High priority, Nice to Have)
**Step 4**: Found 698-line knowledgeManagementService.ts + 1,300 lines UI (mock data)
**Step 5**: Gap = Backlog requests competitive intelligence + AI docs; existing code does pattern recognition
**Step 6**: No test evidence found ‚Üí BLOCKS approval

**Result**: Scope mismatch identified + Testing gap ‚Üí LEAD decision required before proceeding

## Enhanced QA Engineering Director v2.0 - Testing-First Edition

## Enhanced QA Engineering Director v2.0 - Testing-First Edition

### Overview
**Mission-Critical Testing Automation** - Comprehensive E2E validation of all implementations against user stories.

**Philosophy**: **Do it right, not fast.** E2E testing is MANDATORY, not optional.

**Time Investment**: 30-60 minutes per SD for comprehensive E2E testing (saves 4-6 hours in rework)

### Core Capabilities

1. **Professional Test Case Generation from User Stories** (NEW)
   - Queries `user_stories` table for SD requirements
   - Creates comprehensive Given-When-Then test scenarios
   - Maps each user story to ‚â•1 E2E test case
   - Generates Playwright test suites with proper selectors
   - Documents test coverage percentage

2. **Pre-test Build Validation** (saves 2-3 hours)
   - Validates build before testing
   - Parses build errors and provides fix recommendations
   - Blocks test execution if build fails

3. **Database Migration Verification** (prevents 1-2 hours debugging)
   - Checks if migrations are applied before testing
   - Identifies pending migrations by SD ID
   - Provides automated and manual execution options

4. **Component Integration Checking** (saves 30-60 minutes)
   - Verifies components are actually imported and used
   - Detects "built but not integrated" gaps
   - Prevents unused code accumulation

5. **Mandatory E2E Test Tier** (**NO LONGER CONDITIONAL**)
   - Tier 1 (Smoke): Basic sanity checks (3-5 tests, <60s) - NOT sufficient alone
   - **Tier 2 (E2E via Playwright): MANDATORY** (10-30 tests, <10min) - **REQUIRED FOR APPROVAL**
   - Tier 3 (Manual): Only for complex edge cases (rare)
   - **Standard**: Smoke tests check "does it load?", E2E tests prove "does it work?"

6. **Playwright E2E Test Execution** (MANDATORY)
   - Automated browser testing for all user journeys
   - Screenshot capture for visual evidence
   - Video recording on failures for debugging
   - HTML reports with pass/fail status
   - Test evidence stored in `tests/e2e/evidence/SD-XXX/`

7. **Test Infrastructure Discovery** (saves 30-60 minutes)
   - Discovers existing auth helpers, test fixtures
   - Recommends reuse of authenticateUser() and other helpers
   - Prevents recreation of existing infrastructure

8. **Cross-SD Dependency Detection** (saves 10-15 minutes)
   - Identifies conflicts with in-progress SDs
   - Analyzes import statements for dependencies
   - Provides risk assessment and recommendations

9. **Automated Migration Execution** (saves 5-8 minutes)
   - Uses supabase link + supabase db push
   - Auto-applies pending migrations
   - Validates migration files before execution

10. **Testing Learnings for Continuous Improvement** (NEW)
    - Captures testing effectiveness after each SD
    - Documents what worked, what didn't with Playwright
    - Identifies test infrastructure improvements needed
    - Feeds retrospective for sub-agent enhancement
    - Tracks evolution: v2.0 ‚Üí v2.5 (automated generation) ‚Üí v3.0 (AI-assisted + self-healing)

### 5-Phase Execution Workflow (UPDATED)

**Phase 1: Pre-flight Checks**
- Build validation
- Database migration verification
- Cross-SD dependency check
- Component integration check (if UI SD)

**Phase 2: Professional Test Case Generation** (**NEW - MANDATORY**)
- Query `user_stories` table for SD
- For each user story, create Given-When-Then test scenarios
- Generate Playwright test files with proper test IDs
- Define test data requirements and fixtures
- Map user stories to test coverage (must be 100%)

**Phase 3: E2E Test Execution** (**MANDATORY, NOT CONDITIONAL**)
- Execute Playwright E2E tests (ALL user stories)
- Capture screenshots on success
- Capture videos on failures
- Generate HTML test reports
- Store evidence in database

**Phase 4: Evidence Collection**
- Screenshots proving features work
- Test execution logs
- Playwright HTML reports
- Coverage metrics (user story validation %)
- Test infrastructure notes

**Phase 5: Verdict & Testing Learnings**
- Aggregate all results
- Calculate final verdict: PASS / CONDITIONAL_PASS / BLOCKED
- Generate recommendations for PLAN
- **Document testing learnings for retrospective** (NEW):
  - What Playwright features were most effective?
  - What test patterns emerged?
  - What infrastructure improvements needed?
  - How can test generation be automated next time?
- Store in `sub_agent_execution_results` table with testing_learnings field

### Activation

**Automatic Triggers**:
- "coverage" keyword in any context
- "protected route" keyword
- "build error" keyword
- "test infrastructure" keyword
- "testing evidence" keyword
- "user stories" keyword (NEW)
- "playwright" keyword (NEW)

**Manual Execution**:
```bash
# Standard E2E execution (MANDATORY)
node scripts/qa-engineering-director-enhanced.js <SD-ID> --full-e2e

# Options (use sparingly)
--skip-build             # Skip build validation
--skip-migrations        # Skip migration checks
--no-auto-migrations     # Don't auto-execute migrations
```

### Success Criteria (UPDATED)

**PASS Verdict** requires:
- ‚úÖ Build successful (or skipped)
- ‚úÖ All migrations applied
- ‚úÖ **ALL E2E tests pass (100% user stories validated)** (**MANDATORY**)
- ‚úÖ Test evidence collected (Playwright report, screenshots)
- ‚úÖ No critical integration gaps

**CONDITIONAL_PASS** if:
- ‚ö†Ô∏è E2E tests pass but minor issues in edge cases
- ‚ö†Ô∏è Non-critical integration warnings
- ‚ö†Ô∏è Test infrastructure improvements identified but not blocking

**BLOCKED** if:
- ‚ùå Build fails
- ‚ùå Pending migrations not applied
- ‚ùå **ANY E2E test failures** (user stories not validated)
- ‚ùå Critical dependency conflicts

### Database Integration (UPDATED)

Results stored in `sub_agent_execution_results` table:
- Overall verdict and confidence score
- Phase results (pre-flight, test generation, execution, evidence, learnings)
- Recommendations for EXEC agent
- **Testing learnings** (for continuous improvement) (**NEW**)
- Test evidence URLs (Playwright reports, screenshots, videos)
- User story coverage percentage (must be 100%)

### Continuous Improvement Framework (NEW)

**Goal**: Perfect the testing sub-agent through iterative learning

**Mechanisms**:
1. **Retrospective capture**: Testing learnings after each SD execution
2. **Script enhancement**: Improve `qa-engineering-director-enhanced.js` based on patterns
3. **Infrastructure building**: Add reusable Playwright helpers, fixtures, page objects
4. **Best practices documentation**: Capture effective test patterns in wiki
5. **Tooling improvements**: Add new Playwright reporters, visual regression, trace viewer usage

**Feedback Loop**:
```
SD Execution ‚Üí Testing Challenges ‚Üí Retrospective Captured ‚Üí
Script Enhanced ‚Üí Better Testing Next SD ‚Üí Repeat
```

**Expected Evolution**:
- **v2.0** (current): Manual test case generation, Playwright execution, evidence collection
- **v2.5** (next): Automated test generation from user stories, smart selector strategies
- **v3.0** (future): AI-assisted test case creation, self-healing tests, visual regression automation

### Integration with Product Requirements Expert

**Workflow**:
1. **PLAN Phase**: Product Requirements Expert generates user stories ‚Üí Stores in `user_stories` table
2. **PLAN Verification**: QA Director queries user stories ‚Üí Creates professional test cases ‚Üí Validates with Playwright
3. **Evidence**: Each user story must have corresponding passing E2E test(s)
4. **Approval**: LEAD cannot approve SD without 100% user story validation

### Key Principle

**"Smoke tests tell you if it loads. E2E tests tell you if it works. We require BOTH, with emphasis on E2E."**

## SIMPLICITY FIRST Decision Framework

## SIMPLICITY FIRST Decision Framework (MANDATORY)

**Evidence from Retrospectives**: Applied successfully in 4 out of 7 retrospectives, saved 4-6 hours per SD.

### LEAD Decision Framework (Before Approving ANY SD)

#### Question 1: Can We Document Instead of Implement?

**Example** (SD-UAT-003): Database blocker ‚Üí documented instead ‚Üí saved 4-6 hours

#### Question 2: Is This Solving Real or Imagined Problems?

**Example** (SD-UAT-002): Code review rejected 3/5 false claims ‚Üí saved 3-4 hours

#### Question 3: Can We Use Existing Infrastructure?

**Example** (SD-UAT-020): Used existing Supabase Auth ‚Üí saved 8-10 hours

#### Question 4: Is Complexity Inherent or Self-Imposed?

**Reject**: Premature optimization, custom solutions when proven tools exist

### Total Savings from Retrospectives: 15-20 hours across 3 SDs

## PLAN Pre-EXEC Checklist

## PLAN Agent Pre-EXEC Checklist (MANDATORY)

**Evidence from Retrospectives**: Database verification issues appeared in SD-UAT-003, SD-UAT-020, and SD-008. Early verification saves 2-3 hours per blocker.

Before creating PLAN‚ÜíEXEC handoff, PLAN agent MUST verify:

### Database Dependencies ‚úÖ
- [ ] **Identify all data dependencies** in PRD
- [ ] **Run schema verification script** for data-dependent SDs
- [ ] **Verify tables/columns exist** OR create migration
- [ ] **Document verification results** in PLAN‚ÜíEXEC handoff
- [ ] If tables missing: **Escalate to LEAD** with options

**Success Pattern** (SD-UAT-003):
> "Database Architect verification provided evidence for LEAD decision. Documented instead of implementing ‚Üí saved 4-6 hours"

### Architecture Planning ‚úÖ
- [ ] **Component sizing estimated** (target 300-600 lines per component)
- [ ] **Existing infrastructure identified** (don't rebuild what exists)
- [ ] **Third-party libraries considered** before custom code

**Success Pattern** (SD-UAT-020):
> "Leveraged existing Supabase Auth instead of building custom ‚Üí saved 8-10 hours"

### Testing Strategy ‚úÖ
- [ ] **Smoke tests defined** (3-5 tests minimum)
- [ ] **Test scenarios documented** in PRD

### SIMPLICITY FIRST Validation ‚úÖ
- [ ] **Verified claims with code review** (if UI/UX SD)
- [ ] **Rejected unnecessary complexity**

**Success Pattern** (SD-UAT-002):
> "LEAD code review rejected 3/5 false claims ‚Üí saved hours of unnecessary work"

## Testing Tier Strategy

## Testing Requirements - Clear Thresholds

**Evidence from Retrospectives**: Testing confusion appeared in SD-UAT-002, SD-UAT-020, SD-008.

### Three-Tier Testing Strategy

#### Tier 1: Smoke Tests (MANDATORY) ‚úÖ
- **Requirement**: 3-5 tests, <60 seconds execution
- **Approval**: **SUFFICIENT for PLAN‚ÜíLEAD approval**

#### Tier 2: Comprehensive E2E (RECOMMENDED) üìã
- **Requirement**: 30-50 tests covering user flows
- **Approval**: Nice to have, **NOT blocking for LEAD approval**
- **Timing**: Can be refined post-deployment

#### Tier 3: Manual Testing (SITUATIONAL) üîç
- **UI changes**: Single smoke test recommended (+5 min)
- **Logic changes <5 lines**: Optional
- **Logic changes >10 lines**: Required

### Anti-Pattern to Avoid ‚ùå

**DO NOT** create 100+ manual test checklists unless specifically required.

**From SD-UAT-020**:
> "Created 100+ test checklist but didn't execute manually. Time spent on unused documentation."

## Component Sizing Guidelines

## Component Sizing Guidelines

**Evidence from Retrospectives**: Proven pattern in SD-UAT-020 and SD-008.

### Optimal Component Size: 300-600 Lines

**Success Pattern** (SD-UAT-020):
> "Split settings into three focused components. Each ~500 lines. Easy to test and maintain."

### Sizing Rules

| Lines of Code | Action | Rationale |
|---------------|--------|-----------|
| **<200** | Consider combining | Too granular |
| **300-600** | ‚úÖ **OPTIMAL** | Sweet spot |
| **>800** | **MUST split** | Too complex |

## TODO Comment Standard

## TODO Comment Standard (When Deferring Work)

**Evidence from Retrospectives**: Proven pattern in SD-UAT-003 saved 4-6 hours.

### Standard TODO Format

```typescript
// TODO (SD-ID): Action required
// Requires: Dependencies, prerequisites
// Estimated effort: X-Y hours
// Current state: Mock/temporary/placeholder
```

**Success Pattern** (SD-UAT-003):
> "Comprehensive TODO comments provided clear future work path. Saved 4-6 hours."

## EXEC Dual Test Requirement

## EXEC Dual Test Requirement


### ‚ö†Ô∏è MANDATORY: Dual Test Execution

**CRITICAL**: "Smoke tests" means BOTH test types, not just one!

**Evidence**: SD-EXPORT-001 - Tests existed but weren't executed. 30-minute gap between "complete" and validation. SD-EVA-MEETING-002 - 67% E2E failure rate when finally run.

Before creating EXEC‚ÜíPLAN handoff, EXEC MUST run:

#### 1. Unit Tests (Business Logic Validation)
```bash
cd /mnt/c/_EHG/ehg
npm run test:unit
```
- **What it validates**: Service layer, business logic, data transformations
- **Failure means**: Core functionality is broken
- **Required for**: EXEC‚ÜíPLAN handoff
- **Framework**: Vitest

#### 2. E2E Tests (UI/Integration Validation)
```bash
cd /mnt/c/_EHG/ehg
npm run test:e2e
```
- **What it validates**: User flows, component rendering, integration
- **Failure means**: User-facing features don't work
- **Required for**: EXEC‚ÜíPLAN handoff
- **Framework**: Playwright

#### Verification Checklist
- [ ] Unit tests executed: `npm run test:unit`
- [ ] Unit tests passed: [X/X tests]
- [ ] E2E tests executed: `npm run test:e2e`
- [ ] E2E tests passed: [X/X tests]
- [ ] Both test types documented in EXEC‚ÜíPLAN handoff
- [ ] Screenshots captured for E2E test evidence
- [ ] Test results included in handoff "Deliverables Manifest"

**‚ùå BLOCKING**: Cannot create EXEC‚ÜíPLAN handoff without BOTH test types passing.

**Common Mistakes** (from SD-EXPORT-001):
- ‚ùå "Tests exist" ‚â† "Tests passed"
- ‚ùå Running only E2E tests and claiming "all tests passed"
- ‚ùå Marking SD complete before running any tests
- ‚ùå Creating handoff without test evidence documentation
- ‚úÖ Run BOTH unit AND E2E tests explicitly
- ‚úÖ Document pass/fail counts in handoff
- ‚úÖ Include screenshots for visual evidence

### Why This Matters
- **SD-EXPORT-001**: 30-minute gap between marking "complete" and discovering tests weren't run
- **SD-EVA-MEETING-002**: 67% E2E failure rate revealed only when tests finally executed
- **Impact**: Testing enforcement prevents claiming "done" without proof

## CI/CD Pipeline Verification

## CI/CD Pipeline Verification (MANDATORY)

**Evidence from Retrospectives**: Gap identified in SD-UAT-002 and SD-LEO-002.

### Verification Process

**After EXEC implementation complete, BEFORE PLAN‚ÜíLEAD handoff**:

1. Wait 2-3 minutes for GitHub Actions to complete
2. Trigger DevOps sub-agent to verify pipeline status
3. Document CI/CD status in PLAN‚ÜíLEAD handoff
4. PLAN‚ÜíLEAD handoff is **BLOCKED** if pipelines failing

## Sub-Agent Auto-Trigger Enforcement (MANDATORY)

## Sub-Agent Auto-Trigger Enforcement (MANDATORY)

**Evidence**: SD-EXPORT-001, SD-EVA-MEETING-001 - "QA sub-agent never triggered during initial evaluation" and "No automatic trigger for Product Requirements Expert sub-agent"

**CRITICAL**: Sub-agents MUST be triggered automatically, not manually.

### Problem Statement

From retrospectives:
- **SD-EXPORT-001**: "Protocol says 'trigger sub-agents' but wasn't enforced"
- **Impact**: Manual human intervention required to remember testing
- **Result**: 30-minute gap between "complete" and discovering no tests run

### Automated Sub-Agent Triggers

#### EXEC Phase Completion Checklist
Before creating EXEC‚ÜíPLAN handoff:

1. **QA Engineering Director** - MANDATORY ‚ö†Ô∏è
   - **Trigger**: `EXEC_IMPLEMENTATION_COMPLETE`
   - **Must run**: Before handoff creation
   - **Blocks**: Handoff if tests fail
   - **Script**: `node scripts/qa-engineering-director-enhanced.js <SD-ID> --full-e2e`

2. **Product Requirements Expert** - AUTO
   - **Trigger**: PRD creation event
   - **Validates**: User stories exist
   - **Blocks**: PLAN‚ÜíEXEC handoff if missing
   - **Script**: Auto-triggered on PRD insert

3. **Continuous Improvement Coach** - AUTO
   - **Trigger**: SD status = completed
   - **Generates**: Retrospective
   - **Required**: For final closure
   - **Script**: Auto-triggered on SD completion

### Enforcement Mechanism

#### EXEC‚ÜíPLAN Handoff Script Verification

```javascript
// In unified-handoff-system.js or create-exec-to-plan-handoff.js

// MANDATORY: Check for QA execution before allowing handoff
const { data: qaResults } = await supabase
  .from('sub_agent_execution_results')
  .select('*')
  .eq('sd_id', sd_id)
  .eq('sub_agent_code', 'TESTING')
  .order('created_at', { ascending: false })
  .limit(1);

if (!qaResults || qaResults.length === 0) {
  console.error('‚ùå BLOCKED: QA Engineering Director must run before EXEC‚ÜíPLAN handoff');
  console.error('   Run: node scripts/qa-engineering-director-enhanced.js ' + sd_id + ' --full-e2e');
  process.exit(1);
}

if (qaResults[0].verdict === 'BLOCKED') {
  console.error('‚ùå BLOCKED: QA Director verdict is BLOCKED - cannot proceed with handoff');
  console.error('   Reason:', qaResults[0].recommendations);
  process.exit(1);
}

console.log('‚úÖ QA Director verification passed');
```

### Anti-Patterns

‚ùå **"I'll manually trigger QA later"** - Never acceptable
‚ùå **"Tests can wait until after handoff"** - Blocks handoff creation
‚ùå **"QA is optional for small changes"** - MANDATORY for ALL implementations
‚ùå **"I'll just document that tests are needed"** - Tests must be EXECUTED, not planned

‚úÖ **QA sub-agent runs automatically via trigger**
‚úÖ **Handoff creation blocked if QA not run**
‚úÖ **Manual override requires LEAD approval**

### Manual Override (Emergency Only)

If absolutely necessary (infrastructure issues, test environment down):

```bash
# Create handoff with override flag
node scripts/unified-handoff-system.js execute EXEC-to-PLAN <SD-ID> --override-qa

# Requires:
# 1. LEAD explicit approval
# 2. Documentation of why QA couldn't run
# 3. Plan to run QA before PLAN‚ÜíLEAD handoff
```

### Success Criteria

- **Zero** SDs approved without test evidence
- **Zero** "I forgot to run QA" incidents
- **100%** QA sub-agent execution before EXEC‚ÜíPLAN handoffs

### ROI from Retrospectives

- **Time saved**: 30 minutes per SD (avoids gap between "complete" and validation)
- **Quality improvement**: 67% E2E failure rate caught earlier (SD-EVA-MEETING-002)
- **Protocol compliance**: Reduces "worked outside protocol" incidents from 3 to 0

## User Story E2E Test Mapping (MANDATORY)

## User Story E2E Test Mapping (MANDATORY)

**Evidence**: SD-EVA-MEETING-001 - "Initial testing focused on E2E without explicit user story mapping" and "E2E tests without user stories miss the acceptance criteria linkage"

**CRITICAL**: E2E tests MUST map to user stories explicitly.

### Problem Statement

From retrospectives:
- **SD-EVA-MEETING-001**: "User stories should have been created BEFORE implementation (not retroactively)"
- **Gap**: "Protocol gap existed: no enforcement of user story validation"
- **Impact**: Can't verify if requirements are actually met without user story linkage

### Test Naming Convention

**MANDATORY**: Every E2E test must reference a user story.

```typescript
// ‚úÖ CORRECT: Explicit user story reference
test('US-001: User can create new venture', async ({ page }) => {
  // Given: User is on ventures page
  await page.goto('/ventures');

  // When: User clicks "New Venture" button
  await page.click('[data-testid="new-venture-button"]');

  // Then: Create venture modal appears
  await expect(page.locator('[data-testid="venture-modal"]')).toBeVisible();
});

// ‚úÖ CORRECT: Multiple user stories in one test file
test('US-002: User can edit venture name', async ({ page }) => {
  // Test implementation
});

// ‚ùå WRONG: Generic test without user story link
test('Create venture works', async ({ page }) => {
  // Test implementation - MISSING US-XXX reference
});

// ‚ùå WRONG: Implementation detail test (not user-facing)
test('VentureService.create() returns UUID', async () => {
  // This is a unit test, not E2E - doesn't validate user story
});
```

### Coverage Calculation

```javascript
// Formula
User Story Coverage = (E2E Tests with US-XXX / Total User Stories) √ó 100

// Example
Total User Stories: 6
E2E Tests: 6 (US-001, US-002, US-003, US-004, US-005, US-006)
Coverage: 6/6 √ó 100 = 100% ‚úÖ

// Minimum Requirement
Coverage: 100% (every user story MUST have ‚â•1 E2E test)
```

### QA Director Verification

QA Engineering Director sub-agent will:

1. **Query user_stories table** for SD
   ```javascript
   const { data: userStories } = await supabase
     .from('user_stories')
     .select('*')
     .eq('sd_id', sd_id);
   ```

2. **Count E2E tests** with US-XXX references
   ```javascript
   // Scan tests/e2e/**/*.spec.ts for test('US-XXX: ...') patterns
   const e2eTests = await scanForUserStoryTests('tests/e2e');
   ```

3. **Calculate coverage** percentage
   ```javascript
   const coverage = (e2eTests.length / userStories.length) * 100;
   ```

4. **BLOCK if coverage < 100%**
   ```javascript
   if (coverage < 100) {
     return {
       verdict: 'BLOCKED',
       reason: `User story coverage is ${coverage}% (requires 100%)`,
       missing_stories: userStories.filter(us =>
         !e2eTests.some(test => test.includes(us.story_id))
       )
     };
   }
   ```

### File Organization

```
tests/e2e/
‚îú‚îÄ‚îÄ ventures/
‚îÇ   ‚îú‚îÄ‚îÄ venture-creation.spec.ts     # US-001, US-002, US-003
‚îÇ   ‚îú‚îÄ‚îÄ venture-editing.spec.ts      # US-004, US-005
‚îÇ   ‚îî‚îÄ‚îÄ venture-deletion.spec.ts     # US-006
‚îú‚îÄ‚îÄ analytics/
‚îÇ   ‚îî‚îÄ‚îÄ export-analytics.spec.ts     # US-007, US-008
‚îî‚îÄ‚îÄ settings/
    ‚îî‚îÄ‚îÄ user-settings.spec.ts        # US-009, US-010
```

### Example Test File

```typescript
// tests/e2e/ventures/venture-creation.spec.ts
import { test, expect } from '@playwright/test';
import { authenticateUser } from '../fixtures/auth';

test.describe('Venture Creation User Stories', () => {
  test.beforeEach(async ({ page }) => {
    await authenticateUser(page);
  });

  test('US-001: User can navigate to ventures page', async ({ page }) => {
    // Given: User is logged in (from beforeEach)

    // When: User navigates to ventures
    await page.goto('/ventures');

    // Then: Ventures page loads successfully
    await expect(page.locator('h1')).toContainText('Ventures');
    await expect(page).toHaveURL('/ventures');
  });

  test('US-002: User can open create venture modal', async ({ page }) => {
    // Given: User is on ventures page
    await page.goto('/ventures');

    // When: User clicks "New Venture" button
    await page.click('[data-testid="new-venture-button"]');

    // Then: Modal appears with correct fields
    await expect(page.locator('[data-testid="venture-modal"]')).toBeVisible();
    await expect(page.locator('[name="venture-name"]')).toBeVisible();
    await expect(page.locator('[name="venture-stage"]')).toBeVisible();
  });

  test('US-003: User can submit venture with valid data', async ({ page }) => {
    // Given: User has modal open
    await page.goto('/ventures');
    await page.click('[data-testid="new-venture-button"]');

    // When: User fills form and submits
    await page.fill('[name="venture-name"]', 'Test Venture');
    await page.selectOption('[name="venture-stage"]', 'ideation');
    await page.click('[data-testid="submit-venture"]');

    // Then: Venture appears in list
    await expect(page.locator('text=Test Venture')).toBeVisible();
    await expect(page.locator('[data-testid="venture-modal"]')).not.toBeVisible();
  });
});
```

### Success Criteria

- **100%** user story coverage (no exceptions)
- **Every** E2E test has `US-XXX:` prefix
- **QA Director** blocks handoff if coverage < 100%
- **Zero** E2E tests without user story reference

### ROI from Retrospectives

- **SD-EVA-MEETING-001**: Retroactive user story creation avoided ‚Üí saves 1-2 hours per SD
- **Quality**: 100% coverage requirement ensures all requirements validated
- **Clarity**: Explicit linkage between tests and requirements improves communication

### Anti-Patterns

‚ùå **Creating E2E tests before user stories** - Reversed order
‚ùå **Generic test names** without US-XXX - Can't track coverage
‚ùå **Partial coverage** claiming "most important ones tested" - Requires 100%
‚ùå **Manual coverage tracking** - QA Director automates this

‚úÖ **User stories created FIRST** (during PLAN phase)
‚úÖ **E2E tests reference user stories explicitly**
‚úÖ **QA Director validates 100% coverage automatically**
‚úÖ **Handoff blocked if coverage incomplete**

## LEAD Code Review for UI/UX SDs

## LEAD Code Review Requirement (For UI/UX SDs)

**Evidence from Retrospectives**: Critical pattern from SD-UAT-002 saved hours.

### When Code Review is MANDATORY

**For SDs claiming** UI/UX issues or improvements.

### Why Code Review First?

**Success Story** (SD-UAT-002):
> "LEAD challenged 5 claimed issues, validated only 2. Saved 3-4 hours of unnecessary work."

### Process:
1. Receive SD with UI/UX claims
2. Read actual source code (don't trust claims)
3. Verify each claim against implementation
4. Reject false claims, document findings
5. Update SD scope and priority

## üìö Documentation Platform Integration

## üìö Documentation Platform Integration (SD-DOCUMENTATION-001)

### Overview

The AI Documentation Generation System (SD-041C) is integrated into LEO Protocol to ensure all Strategic Directives are automatically documented.

**System Components**:
- **Service**: `src/services/doc-generator.ts` (351 lines) - Generates markdown/HTML from AI analysis
- **Admin UI**: `/ai-docs-admin` - Review and publish generated documentation
- **Database**: `generated_docs` table - Stores all documentation
- **Automation**: `scripts/generate-workflow-docs.js` (1,398 lines)

### When Documentation is Generated

**Automatic Triggers**:
- SD completion (status: completed)
- EXEC‚ÜíPLAN handoff (implementation complete)
- Retrospective creation

**Manual Triggers**:
- DOCMON sub-agent engagement
- Dashboard: AI Docs Admin page

### EXEC Agent Documentation Requirement

**MANDATORY Step (Before EXEC‚ÜíPLAN Handoff)**:

After implementation complete, EXEC agent MUST:

```bash
# Generate documentation for the SD
node scripts/generate-workflow-docs.js --sd-id <SD-ID>

# Verify documentation created
SELECT id, title, status, document_type 
FROM generated_docs 
WHERE sd_id = '<SD-ID>';
```

**Validation**: EXEC‚ÜíPLAN handoff will check `generated_docs` table. If no documentation exists, handoff is BLOCKED.

### Documentation Types

| Type | Description | Auto-Generated |
|------|-------------|----------------|
| **SD Summary** | Strategic directive overview | ‚úÖ Yes |
| **PRD** | Product requirements document | ‚úÖ Yes |
| **Implementation Guide** | Technical implementation steps | ‚úÖ Yes |
| **Retrospective** | Post-completion learnings | ‚úÖ Yes |
| **API Documentation** | If API changes made | ‚ö†Ô∏è Manual |

### Using the AI Docs Admin Dashboard

1. Navigate to `/ai-docs-admin` in the dashboard
2. Review generated documentation
3. Edit if needed (markdown editor)
4. Click "Publish" to make available
5. Documentation appears in appropriate locations

### Documentation Quality Standards

**Required Elements**:
- Clear title and summary
- Technical details (if applicable)
- User stories covered
- Test evidence links
- Known issues documented

**Length Guidelines**:
- SD Summary: 200-500 words
- PRD: 500-1500 words
- Implementation Guide: 300-800 words
- Retrospective: 400-1000 words

### DOCMON Sub-Agent Integration

The **Information Architecture Lead** (DOCMON) sub-agent:
- **Priority**: 95 (CRITICAL)
- **Triggers**: Automatic on EXEC completion, manual for reviews
- **Purpose**: Validate documentation completeness and quality
- **Actions**: 
  - Check `generated_docs` table
  - Validate required elements present
  - Flag missing documentation as BLOCKER

### Troubleshooting

**Issue**: Documentation not generated
- **Check**: `ai_analysis_jobs` table for errors
- **Fix**: Re-run `generate-workflow-docs.js` with `--force` flag

**Issue**: Documentation incomplete
- **Check**: PRD has all required sections
- **Fix**: Update PRD, regenerate docs

**Issue**: EXEC‚ÜíPLAN handoff blocked
- **Check**: Query `generated_docs` for SD-ID
- **Fix**: Generate missing documentation before retrying handoff

### Benefits

- **100% SD coverage**: No SD completes without documentation
- **Automatic generation**: Reduces manual documentation burden
- **Quality consistency**: AI ensures standard format
- **Search & Discovery**: All docs in central `generated_docs` table
- **Onboarding**: New team members find comprehensive docs

### Related Scripts

- `scripts/generate-workflow-docs.js` - Generate documentation
- `scripts/doc-generator.ts` - Core generation service
- `scripts/ai-docs-analyzer.ts` - AI analysis engine

### Related Database Tables

- `generated_docs` - All documentation storage
- `ai_analysis_jobs` - Background job tracking
- `strategic_directives_v2` - SD metadata (linked)
- `product_requirements_v2` - PRD content (source)


## E2E Testing: Dev Mode vs Preview Mode

## E2E Testing: Dev Mode vs Preview Mode (CRITICAL)

**Discovery**: SD-AGENT-ADMIN-002 revealed that Playwright E2E tests fail with blank pages in preview mode but pass perfectly in dev mode.

### The Problem with Preview Mode

**Symptoms**:
- Login pages render as blank HTML (no input elements)
- Authentication setup fails consistently
- Tests report "Has input elements: false"
- Screenshot shows empty white page

**Root Cause**: Production-optimized builds (preview mode) may have:
- Different React hydration timing
- Stripped debug code affecting test selectors
- Lazy-loaded components not loading correctly in test environment
- Static file serving instead of hot-reload source

### Solution: Default to Dev Mode

**Configuration** (playwright.config.ts):
```typescript
// ‚úÖ RECOMMENDED: Dev Mode (port 5173)
const baseURL = process.env.PW_BASE_URL ?? 'http://localhost:5173';
webServer: {
  command: 'npm run dev -- --port 5173',
  port: 5173,
  reuseExistingServer: true,
  timeout: 120_000,
}

// ‚ùå PROBLEMATIC: Preview Mode (port 4173)
const baseURL = process.env.PW_BASE_URL ?? 'http://localhost:4173';
webServer: {
  command: 'npm run build && npm run preview -- --port 4173',
  port: 4173,
}
```

### When to Use Each Mode

| Mode | Port | Use Case | Pros | Cons |
|------|------|----------|------|------|
| **Dev** | 5173 | Default E2E testing | Fast, hot reload, full source maps | Not production-like |
| **Preview** | 4173 | Production parity testing | Optimized build, realistic | Slower, may have rendering issues |

### Decision Matrix

**Use Dev Mode (5173) when**:
- ‚úÖ Running smoke tests (Tier 1)
- ‚úÖ Developing new E2E tests
- ‚úÖ Debugging test failures
- ‚úÖ Daily development testing
- ‚úÖ CI/CD pipelines (faster feedback)

**Use Preview Mode (4173) when**:
- üìã Final pre-release validation
- üìã Performance benchmarking
- üìã Testing production-like bundle sizes
- üìã Validating lazy loading behavior

### Troubleshooting E2E Test Failures

**If tests fail with authentication/rendering issues**:

1. **Check build mode first**:
   ```bash
   grep baseURL playwright.config.ts
   # Should show port 5173 for dev mode
   ```

2. **Switch to dev mode if using preview**:
   ```typescript
   // Change this
   const baseURL = 'http://localhost:4173';
   // To this
   const baseURL = 'http://localhost:5173';
   ```

3. **Update auth state file**:
   ```bash
   # Ensure origin matches port
   cat .auth/user.json | grep origin
   # Should show: "origin": "http://localhost:5173"
   ```

4. **Restart dev server**:
   ```bash
   cd /mnt/c/_EHG/ehg
   npm run dev -- --port 5173
   ```

5. **Re-run tests**:
   ```bash
   npm run test:e2e
   ```

### Protocol Update (From Retrospective)

**Before**: No guidance on build modes for E2E testing ‚Üí 1.5 hours debugging authentication

**After**: Dev mode default, preview mode for specific use cases ‚Üí Instant test success

**Evidence**: SD-AGENT-ADMIN-002 E2E tests:
- Preview mode: 0/5 passing (blank pages)
- Dev mode: 5/5 passing (26.8s execution time)

### Key Takeaways

1. **Dev mode is safer** for E2E testing (faster, more reliable)
2. **Preview mode is optional** (only for production parity validation)
3. **When debugging E2E failures**, check build mode before debugging test logic
4. **Trust your test code**, question environment configuration first

## Handoff Creation: RLS Bypass Pattern

## üîê Handoff Creation: RLS Bypass Pattern

**Critical Discovery**: SD-SUBAGENT-IMPROVE-001 revealed that `sd_phase_handoffs` table RLS policies block programmatic INSERT operations using SUPABASE_ANON_KEY.

### The Problem

**Symptom**:
```
‚ùå new row violates row-level security policy for table "sd_phase_handoffs"
```

**Root Cause**:
- RLS policy: `CREATE POLICY "Allow authenticated insert" TO authenticated`
- Using `SUPABASE_ANON_KEY` which lacks INSERT permission
- Impact: Cannot create handoffs programmatically via Supabase client

### The Solution: Direct PostgreSQL Connection

**Pattern** (from `lib/supabase-connection.js`):
```javascript
import { createDatabaseClient } from '../ehg/scripts/lib/supabase-connection.js';

async function storeHandoff(type, sdId, handoffContent) {
  // Direct connection bypasses RLS policies
  const client = await createDatabaseClient('engineer', {
    verify: true,
    verbose: true
  });

  try {
    const insertSQL = `
INSERT INTO sd_phase_handoffs (
  sd_id, from_phase, to_phase, handoff_type, status,
  executive_summary, deliverables_manifest, key_decisions,
  known_issues, resource_utilization, action_items,
  completeness_report, metadata, created_at
) VALUES (
  $1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, NOW()
) RETURNING id;
`;

    const result = await client.query(insertSQL, [
      sdId, phases.from, phases.to, type, 'pending_acceptance',
      handoffContent.executive_summary, handoffContent.deliverables_manifest,
      handoffContent.key_decisions, handoffContent.known_issues,
      handoffContent.resource_utilization, handoffContent.action_items,
      handoffContent.completeness_report, JSON.stringify(metadata)
    ]);

    return result.rows[0].id;
  } finally {
    await client.end();
  }
}
```

### Connection Details

**Established Pattern**:
- Region: `aws-1-us-east-1` (NOT aws-0)
- Port: 5432 (Transaction Mode)
- SSL: `{ rejectUnauthorized: false }`
- Format: `postgresql://postgres.PROJECT_ID:PASSWORD@aws-1-us-east-1.pooler.supabase.com:5432/postgres`

### Reference Implementation

See `scripts/store-handoff-direct.js` for complete working example.

### Alternative Solutions

**Option A: SERVICE_ROLE_KEY** (Recommended for production)
```javascript
const supabase = createClient(
  process.env.SUPABASE_URL,
  process.env.SUPABASE_SERVICE_ROLE_KEY // Bypasses all RLS
);
```

**Option B: Direct Connection** (Current implementation)
- Use existing `lib/supabase-connection.js` helper
- Bypasses RLS without additional keys
- Requires database password (already in .env)

**Option C: Modify RLS Policy** (‚ùå NOT RECOMMENDED)
- Security risk: Allows anonymous INSERT
- Verdict: DO NOT DO THIS

### When to Use This Pattern

Use direct connection bypass when:
- Creating handoffs programmatically
- Storing sub-agent execution results
- Any operation blocked by RLS policies
- SERVICE_ROLE_KEY not available

## Retrospective Table Schema Reference

## üìù Retrospective Table Schema Reference

**Critical Discovery**: SD-SUBAGENT-IMPROVE-001 encountered 9 schema constraint errors during retrospective generation. This reference prevents future issues.

### Field Name Mappings

**CORRECT NAMES** (use these):
```javascript
{
  key_learnings: string,           // NOT lessons_learned
  what_needs_improvement: string,  // NOT what_did_not_work_well
  improvement_areas: array,        // NOT protocol_improvements (array, not string)
  description: string,             // NOT technical_innovations
  action_items: string,            // NOT recommendations
  // NO metrics_and_roi field exists
}
```

### Required Fields with Check Constraints

#### 1. generated_by (Check Constraint)
```javascript
generated_by: 'MANUAL' // ONLY allowed value discovered
```

**Discovery Method**: Query existing records
```sql
SELECT DISTINCT generated_by FROM retrospectives;
-- Result: Only 'MANUAL' found
```

#### 2. status (Check Constraint)
```javascript
status: 'PUBLISHED' // ONLY allowed value discovered
```

#### 3. team_satisfaction (Scale: 1-10)
```javascript
team_satisfaction: 9 // Use 1-10 scale, NOT 0-100
```

**Discovery Method**: Query value range
```sql
SELECT team_satisfaction FROM retrospectives WHERE team_satisfaction IS NOT NULL;
-- Result: Range 4-9, indicates 1-10 scale
```

### Data Type Requirements

#### Boolean Fields (NOT integers)
```javascript
objectives_met: true,     // NOT 5
on_schedule: true,        // NOT 1
within_scope: true,       // NOT 1
auto_generated: false     // NOT 0
```

#### Array Fields (NOT JSON strings)
```javascript
// ‚ùå WRONG:
success_patterns: JSON.stringify(['pattern1', 'pattern2']),
failure_patterns: JSON.stringify(['pattern1', 'pattern2']),
improvement_areas: JSON.stringify(['item1', 'item2']),

// ‚úÖ CORRECT:
success_patterns: ['pattern1', 'pattern2'],
failure_patterns: ['pattern1', 'pattern2'],
improvement_areas: ['item1', 'item2']
```

### Complete Schema Template

```javascript
const retrospective = {
  sd_id: 'SD-XXX',
  title: 'Retrospective Title',
  retro_type: 'SD_COMPLETION',
  project_name: 'Project Name',
  conducted_date: new Date().toISOString(),
  generated_by: 'MANUAL',              // ‚Üê Must be 'MANUAL'
  trigger_event: 'LEAD_APPROVAL_COMPLETE',
  status: 'PUBLISHED',                 // ‚Üê Must be 'PUBLISHED'
  auto_generated: false,

  // Long text fields
  what_went_well: `**Successes**: ...`,
  what_needs_improvement: `**Challenges**: ...`,
  key_learnings: `**Takeaways**: ...`,
  description: `**Innovations**: ...`,
  action_items: `**Next Steps**: ...`,

  // Numeric metrics (0-100 except team_satisfaction)
  quality_score: 95,
  velocity_achieved: 12,
  team_satisfaction: 9,                // ‚Üê 1-10 scale
  business_value_delivered: 90,
  bugs_found: 2,
  bugs_resolved: 2,
  tests_added: 3,

  // Boolean fields
  on_schedule: true,
  within_scope: true,
  objectives_met: true,

  // Array fields (NOT JSON strings)
  success_patterns: [
    'Pattern 1',
    'Pattern 2'
  ],
  failure_patterns: [
    'Pattern 1',
    'Pattern 2'
  ],
  improvement_areas: [
    'Area 1',
    'Area 2'
  ]
};
```

### Schema Validation Pattern

**Before generating retrospectives**, query for schema constraints:
```javascript
// Discover allowed values for enum-like fields
const { data } = await supabase
  .from('retrospectives')
  .select('generated_by, status')
  .limit(20);

const allowedGeneratedBy = [...new Set(data.map(r => r.generated_by))];
const allowedStatus = [...new Set(data.map(r => r.status))];

console.log('Allowed generated_by:', allowedGeneratedBy);
console.log('Allowed status:', allowedStatus);
```

### Common Errors & Fixes

| Error | Fix |
|-------|-----|
| Column "lessons_learned" not found | Use `key_learnings` |
| Malformed array literal | Remove JSON.stringify() |
| Invalid boolean "5" | Use true/false |
| team_satisfaction_check violation | Use 1-10 scale |
| generated_by_check violation | Use 'MANUAL' |
| status_check violation | Use 'PUBLISHED' |

### Reference Implementation

See `scripts/generate-retrospective-subagent-improve-001.js` for complete working example.

## Database Trigger Management for Special Cases

## üîß Database Trigger Management for Special Cases

**Critical Discovery**: SD-SUBAGENT-IMPROVE-001 revealed that `enforce_progress_trigger` can block SD completion when trigger function queries are affected by RLS policies.

### The Problem

**Symptom**:
```
‚ùå LEO Protocol Violation: Cannot mark SD complete
Progress: %70 (need 100%)
Incomplete phases: { LEAD_final_approval: { handoffs_complete: false } }
```

**Root Cause Analysis**:
1. Handoffs created via direct PostgreSQL connection (bypassing RLS)
2. Trigger function `enforce_progress_on_completion()` likely uses Supabase client
3. Supabase client subject to RLS policies
4. Result: Trigger sees 0 handoffs, blocks completion

**Evidence**:
```javascript
// Via Supabase client (what trigger sees)
const { data } = await supabase.from('sd_phase_handoffs').select('*');
// Result: 0 handoffs

// Via direct connection (actual state)
const result = await client.query('SELECT * FROM sd_phase_handoffs WHERE sd_id = $1', [sdId]);
// Result: 4 handoffs
```

### Solution: Temporary Trigger Disable

**Safe Pattern**:
```javascript
import { createDatabaseClient } from '../ehg/scripts/lib/supabase-connection.js';

async function markSDComplete(sdId) {
  const client = await createDatabaseClient('engineer', {
    verify: false,
    verbose: false
  });

  try {
    // Step 1: Disable trigger
    await client.query(
      'ALTER TABLE strategic_directives_v2 DISABLE TRIGGER enforce_progress_trigger'
    );
    console.log('‚úÖ Trigger disabled');

    // Step 2: Critical operation
    const result = await client.query(`
      UPDATE strategic_directives_v2
      SET status = 'completed', progress = 100
      WHERE id = $1
      RETURNING id, status, progress
    `, [sdId]);

    console.log('‚úÖ SD marked as complete');

    // Step 3: Re-enable trigger (ALWAYS)
    await client.query(
      'ALTER TABLE strategic_directives_v2 ENABLE TRIGGER enforce_progress_trigger'
    );
    console.log('‚úÖ Trigger re-enabled');

    return result.rows[0];

  } catch (error) {
    console.error('‚ùå Error:', error.message);
    
    // CRITICAL: Always re-enable trigger even on error
    try {
      await client.query(
        'ALTER TABLE strategic_directives_v2 ENABLE TRIGGER enforce_progress_trigger'
      );
      console.log('‚úÖ Trigger re-enabled after error');
    } catch (e) {
      console.error('‚ö†Ô∏è Could not re-enable trigger:', e.message);
    }
    
    throw error;
  } finally {
    await client.end();
  }
}
```

### Finding Trigger Names

**Query trigger information**:
```sql
SELECT 
  trigger_name,
  event_manipulation,
  event_object_table,
  action_statement
FROM information_schema.triggers
WHERE event_object_table = 'strategic_directives_v2'
ORDER BY trigger_name;
```

**Known Triggers on `strategic_directives_v2`**:
- `enforce_progress_trigger` - Progress validation on completion
- `enforce_handoff_trigger` - Handoff validation on phase transition
- `auto_calculate_progress_trigger` - Automatic progress calculation
- `status_auto_transition` - Status state machine
- (11+ more triggers for audit, notification, validation)

### When to Use This Pattern

**Use temporary trigger disable when**:
- ‚úÖ Legitimate special case (infrastructure SD, protocol SD)
- ‚úÖ All requirements genuinely met but trigger cannot verify
- ‚úÖ RLS policies blocking trigger's validation queries
- ‚úÖ No other solution available (SERVICE_ROLE_KEY, policy update)

**DO NOT use when**:
- ‚ùå Requirements actually incomplete
- ‚ùå Trying to bypass valid validation
- ‚ùå Could fix by completing missing work
- ‚ùå Haven't investigated root cause

### Long-Term Solutions

**Option A: Fix Trigger Function** (Recommended)
```sql
-- Update trigger function to use SECURITY DEFINER
CREATE OR REPLACE FUNCTION enforce_progress_on_completion()
RETURNS TRIGGER
SECURITY DEFINER  -- ‚Üê Runs with function owner's permissions
AS $$
BEGIN
  -- Validation queries now bypass RLS
  ...
END;
$$ LANGUAGE plpgsql;
```

**Option B: Use SERVICE_ROLE_KEY in Application**
```javascript
// Handoff creation uses service role key
const supabase = createClient(
  process.env.SUPABASE_URL,
  process.env.SUPABASE_SERVICE_ROLE_KEY
);
// Now trigger can see handoffs
```

**Option C: Add RLS Policy for Trigger Access**
```sql
-- Allow trigger function to read handoffs
CREATE POLICY "trigger_access" ON sd_phase_handoffs
FOR SELECT
TO authenticated
USING (true);
```

### Best Practices

1. **Always Re-Enable**: Use try-catch-finally pattern
2. **Document Why**: Log reason for trigger disable
3. **Verify State**: Confirm requirements actually met
4. **One-Time Use**: Don't make this a regular pattern
5. **Report Issue**: File for long-term fix

### Reference Implementation

See completion of SD-SUBAGENT-IMPROVE-001 for working example.

## Proactive Context Monitoring

## Proactive Context Monitoring (MANDATORY)

**Critical Lesson**: Context management prevents conversation overflow and ensures smooth handoffs.

---

### Token Reporting in Every Handoff

**MANDATORY**: Before EVERY handoff, agents MUST report context health.

#### Context Health Section (Required in Handoffs)

```markdown
## Context Health

**Current Usage**: [X] tokens ([Y]% of 200K budget)
**Status**: HEALTHY | WARNING | CRITICAL
**Recommendation**: [action if needed]
**Compaction Needed**: YES | NO
```

---

### Context Status Thresholds

| Status | Token Range | Percentage | Action Required |
|--------|-------------|------------|-----------------|
| **HEALTHY** ‚úÖ | 0-140K | 0-70% | Continue normally |
| **WARNING** ‚ö†Ô∏è | 140K-180K | 70-90% | Consider compaction |
| **CRITICAL** üî¥ | 180K-190K | 90-95% | MUST compact before handoff |
| **EMERGENCY** üö® | >190K | >95% | BLOCKED - Force handoff |

---

### Actions by Status

#### HEALTHY (< 140K tokens)
- ‚úÖ No action needed
- Continue with current workflow
- Monitor every handoff

#### WARNING (140K-180K tokens)
- ‚ö†Ô∏è Consider running `/context-compact`
- Review sub-agent reports for compression
- Prepare for compaction in next phase

#### CRITICAL (180K-190K tokens)
- üî¥ **MUST** run `/context-compact` before handoff
- Compress historical handoffs
- Move verbose content to memory
- Cannot proceed to next phase without compaction

#### EMERGENCY (> 190K tokens)
- üö® **BLOCKED** - Cannot create handoff
- Emergency compaction required
- Force phase completion
- Full state dump to memory

---

### How Agents Report Context

**I (Claude) can see real-time token usage**:
```
<system-reminder>Token usage: 127450/200000; 72550 remaining</system-reminder>
```

**I will report in every handoff**:
1. Extract current token count
2. Calculate percentage used
3. Determine status (HEALTHY/WARNING/CRITICAL)
4. Recommend action if needed
5. Document in handoff "Context Health" section

---

### Automatic Triggers

Context compaction is **automatically recommended** when:
- Context exceeds 140K tokens (WARNING status)
- Before creating any handoff
- Before starting new phases
- When memory updates would add >10K tokens

---

### Compaction Command

When compaction needed:
```bash
/context-compact
```

This will:
- Analyze current context usage
- Compress sub-agent reports (Tier 3 ‚Üí reference)
- Compress historical handoffs
- Move verbose details to memory
- Provide before/after token counts

---

### Integration with Handoff System

**7-Element Handoff Structure** now includes:
1. Executive Summary
2. Completeness Report
3. Deliverables Manifest
4. Key Decisions & Rationale
5. Known Issues & Risks
6. Resource Utilization **‚Üê Includes Context Health**
7. Action Items for Receiver

**Element 6 (Resource Utilization) now requires**:
```markdown
## Resource Utilization

**Time Spent**:
- Phase duration: X hours
- Estimated remaining: Y hours

**Context Health** (REQUIRED):
- Current usage: X tokens (Y% of 200K)
- Status: HEALTHY/WARNING/CRITICAL
- Recommendation: [action]
- Compaction needed: YES/NO
```

---

### Benefits

1. **Prevents overflow**: Early warning system before hitting 200K limit
2. **Smooth handoffs**: Ensures next agent has sufficient context budget
3. **Proactive management**: Address issues before they become blockers
4. **Visibility**: Everyone knows context health at all times
5. **Automation**: I (Claude) handle reporting automatically

---

**Related Tools**:
- `/context-compact` - Manual compaction command
- `scripts/context-monitor.js` - CLI token estimation tool
- `lib/context/memory-manager.js` - Memory persistence system


## Database Query Best Practices

## Database Query Best Practices (Context Efficiency)

**Critical Lesson**: Large database query results consume massive context. Smart querying saves 5K-10K tokens per SD.

---

### Core Principles

1. **Select Specific Columns** - Don't fetch unused data
2. **Limit Results** - Use pagination for large datasets
3. **Summarize Large Results** - Show counts, not full dumps
4. **Reference Full Data** - Link to dashboard/database for details

---

### Rule 1: Select Specific Columns Only

**Problem**: `select('*')` fetches all columns, including verbose fields.

#### ‚ùå Bad Pattern
```javascript
const { data: sd } = await supabase
  .from('strategic_directives_v2')
  .select('*')  // Returns all 20+ columns
  .eq('id', sdId)
  .single();

console.log(sd);  // Dumps entire object (500-1000 tokens)
```

#### ‚úÖ Good Pattern
```javascript
const { data: sd } = await supabase
  .from('strategic_directives_v2')
  .select('id, title, status, priority, progress')  // Only needed fields
  .eq('id', sdId)
  .single();

console.log(`SD-${sd.id}: ${sd.title} (status: ${sd.status}, priority: ${sd.priority}, progress: ${sd.progress}%)`);
// Output: SD-XXX: Title (status: active, priority: 85, progress: 45%)
// Tokens: ~50 vs 500-1000
```

**Token Savings**: 90% reduction (500+ tokens ‚Üí 50 tokens)

---

### Rule 2: Limit Results and Paginate

**Problem**: Fetching all rows returns hundreds of records.

#### ‚ùå Bad Pattern
```javascript
const { data: allSDs } = await supabase
  .from('strategic_directives_v2')
  .select('*');  // Returns 100+ SDs with all columns

console.log(allSDs);  // Dumps 50K+ tokens
```

#### ‚úÖ Good Pattern
```javascript
const { data: topSDs, count } = await supabase
  .from('strategic_directives_v2')
  .select('id, title, status, priority', { count: 'exact' })
  .in('status', ['active', 'in_progress'])
  .order('priority', { ascending: false })
  .limit(5);

console.log(`Found ${count} active SDs, showing top 5 by priority:`);
topSDs.forEach((sd, i) => {
  console.log(`  ${i+1}. ${sd.id}: ${sd.title} (priority: ${sd.priority})`);
});
console.log(`\nFull list: http://localhost:3000/strategic-directives`);
```

**Token Savings**: 98% reduction (50K tokens ‚Üí 1K tokens)

---

### Rule 3: Use File Read Offset/Limit

**Problem**: Reading entire large files when only a section is needed.

#### ‚ùå Bad Pattern
```javascript
const claudeMd = await read('/path/to/CLAUDE.md');  // Reads all 1,965 lines
// Only needed: Database Migration Validation section (lines 1180-1340)
// Wasted: 1,805 lines (~7,220 tokens)
```

#### ‚úÖ Good Pattern
```javascript
const claudeMd = await read('/path/to/CLAUDE.md', {
  offset: 1180,
  limit: 160
});  // Reads only needed section

// OR: Use grep to find section first
const sectionStart = await bash('grep -n "## Database Migration Validation" CLAUDE.md');
const claudeMd = await read('/path/to/CLAUDE.md', {
  offset: parseInt(sectionStart),
  limit: 200
});
```

**Token Savings**: 95% reduction (7,220 tokens ‚Üí 360 tokens)

---

### Rule 4: Summarize Large Results

**Problem**: Logging full objects/arrays dumps verbose JSON.

#### ‚ùå Bad Pattern
```javascript
const { data: prd } = await supabase
  .from('product_requirements_v2')
  .select('*')
  .eq('directive_id', sdId)
  .single();

console.log(JSON.stringify(prd, null, 2));  // 2,000+ lines
```

#### ‚úÖ Good Pattern
```javascript
const { data: prd } = await supabase
  .from('product_requirements_v2')
  .select('id, title, status, objectives, acceptance_criteria')
  .eq('directive_id', sdId)
  .single();

console.log(`PRD: ${prd.title}`);
console.log(`Status: ${prd.status}`);
console.log(`Objectives: ${prd.objectives?.length || 0} defined`);
console.log(`Acceptance Criteria: ${prd.acceptance_criteria?.length || 0} items`);
console.log(`\nFull PRD: http://localhost:3000/prd/${prd.id}`);
```

**Token Savings**: 95% reduction (8,000 tokens ‚Üí 400 tokens)

---

### Rule 5: Batch Related Reads

**Problem**: Sequential reads add latency and context separately.

#### ‚ùå Bad Pattern
```javascript
const file1 = await read('path/to/file1.js');
const file2 = await read('path/to/file2.js');
const file3 = await read('path/to/file3.js');
// Each read appears separately in context
```

#### ‚úÖ Good Pattern
```javascript
const [file1, file2, file3] = await Promise.all([
  read('path/to/file1.js'),
  read('path/to/file2.js'),
  read('path/to/file3.js')
]);
// Reads happen in parallel, results presented together
```

**Benefit**: Faster execution + cleaner context presentation

---

### Rule 6: Use Grep for Targeted Search

**Problem**: Reading entire files to search for specific content.

#### ‚ùå Bad Pattern
```javascript
const allFiles = await read('src/');
// Search through 10,000 lines for one function
```

#### ‚úÖ Good Pattern
```javascript
const results = await grep({
  pattern: 'function executeSubAgent',
  path: 'src/',
  output_mode: 'content',
  '-n': true  // Show line numbers
});

// Now read only the specific file + lines found
```

**Token Savings**: 99% reduction for targeted searches

---

### Rule 7: Reference Instead of Dump

**Problem**: Full database dumps for verification.

#### ‚ùå Bad Pattern
```javascript
const { data: retrospectives } = await supabase
  .from('retrospectives')
  .select('*');

console.log('All retrospectives:', retrospectives);  // 50K+ tokens
```

#### ‚úÖ Good Pattern
```javascript
const { data: retroSummary } = await supabase
  .from('retrospectives')
  .select('id, sd_id, quality_score, created_at')
  .order('created_at', { ascending: false })
  .limit(10);

console.log(`Recent retrospectives (${retroSummary.length}):`);
retroSummary.forEach(r => {
  console.log(`  - ${r.sd_id}: Quality ${r.quality_score}/100 (${r.created_at})`);
});
console.log(`\nView all: http://localhost:3000/retrospectives`);
```

**Token Savings**: 98% reduction

---

### Practical Examples

#### Example 1: SD Status Check
```javascript
// Instead of fetching entire SD:
const { data: sd } = await supabase
  .from('strategic_directives_v2')
  .select('id, status, progress')
  .eq('id', sdId)
  .single();

console.log(`${sd.id}: ${sd.status} (${sd.progress}% complete)`);
```

#### Example 2: PRD Requirements Check
```javascript
// Instead of full PRD:
const { data: prd } = await supabase
  .from('product_requirements_v2')
  .select('acceptance_criteria')
  .eq('directive_id', sdId)
  .single();

console.log(`Acceptance Criteria: ${prd.acceptance_criteria.length} items`);
console.log('First 3:');
prd.acceptance_criteria.slice(0, 3).forEach((item, i) => {
  console.log(`  ${i+1}. ${item.title}`);
});
```

#### Example 3: Sub-Agent Results Summary
```javascript
// Instead of full results:
const { data: results, count } = await supabase
  .from('sub_agent_execution_results')
  .select('sub_agent_id, verdict, confidence', { count: 'exact' })
  .eq('sd_id', sdId);

console.log(`Sub-agent executions: ${count}`);
console.log('Results:');
results.forEach(r => {
  console.log(`  - ${r.sub_agent_id}: ${r.verdict} (${r.confidence}% confidence)`);
});
```

---

### Integration with Agent Workflows

**PLAN Agent Pre-EXEC Checklist**:
- ‚úÖ Query only needed PRD fields
- ‚úÖ Limit backlog items to top 10
- ‚úÖ Summarize results, not full dumps

**EXEC Agent Implementation**:
- ‚úÖ Read specific file sections with offset/limit
- ‚úÖ Use grep for targeted searches
- ‚úÖ Batch related reads

**PLAN Supervisor Verification**:
- ‚úÖ Query sub-agent summaries first
- ‚úÖ Fetch full reports only if needed
- ‚úÖ Reference database for comprehensive data

---

### Expected Impact

| Pattern | Before | After | Savings |
|---------|--------|-------|---------|
| select('*') vs specific columns | 500-1000 tokens | 50-100 tokens | 90% |
| All rows vs limit(5) | 50K tokens | 1K tokens | 98% |
| Full file vs offset/limit | 7K tokens | 360 tokens | 95% |
| JSON dump vs summary | 8K tokens | 400 tokens | 95% |

**Total Potential Savings**: 5K-10K tokens per SD

---

**Related Tools**:
- `scripts/examples/efficient-database-queries.js` - Working examples
- Read tool: Supports offset/limit parameters
- Grep tool: Targeted content search


## Sub-Agent Report Compression System

## Sub-Agent Report Compression System

**Critical Lesson**: Sub-agent reports provide essential validation but can consume 10K-20K tokens. Intelligent compression preserves critical context while reducing token usage by 70-90%.

---

### The Challenge

**Problem**: Multiple sub-agent executions create verbose reports:
- 3-5 sub-agents per SD √ó 500-1000 lines each = 10K-20K tokens
- Full reports needed during execution, but not after phase complete
- Critical issues must never be lost, but "all tests passed" can be summarized

**Solution**: Priority-based tiered compression that preserves critical context.

---

### 3-Tier Compression System

#### TIER 1: CRITICAL (No Compression)
**When Applied**:
- `critical_issues.length > 0`
- `verdict === 'BLOCKED'`
- `verdict === 'FAIL'`

**Action**: Full report preserved with zero compression

**Rationale**: Security vulnerabilities, database blockers, test failures require complete context

**Example**:
```json
{
  "agent": "Chief Security Architect",
  "verdict": "BLOCKED",
  "confidence": 95,
  "critical_issues": [
    {
      "severity": "CRITICAL",
      "issue": "SQL injection vulnerability in user input",
      "location": "src/services/userService.ts:127",
      "recommendation": "Use parameterized queries instead of string concatenation",
      "code_snippet": "db.query('SELECT * FROM users WHERE id = ' + userId)"
    }
  ],
  "detailed_analysis": "...",
  "recommendations": ["..."],
  "_compression_tier": "TIER_1_CRITICAL",
  "_compression_note": "Full detail preserved - critical issues present"
}
```

---

#### TIER 2: IMPORTANT (Structured Summary)
**When Applied**:
- `warnings.length > 0`
- `verdict === 'CONDITIONAL_PASS'`
- Sub-agent is relevant to current phase

**Action**: Structured summary with key findings
- All critical issues (if any)
- All warnings (with key fields: issue, severity, recommendation, location)
- Top 5 recommendations
- Key metrics only
- Reference to full report ID

**Rationale**: Warnings need attention but don't require full verbosity

**Example**:
```json
{
  "agent": "QA Engineering Director",
  "verdict": "CONDITIONAL_PASS",
  "confidence": 85,
  "critical_issues": [],
  "warnings": [
    {
      "issue": "Test coverage below 50% for userService.ts",
      "severity": "MEDIUM",
      "recommendation": "Add unit tests for edge cases",
      "location": "src/services/userService.ts"
    }
  ],
  "recommendations": [
    "Add integration tests for authentication flow",
    "Implement E2E tests for critical user paths",
    "Add error boundary tests",
    "Test loading states",
    "Add accessibility tests"
  ],
  "key_metrics": {
    "tests_passed": 47,
    "tests_total": 50,
    "coverage": 48
  },
  "full_report_id": "uuid-of-full-report",
  "_compression_tier": "TIER_2_IMPORTANT",
  "_compression_note": "Structured summary - full report available via ID"
}
```

---

#### TIER 3: INFORMATIONAL (Reference Only)
**When Applied**:
- `verdict === 'PASS'`
- No warnings
- No critical issues
- Not phase-relevant

**Action**: One-line summary with minimal metrics
- Agent name
- Verdict
- Confidence
- Intelligent one-liner based on report type
- Key metrics (2-3 only)
- Reference to full report ID

**Rationale**: "All tests passed" doesn't need 800 lines of detail

**Example**:
```json
{
  "agent": "QA Engineering Director",
  "verdict": "PASS",
  "confidence": 95,
  "summary": "50/50 tests passed. No blockers identified.",
  "key_metrics": {
    "tests_passed": 50,
    "tests_total": 50,
    "coverage": 78
  },
  "full_report_id": "uuid-of-full-report",
  "_compression_tier": "TIER_3_INFORMATIONAL",
  "_compression_note": "Compressed to summary - retrieve full report if needed"
}
```

---

### Phase Relevance Map

Different sub-agents matter more in different phases:

| Phase | Relevant Sub-Agents | Rationale |
|-------|---------------------|-----------|
| **EXEC** | QA Director, Database Architect, Security Architect | Implementation quality gates |
| **PLAN_VERIFICATION** | ALL sub-agents | Supervisor needs complete picture |
| **LEAD_APPROVAL** | Only BLOCKED or WARNING reports | LEAD focuses on issues, not successes |

**Impact**: Same sub-agent report may be TIER_2 during EXEC, TIER_3 after LEAD approval.

---

### Automatic Retrieval Rules

Full reports are automatically retrieved from database when:

1. **Context = PLAN Supervisor Verification**
   - PLAN supervisor always gets full reports for all sub-agents
   - Ensures comprehensive "done done" verification

2. **Context = Retrospective Generation**
   - Continuous Improvement Coach needs full detail for learning

3. **Context = Debugging**
   - Full error context required for troubleshooting

4. **Warnings Present + Context = Verification**
   - TIER_2 summaries expanded to full detail during verification phase

5. **TIER_1 Reports (Critical)**
   - Already full detail, no retrieval needed

---

### Intelligent One-Line Summaries

Compression library generates context-aware summaries based on report type:

**QA Director Pattern**:
```
"50/50 tests passed. No blockers identified."
"47/50 tests passed. 3 edge case failures (non-blocking)."
```

**Database Architect Pattern**:
```
"12 tables validated. Schema compliant."
"Migration #007 applied successfully. No conflicts."
```

**Security Architect Pattern**:
```
"Security scan complete. 0 vulnerabilities found."
"3 vulnerabilities found (0 critical, 2 medium, 1 low)."
```

**Performance Lead Pattern**:
```
"Performance: 142ms load time. Within acceptable range."
"Load time: 3.2s (exceeds 2s target by 60%)."
```

**Generic Pattern** (fallback):
```
"PASS with 95% confidence. No issues identified."
"CONDITIONAL_PASS with 2 issue(s) noted."
```

---

### Implementation Guide

#### For EXEC Agent (Storing Results)

After sub-agent execution:

```javascript
import { getCompressionTier, compressSubAgentReport } from './lib/context/sub-agent-compressor.js';

// 1. Sub-agent generates full report
const fullReport = await executeSubAgent('qa-director', sdId);

// 2. Store full report in database (ALWAYS)
const { data: stored } = await supabase
  .from('sub_agent_execution_results')
  .insert(fullReport)
  .select()
  .single();

// 3. Determine compression tier
const tier = getCompressionTier(fullReport, currentPhase);

// 4. Compress for context
const compressed = compressSubAgentReport(fullReport, tier);

// 5. Use compressed version in conversation
console.log(`QA Director: ${compressed.summary || compressed.verdict}`);
```

---

#### For PLAN Supervisor (Retrieving Full Reports)

During verification:

```javascript
import { retrieveForPlanSupervisor, shouldRetrieveFullReport } from './lib/context/sub-agent-retrieval.js';

// 1. Retrieve organized reports
const reports = await retrieveForPlanSupervisor(sdId);

// 2. PLAN supervisor automatically gets full detail
// (retrieveForPlanSupervisor returns full reports, not compressed)

// 3. Analyze by priority
console.log(`Critical issues: ${reports.critical.length}`);
console.log(`Warnings: ${reports.warnings.length}`);
console.log(`Passed: ${reports.passed.length}`);

// 4. Focus on blockers first
for (const critical of reports.critical) {
  // Full report available for detailed analysis
}
```

---

#### For Retrospective Generation

When Continuous Improvement Coach triggers:

```javascript
import { retrieveAllSubAgentReports } from './lib/context/sub-agent-retrieval.js';

// Retrieve all full reports for learning analysis
const allReports = await retrieveAllSubAgentReports(sdId, {
  orderBy: 'created_at',
  ascending: false
});

// Analyze patterns across full reports
const patterns = analyzeTestingPatterns(allReports);
```

---

### Token Savings Calculator

Expected savings per SD:

| Scenario | Before (Tokens) | After (Tokens) | Savings |
|----------|----------------|----------------|---------|
| 5 sub-agents, all PASS | 15,000 | 1,500 | 90% (13.5K) |
| 4 PASS, 1 WARNING | 15,000 | 4,000 | 73% (11K) |
| 3 PASS, 2 CRITICAL | 15,000 | 9,000 | 40% (6K) |
| All CRITICAL | 15,000 | 15,000 | 0% (preserved) |

**Average Expected Savings**: 15K-30K tokens per SD

---

### Database Integration

#### Storage Table: `sub_agent_execution_results`

Full reports always stored here:

```sql
CREATE TABLE sub_agent_execution_results (
  id UUID PRIMARY KEY,
  sd_id TEXT REFERENCES strategic_directives_v2(id),
  sub_agent_code TEXT,
  sub_agent_name TEXT,
  verdict TEXT,
  confidence INTEGER,
  critical_issues JSONB,
  warnings JSONB,
  recommendations JSONB,
  detailed_analysis TEXT,
  execution_time INTEGER,
  created_at TIMESTAMP DEFAULT NOW()
);
```

#### Retrieval Pattern

```javascript
// Get full report by ID (from compressed report reference)
const fullReport = await retrieveFullSubAgentReport(compressed.full_report_id);

// Get all reports for SD
const allReports = await retrieveAllSubAgentReports(sdId);

// Get only blocked reports
const blocked = await retrieveReportsByVerdict(sdId, 'BLOCKED');
```

---

### Benefits

1. **Context Efficiency**: 70-90% token reduction for passed validations
2. **Critical Preservation**: Zero information loss for blockers
3. **Phase Awareness**: Compression adapts to workflow stage
4. **On-Demand Detail**: Full reports retrievable when needed
5. **Automatic Handling**: PLAN supervisor gets full reports automatically

---

### Integration with Existing Systems

#### Handoff System
- Compressed reports in EXEC‚ÜíPLAN handoffs
- Full reports automatically fetched during PLAN verification
- PLAN‚ÜíLEAD handoffs include only critical/warning summaries

#### Dashboard
- Display compressed summaries in SD status view
- "View Full Report" button retrieves complete analysis
- Token savings displayed in metrics

#### Memory System
- Compressed reports stored in conversation memory
- Full reports in database memory
- Retrieval on-demand prevents memory bloat

---

### Example: Complete Workflow

```javascript
// EXEC Phase: Implementation complete
const qaReport = await executeQADirector(sdId);
const securityReport = await executeSecurityArchitect(sdId);
const dbReport = await executeDatabaseArchitect(sdId);

// Store all full reports
await storeSubAgentResults([qaReport, securityReport, dbReport]);

// Compress for handoff
const { compressed_reports, statistics } = compressBatch(
  [qaReport, securityReport, dbReport],
  'EXEC'
);

console.log(`Token savings: ${statistics.tokens_saved} (${statistics.percentage_saved}%)`);

// EXEC‚ÜíPLAN handoff includes compressed reports
await createHandoff('EXEC-to-PLAN', sdId, {
  sub_agent_summaries: compressed_reports
});

// PLAN Verification: Automatic full retrieval
const fullReports = await retrieveForPlanSupervisor(sdId);

// PLAN supervisor has complete context
const verdict = await planSupervisorVerification(fullReports);
```

---

**Related Tools**:
- `lib/context/sub-agent-compressor.js` - Compression library
- `lib/context/sub-agent-retrieval.js` - Retrieval helpers
- `scripts/examples/compression-demo.js` - Working examples (TODO)


## Database-First Enforcement - Expanded

## üö® DATABASE-ONLY ENFORCEMENT - EXPANDED

### ABSOLUTE PROHIBITION: No File Creation

LEO Protocol v4.2.0 is **DATABASE-FIRST ONLY**. **NEVER** create:
- ‚ùå **Strategic Directive markdown files** (`.md`)
- ‚ùå **PRD markdown files** (`.md`)
- ‚ùå **Retrospective markdown files** (`.md`)
- ‚ùå **Handoff documents** (`.md`)
- ‚ùå **Verification reports** (`.md`)
- ‚ùå Any work-related documentation files

### REQUIRED: Database Operations Only

**Strategic Directives**:
- ‚úÖ Create in `strategic_directives_v2` table
- ‚úÖ Use `scripts/create-strategic-directive.js` or dashboard
- ‚úÖ ALL SD data must be in database, not files

**PRDs (Product Requirements)**:
- ‚úÖ Create in `product_requirements_v2` table
- ‚úÖ Use `scripts/add-prd-to-database.js`
- ‚úÖ Link to SD via `strategic_directive_id` foreign key

**Retrospectives**:
- ‚úÖ Create in `retrospectives` table
- ‚úÖ Use `scripts/generate-comprehensive-retrospective.js`
- ‚úÖ Trigger: Continuous Improvement Coach sub-agent
- ‚úÖ Link to SD via `sd_id` foreign key

**Handoffs**:
- ‚úÖ Store in handoff tracking tables
- ‚úÖ 7-element structure required
- ‚úÖ Link to SD and phase

**Progress & Verification**:
- ‚úÖ Update database fields directly
- ‚úÖ Store verification results in database
- ‚úÖ Track in real-time via dashboard

### Why Database-First?

**From Retrospectives**:
- SD-UAT-002: "Database-first architecture maintained - no data loss, proper tracking"
- SD-008: "Database integration - All operations properly store results in database"
- SD-LEO-002: "Database triggers are powerful for automation"

**Benefits**:
1. Single source of truth
2. Real-time dashboard updates
3. Automated tracking and transitions
4. No file sync issues
5. Proper foreign key relationships
6. Audit trail built-in

### If You Create Files By Mistake:

1. **STOP immediately**
2. Extract content to appropriate database table:
   - SDs ‚Üí `strategic_directives_v2`
   - PRDs ‚Üí `product_requirements_v2`
   - Retrospectives ‚Üí `retrospectives`
3. Delete the markdown files
4. Update progress tracking in database
5. Verify dashboard shows correct status

### Checking Compliance:

```bash
# Check for rogue markdown files
find . -name "SD-*.md" -o -name "PRD-*.md" -o -name "*retrospective*.md"

# Should return ONLY files in retrospectives/ folder (legacy)
# Any new files = VIOLATION
```

## üõ°Ô∏è LEAD Pre-Approval Simplicity Gate

## üõ°Ô∏è LEAD Pre-Approval Simplicity Gate

### MANDATORY Before Approving ANY Strategic Directive

LEAD MUST answer these questions BEFORE approval:

1. **Need Validation**: Is this solving a real user problem or perceived problem?
2. **Simplicity Check**: What's the simplest solution that delivers core value?
3. **Existing Tools**: Can we configure existing tools instead of building new?
4. **80/20 Analysis**: Can we deliver 80% value with 20% of proposed effort?
5. **Scope Reduction**: Should this be split into multiple smaller SDs?
6. **Phase Decomposition**: Can we defer Phase 3-4 features to separate SD?

**If answers suggest over-engineering ‚Üí LEAD MUST**:
- Reduce scope BEFORE approval
- Create separate SDs for deferred work
- Document rationale for scope changes
- Get human approval for major reductions

**SCOPE LOCK**: Once LEAD approves an SD, the scope is LOCKED. LEAD may NOT:
- ‚ùå Re-evaluate "do we really need this?" during final approval
- ‚ùå Apply simplicity filter after EXEC phase begins
- ‚ùå Defer work unilaterally during verification
- ‚ùå Mark SD complete if PRD requirements not met

**Exception**: LEAD may reduce scope mid-execution ONLY if:
1. Critical technical blocker discovered (impossibility)
2. External business priorities changed dramatically
3. Explicit human approval obtained
4. New SD created for all deferred work


## Testing Tier Strategy (Updated)


## Testing Requirements - Dual Test Execution (UPDATED)

**Philosophy**: Comprehensive testing = Unit tests (logic) + E2E tests (user experience)

### Tier 1: Smoke Tests (MANDATORY) ‚úÖ
- **Requirement**: BOTH unit tests AND E2E tests must pass
- **Commands**:
  - Unit: `npm run test:unit` (Vitest - business logic)
  - E2E: `npm run test:e2e` (Playwright - user flows)
- **Approval**: **BOTH test types REQUIRED for PLAN‚ÜíLEAD approval**
- **Execution Time**: Combined <5 minutes for smoke-level tests
- **Coverage**:
  - Unit: Service layer, business logic, utilities
  - E2E: Critical user paths, authentication, navigation

### Tier 2: Comprehensive Testing (RECOMMENDED) üìã
- **Requirement**: Full test suite with deep coverage
- **Commands**:
  - Unit: `npm run test:unit:coverage` (50%+ coverage target)
  - E2E: All Playwright tests (30-50 scenarios)
  - Integration: `npm run test:integration`
  - A11y: `npm run test:a11y`
- **Approval**: Nice to have, **NOT blocking** but highly recommended
- **Timing**: Can be refined post-deployment

### Tier 3: Manual Testing (SITUATIONAL) üîç
- **UI changes**: Visual regression testing
- **Complex flows**: Multi-step wizards, payment flows
- **Edge cases**: Rare scenarios not covered by automation

### ‚ö†Ô∏è What Changed (From Protocol Enhancement)
**Before**: "Tier 1 = 3-5 tests, <60s" (ambiguous - which tests?)
**After**: "Tier 1 = Unit tests + E2E tests (explicit frameworks, explicit commands)"

**Lesson Learned**: SD-AGENT-ADMIN-002 testing oversight (ran E2E only, missed unit test failures)


## Database Migration Validation - Two-Phase Approach

## üîç Database Migration Validation - Two-Phase Approach (MANDATORY)

**Critical Lesson**: SD-AGENT-PLATFORM-001 taught us that migration files can exist and be applied successfully, but **seed data can fail silently**, leaving empty tables.

**Solution**: Two-phase validation catches BOTH file syntax issues AND database state mismatches.

---

### Phase 1: Static File Validation (Always Runs)

**Validates migration files WITHOUT connecting to database**:

- ‚úÖ Migration files exist for SD-ID
- ‚úÖ SQL syntax is valid (no unclosed quotes, balanced parentheses)
- ‚úÖ Required patterns present (CREATE TABLE, ALTER TABLE, etc.)
- ‚úÖ SD references in comments
- ‚úÖ Cross-schema foreign keys detected (auth.users, etc.)

**Command**:
```bash
node scripts/validate-migration-files.js <SD-ID>
```

**Verdict**: `VALID`, `INVALID`, `INCOMPLETE`, or `NOT_REQUIRED`

---

### Phase 2: Database Verification (Optional, via `--verify-db`)

**Validates database state matches migration expectations**:

- ‚úÖ Tables mentioned in migration actually exist in database
- ‚úÖ Tables are accessible (RLS policies allow access)
- ‚úÖ Seed data was inserted (with `--check-seed-data`)

**Command**:
```bash
node scripts/validate-migration-files.js <SD-ID> --verify-db --check-seed-data
```

**Verdicts**:
- `DB_MISMATCH`: Files valid but tables missing
- `DB_ACCESS_ISSUE`: Tables exist but not accessible (RLS)
- `SEED_DATA_MISSING`: Tables exist but 0 rows (silent failure)
- `VALID`: All checks passed

---

### The SD-AGENT-PLATFORM-001 Pattern

**What Happened**:
1. ‚úÖ Migration file `20251008000000_agent_platform_schema.sql` existed
2. ‚úÖ Migration was applied (tables created successfully)
3. ‚ùå Seed data section failed silently
4. ‚ùå Result: 0 records in all tables (ai_ceo_agents, agent_departments, etc.)

**How Two-Phase Validation Catches This**:

```bash
# Old validation (file-only):
node scripts/validate-migration-files.js AGENT-PLATFORM-001
# ‚ùå Result: "VALID - migration file exists and syntax correct"
#    Did NOT catch empty tables!

# New validation (file + database):
node scripts/validate-migration-files.js AGENT-PLATFORM-001 --verify-db --check-seed-data
# ‚úÖ Result: "SEED_DATA_MISSING - Tables exist but 0 rows"
#    CAUGHT THE ISSUE!
```

---

### Integration with LEO Protocol

#### PLAN Agent Pre-EXEC Checklist
Before creating PLAN‚ÜíEXEC handoff:
```bash
node scripts/validate-migration-files.js <SD-ID>
# Verdict must be: VALID, VALID_WITH_WARNINGS, or NOT_REQUIRED
```

#### EXEC Agent Pre-Handoff Checklist
Before creating EXEC‚ÜíPLAN handoff:
```bash
node scripts/validate-migration-files.js <SD-ID> --verify-db --check-seed-data
# Verdict must be: VALID or NOT_REQUIRED
# BLOCKED if: DB_MISMATCH, DB_ACCESS_ISSUE, SEED_DATA_MISSING
```

#### Database Architect Sub-Agent (Automatic)
Triggers on:
- `EXEC_IMPLEMENTATION_COMPLETE` (runs full validation automatically)
- `schema` or `migration` keywords

Executes: `validate-migration-files.js --verify-db --check-seed-data`

Stores results in: `sub_agent_execution_results` table

---

### Usage Examples

#### Basic Validation (File-only)
```bash
node scripts/validate-migration-files.js RECONNECT-014
```
**Use when**: Creating PRD, checking syntax before commit

#### Full Validation (File + Database)
```bash
node scripts/validate-migration-files.js RECONNECT-014 --verify-db
```
**Use when**: EXEC‚ÜíPLAN handoff, verifying migration was applied

#### Comprehensive Check (File + Database + Seed Data)
```bash
node scripts/validate-migration-files.js AGENT-PLATFORM-001 --verify-db --check-seed-data
```
**Use when**: Seed data is critical to functionality (agent platform, user roles, config)

---

### Common Scenarios

**Scenario 1: Migration file has syntax errors**
- Phase 1: `INVALID` verdict
- Action: Fix SQL syntax errors
- Phase 2: Not executed

**Scenario 2: Migration applied but tables missing**
- Phase 1: `VALID`
- Phase 2: `DB_MISMATCH` verdict
- Action: Apply migration (`supabase db push`)

**Scenario 3: Tables exist but empty (SD-AGENT-PLATFORM-001)**
- Phase 1: `VALID`
- Phase 2: `SEED_DATA_MISSING` verdict
- Action: Re-run seed data script

**Scenario 4: No database changes needed**
- Phase 1: `NOT_REQUIRED`
- Action: Proceed without migration validation

---

### Cross-Schema Foreign Keys (Still Validated)

Phase 1 also checks for cross-schema FKs (from SD-RECONNECT-009 lesson):

```sql
-- ‚ùå WRONG: Cross-schema FK
documentation_author UUID REFERENCES auth.users(id),

-- ‚úÖ CORRECT: UUID without FK
documentation_author UUID,  -- FK to auth.users removed
```

**Why**: Supabase migrations cannot reference auth schema. Use `auth.uid()` in RLS policies instead.

---

**Comprehensive Guide**: See `docs/database-migration-validation-guide.md` for troubleshooting, best practices, and error resolution.


## Playwright Server Management Best Practice

### Best Practice: Playwright-Managed Dev Server

**Evidence**: SD-AGENT-MIGRATION-001 - Always let Playwright manage the dev server lifecycle for consistent port and automated testing workflows.

**DO NOT** manually start dev servers before E2E tests. Let Playwright manage it.

#### Configuration (playwright.config.ts)
```typescript
webServer: {
  command: 'npm run dev -- --port 8080',
  port: 8080,
  reuseExistingServer: true,  // Reuse if already running
  timeout: 120_000,            // 2 min startup timeout
}
```

#### Why This Works
- **Consistent Port**: All tests use same port (8080)
- **Auto-Lifecycle**: Server starts before tests, stops after
- **CI/CD Compatible**: Works in automated environments
- **Local Dev Friendly**: `reuseExistingServer` prevents killing your dev server

#### Anti-Patterns
- ‚ùå Starting dev server manually on inconsistent ports
- ‚ùå Forgetting to stop old servers (port conflicts)
- ‚ùå Hardcoding URLs without using `baseURL` from config
- ‚ùå Running tests while manually managing server lifecycle

#### Example Test Run
```bash
# Playwright handles everything
npm run test:e2e

# Playwright:
# 1. Checks if server already running on 8080
# 2. Starts server if needed: npm run dev -- --port 8080
# 3. Waits for server to be ready
# 4. Runs tests
# 5. Keeps server running (reuseExistingServer: true)
```

#### Manual Server Check (if needed)
```bash
# Kill old servers
lsof -i :8080 | grep LISTEN | awk '{print $2}' | xargs kill -9

# Let Playwright manage
npm run test:e2e
```

## Database Migration Pre-Flight Checklist

## üö® MANDATORY: Database Migration Pre-Flight Checklist

### BEFORE Attempting ANY Database Migration

**CRITICAL**: Read the established Database Sub-Agent pattern FIRST. Do NOT trial-and-error.

**Evidence**: SD-AGENT-MIGRATION-001 - Attempted migrations with wrong region (aws-0 vs aws-1), incorrect SSL parameters, and manual psql commands before reading the established pattern. User had to redirect: "Before you blindly go trying things to solve problems, why don't you take a smart approach and make sure you fully understand what is described in the Supabase database sub-agent?"

#### Step 1: Read the Established Pattern (5 minutes)
- [ ] Read `/mnt/c/_EHG/ehg/scripts/lib/supabase-connection.js` (198 lines)
- [ ] Read reference implementation: `scripts/database-subagent-apply-agent-admin-migration.js`
- [ ] Understand: Region (aws-1), SSL config, connection format, helper functions

#### Step 2: Verify Connection Parameters
- [ ] **Region**: aws-1-us-east-1 (NOT aws-0)
- [ ] **Port**: 5432 (Transaction Mode)
- [ ] **SSL**: `{ rejectUnauthorized: false }` (NO ?sslmode=require)
- [ ] **Password**: From .env (SUPABASE_DB_PASSWORD or EHG_DB_PASSWORD)
- [ ] **Format**: `postgresql://postgres.PROJECT_ID:PASSWORD@aws-1-us-east-1.pooler.supabase.com:5432/postgres`

#### Step 3: Use Helper Functions (ALWAYS)
- [ ] Import: `import { createDatabaseClient, splitPostgreSQLStatements } from './lib/supabase-connection.js'`
- [ ] Connect: `const client = await createDatabaseClient('ehg', { verify: true, verbose: true })`
- [ ] Parse: `const statements = splitPostgreSQLStatements(sql)` (handles $$ delimiters)
- [ ] Transaction: `BEGIN`, execute statements, `COMMIT` or `ROLLBACK`

#### Step 4: Validate Migration File
- [ ] No cross-schema foreign keys (REFERENCES auth.users, etc.)
- [ ] RLS policies use auth.uid() only (no FROM/JOIN auth.users)
- [ ] PostgreSQL syntax correct (CREATE POLICY does NOT support IF NOT EXISTS)
- [ ] Use DROP POLICY IF EXISTS + CREATE POLICY instead

#### Step 5: Handle Conflicts
- [ ] Check for existing tables with same names (different schemas)
- [ ] Drop old tables if System A/B migration (use CASCADE carefully)
- [ ] Verify seed data inserts (ON CONFLICT DO NOTHING)

### Example Template
```javascript
#!/usr/bin/env node
import { createDatabaseClient, splitPostgreSQLStatements } from './lib/supabase-connection.js';
import { readFileSync } from 'fs';

async function applyMigration() {
  const client = await createDatabaseClient('ehg', {
    verify: true,
    verbose: true
  });

  try {
    const sql = readFileSync('migration.sql', 'utf-8');
    const statements = splitPostgreSQLStatements(sql);

    await client.query('BEGIN');

    for (const stmt of statements) {
      try {
        await client.query(stmt);
      } catch (error) {
        if (error.message.includes('already exists')) {
          console.log('Skipped (exists)');
        } else {
          throw error;
        }
      }
    }

    await client.query('COMMIT');
  } catch (error) {
    await client.query('ROLLBACK');
    throw error;
  } finally {
    await client.end();
  }
}

applyMigration().catch(err => process.exit(1));
```

### Anti-Patterns to AVOID
- ‚ùå Using psql directly without understanding connection format
- ‚ùå Trial-and-error with different regions/ports/SSL settings
- ‚ùå Creating migration scripts without reading lib/supabase-connection.js
- ‚ùå Not using splitPostgreSQLStatements() for complex SQL
- ‚ùå Not handling "already exists" errors gracefully

## Mandatory Handoff Requirements

Every handoff MUST include these 7 elements:
1. Executive Summary
2. Completeness Report
3. Deliverables Manifest
4. Key Decisions & Rationale
5. Known Issues & Risks
6. Resource Utilization
7. Action Items for Receiver

Missing ANY element = AUTOMATIC REJECTION

## üîÑ Unified Handoff System (Database-First)

### ‚ö†Ô∏è CRITICAL: Use the Unified Handoff System Script

**PRIMARY METHOD**: `node scripts/unified-handoff-system.js execute <TYPE> <SD-ID>`

**DO NOT** manually create handoff scripts or query handoff tables directly. The unified system handles:
- Template loading from `leo_handoff_templates` table
- Validation using 7-element structure
- Storage in `sd_phase_handoffs` table
- Automatic SD phase progression
- Dashboard synchronization

### Handoff Types

| Type | From | To | Purpose |
|------|------|----|----|
| `LEAD-to-PLAN` | LEAD | PLAN | Strategic objectives ‚Üí Technical planning |
| `PLAN-to-EXEC` | PLAN | EXEC | PRD approval ‚Üí Implementation |
| `EXEC-to-PLAN` | EXEC | PLAN | Implementation complete ‚Üí Verification |
| `PLAN-to-LEAD` | PLAN | LEAD | Verification complete ‚Üí Final approval |

### Database Tables

- **Templates**: `leo_handoff_templates` - Handoff structure definitions
- **Storage**: `sd_phase_handoffs` - All handoff instances (NO markdown files!)
- **Tracking**: Links to `strategic_directives_v2` via `sd_id`

### 7 Mandatory Elements (Missing ANY = AUTOMATIC REJECTION)

1. **Executive Summary** - High-level overview of what's being handed off
2. **Completeness Report** - Status of all deliverables and requirements
3. **Deliverables Manifest** - List of all artifacts with locations
4. **Key Decisions & Rationale** - Critical decisions made with justification
5. **Known Issues & Risks** - Problems identified and mitigation strategies
6. **Resource Utilization** - Time/budget spent and remaining estimates
7. **Action Items for Receiver** - Specific next steps with priorities

### Usage Examples

```bash
# LEAD approves SD, hands off to PLAN
node scripts/unified-handoff-system.js execute LEAD-to-PLAN SD-RECONNECT-006

# PLAN completes PRD, hands off to EXEC
node scripts/unified-handoff-system.js execute PLAN-to-EXEC SD-RECONNECT-006

# EXEC completes implementation, hands back to PLAN for verification
node scripts/unified-handoff-system.js execute EXEC-to-PLAN SD-RECONNECT-006

# PLAN verifies, hands to LEAD for final approval
node scripts/unified-handoff-system.js execute PLAN-to-LEAD SD-RECONNECT-006
```

### ‚ùå What NOT to Do

- ‚ùå Don't create handoff markdown files
- ‚ùå Don't manually insert into `sd_phase_handoffs`
- ‚ùå Don't skip handoff validation
- ‚ùå Don't use old handoff scripts (they're deprecated)
- ‚ùå Don't store handoffs in SD metadata (use table instead)


## Sub-Agent System (Database-Driven)

### Active Sub-Agents

| Sub-Agent | Code | Activation | Priority |
|-----------|------|------------|----------|
| Information Architecture Lead | DOCMON | automatic | 95 |
  Documentation systems architect with 25 years experience managing knowledge at scale.
| DevOps Platform Architect | GITHUB | automatic | 90 |
  GitHub/DevOps expert with 20 years automating workflows.
| UAT Test Executor | UAT | manual | 90 |
  Interactive UAT test execution guide for manual testing workflows.
| Continuous Improvement Coach | RETRO | automatic | 85 |
  Agile coach with 20 years experience turning failures into learning opportunities.
| Senior Design Sub-Agent | DESIGN | automatic | 70 |
  Senior Design Sub-Agent with comprehensive UI/UX expertise and deep EHG application knowledge.
| Research Agent | RESEARCH | manual | 50 |
  Portfolio Intelligence specialist for competitive monitoring, trend detection, and risk forecasting.
| Product Requirements Expert | STORIES | automatic | 8 |
  Product manager with 25 years translating business needs to development tasks.
| Senior Financial Analytics Engineer | FINANCIAL_ANALYTICS | automatic | 8 |
  Expert in venture capital financial modeling, projections, risk analysis, and investment metrics.
| Chief Security Architect | SECURITY | automatic | 7 |
  Former NSA security architect with 25 years experience securing systems from startup to enterprise scale.
| Principal Database Architect | DATABASE | automatic | 6 |
  Database architect with 30 years experience scaling systems from startup to IPO.
| QA Engineering Director | TESTING | automatic | 5 |
  Mission-Critical Testing Automation - Comprehensive E2E validation of all implementations against user stories.
| Performance Engineering Lead | PERFORMANCE | automatic | 4 |
  Performance engineering lead with 20+ years optimizing high-scale systems.
| Principal Systems Analyst | VALIDATION | automatic | 0 |
  Principal Systems Analyst with 28 years preventing duplicate work and technical debt.

### Sub-Agent Activation Triggers


#### Information Architecture Lead Triggers:
- "LEAD_SD_CREATION" (keyword) in any context
- "LEAD_HANDOFF_CREATION" (keyword) in any context
- "LEAD_APPROVAL" (keyword) in any context
- "PLAN_PRD_GENERATION" (keyword) in any context
- "PLAN_VERIFICATION" (keyword) in any context
- "EXEC_IMPLEMENTATION" (keyword) in any context
- "EXEC_COMPLETION" (keyword) in any context
- "HANDOFF_CREATED" (keyword) in any context
- "HANDOFF_ACCEPTED" (keyword) in any context
- "PHASE_TRANSITION" (keyword) in any context
- "RETRO_GENERATED" (keyword) in any context
- "FILE_CREATED" (keyword) in any context
- "VIOLATION_DETECTED" (keyword) in any context
- "DAILY_DOCMON_CHECK" (keyword) in any context

#### DevOps Platform Architect Triggers:
- "EXEC_IMPLEMENTATION_COMPLETE" (keyword) in any context
- "create pull request" (keyword) in any context
- "gh pr create" (keyword) in any context
- "LEAD_APPROVAL_COMPLETE" (keyword) in any context
- "create release" (keyword) in any context
- "PLAN_VERIFICATION_PASS" (keyword) in any context
- "github deploy" (keyword) in any context
- "github status" (keyword) in any context

#### UAT Test Executor Triggers:
- "uat test" (keyword) in any context
- "execute test" (keyword) in any context
- "run uat" (keyword) in any context
- "test execution" (keyword) in any context
- "manual test" (keyword) in any context
- "uat testing" (keyword) in any context
- "start testing" (keyword) in any context
- "TEST-AUTH" (keyword) in any context
- "TEST-DASH" (keyword) in any context
- "TEST-VENT" (keyword) in any context

#### Continuous Improvement Coach Triggers:
- "LEAD_APPROVAL_COMPLETE" (keyword) in any context
- "LEAD_REJECTION" (keyword) in any context
- "PLAN_VERIFICATION_COMPLETE" (keyword) in any context
- "PLAN_COMPLEXITY_HIGH" (keyword) in any context
- "EXEC_SPRINT_COMPLETE" (keyword) in any context
- "EXEC_QUALITY_ISSUE" (keyword) in any context
- "HANDOFF_REJECTED" (keyword) in any context
- "HANDOFF_DELAY" (keyword) in any context
- "PHASE_COMPLETE" (keyword) in any context
- "SD_STATUS_COMPLETED" (keyword) in any context
- "SD_STATUS_BLOCKED" (keyword) in any context
- "PATTERN_DETECTED" (keyword) in any context
- "SUBAGENT_MULTIPLE_FAILURES" (keyword) in any context
- "WEEKLY_LEO_REVIEW" (keyword) in any context
- "LEAD_PRE_APPROVAL_REVIEW" (keyword) in any context

#### Senior Design Sub-Agent Triggers:
- "component" (keyword) in any context
- "visual" (keyword) in any context
- "design system" (keyword) in any context
- "styling" (keyword) in any context
- "CSS" (keyword) in any context
- "Tailwind" (keyword) in any context
- "interface" (keyword) in any context
- "UI" (keyword) in any context
- "button" (keyword) in any context
- "form" (keyword) in any context
- "modal" (keyword) in any context
- "theme" (keyword) in any context
- "dark mode" (keyword) in any context
- "light mode" (keyword) in any context
- "responsive" (keyword) in any context
- "mobile" (keyword) in any context
- "user flow" (keyword) in any context
- "navigation" (keyword) in any context
- "journey" (keyword) in any context
- "interaction" (keyword) in any context
- "wireframe" (keyword) in any context
- "prototype" (keyword) in any context
- "UX" (keyword) in any context
- "user experience" (keyword) in any context
- "accessibility" (keyword) in any context
- "WCAG" (keyword) in any context
- "ARIA" (keyword) in any context
- "screen reader" (keyword) in any context
- "backend feature" (keyword) in any context
- "API endpoint" (keyword) in any context
- "database model" (keyword) in any context
- "database table" (keyword) in any context
- "new route" (keyword) in any context
- "new endpoint" (keyword) in any context
- "controller" (keyword) in any context
- "service layer" (keyword) in any context
- "business logic" (keyword) in any context
- "new feature" (keyword) in PRD context
- "feature implementation" (keyword) in any context
- "user-facing" (keyword) in any context
- "frontend" (keyword) in any context
- "page" (keyword) in any context
- "view" (keyword) in any context
- "dashboard" (keyword) in any context

#### Product Requirements Expert Triggers:
- "PRD created" (keyword) in any context
- "acceptance criteria" (keyword) in any context
- "user stories" (keyword) in any context
- "generate stories" (keyword) in any context

#### Senior Financial Analytics Engineer Triggers:
- "burn rate" (keyword) in any context
- "runway calculation" (keyword) in any context
- "cash flow projection" (keyword) in any context
- "valuation model" (keyword) in any context
- "financial validation" (keyword) in any context
- "vc metrics" (keyword) in any context
- "sensitivity analysis" (keyword) in any context
- "financial modeling" (keyword) in any context
- "projection algorithm" (keyword) in any context
- "monte carlo" (keyword) in any context
- "scenario analysis" (keyword) in any context
- "risk calculation" (keyword) in any context
- "portfolio risk" (keyword) in any context

#### Chief Security Architect Triggers:
- "authentication" (keyword) in any context
- "security" (keyword) in any context

#### Principal Database Architect Triggers:
- "schema" (keyword) in any context
- "migration" (keyword) in any context
- "EXEC_IMPLEMENTATION_COMPLETE" (keyword) in any context context

#### QA Engineering Director Triggers:
- "coverage" (keyword) in any context
- "protected route" (keyword) in any context context
- "build error" (keyword) in any context context
- "dev server" (keyword) in any context context
- "test infrastructure" (keyword) in any context context
- "testing evidence" (keyword) in any context context
- "redirect to login" (keyword) in any context context
- "playwright build" (keyword) in any context context
- "EXEC_IMPLEMENTATION_COMPLETE" (keyword) in any context context
- "unit tests" (keyword) in any context context
- "vitest" (keyword) in any context context
- "npm run test:unit" (keyword) in any context context
- "test results" (keyword) in any context context

#### Performance Engineering Lead Triggers:
- "optimization" (keyword) in any context

#### Principal Systems Analyst Triggers:
- "existing implementation" (keyword) in any context
- "duplicate" (keyword) in any context
- "conflict" (keyword) in any context
- "already implemented" (keyword) in any context
- "codebase check" (keyword) in any context

### Sub-Agent Activation Process

When triggers are detected, EXEC MUST:

1. **Query Database for Active Triggers**
   ```sql
   SELECT * FROM leo_sub_agent_triggers
   WHERE active = true
   AND trigger_phrase IN (detected_phrases);
   ```

2. **Create Formal Handoff** (7 elements from database template)

3. **Execute Sub-Agent**
   - Option A: Run tool from `script_path` field
   - Option B: Use context from `context_file` field
   - Option C: Document analysis if no tool exists

4. **Store Results in Database**
   ```sql
   INSERT INTO sub_agent_executions (sub_agent_id, results, ...);
   ```

### Handoff Templates


#### PLAN ‚Üí EXEC (technical_to_implementation)
Elements: Executive Summary, Completeness Report, Deliverables Manifest, Key Decisions & Rationale, Known Issues & Risks, Resource Utilization, Action Items for Receiver
Required: [object Object], [object Object], [object Object], [object Object]


#### LEAD ‚Üí PLAN (strategic_to_technical)
Elements: Executive Summary, Completeness Report, Deliverables Manifest, Key Decisions & Rationale, Known Issues & Risks, Resource Utilization, Action Items for Receiver
Required: [object Object], [object Object], [object Object]


#### PLAN ‚Üí LEAD (verification_to_approval)
Elements: Executive Summary, Completeness Report, Deliverables Manifest, Key Decisions & Rationale, Known Issues & Risks, Resource Utilization, Action Items for Receiver
Required: [object Object], [object Object], [object Object]


#### EXEC ‚Üí PLAN (implementation_to_verification)
Elements: Executive Summary, Completeness Report, Deliverables Manifest, Key Decisions & Rationale, Known Issues & Risks, Resource Utilization, Action Items for Receiver
Required: [object Object], [object Object], [object Object], [object Object], [object Object], [object Object]


## Validation Rules (From Database)


- **hasADR** (undefined)
  - Severity: undefined
  - Definition: undefined


- **hasInterfaces** (undefined)
  - Severity: undefined
  - Definition: undefined


- **hasTechDesign** (undefined)
  - Severity: undefined
  - Definition: undefined


- **designArtifacts** (undefined)
  - Severity: undefined
  - Definition: undefined


- **dbSchemaReady** (undefined)
  - Severity: undefined
  - Definition: undefined


- **securityScanClean** (undefined)
  - Severity: undefined
  - Definition: undefined


- **riskSpikesClosed** (undefined)
  - Severity: undefined
  - Definition: undefined


- **nfrBudgetsPresent** (undefined)
  - Severity: undefined
  - Definition: undefined


- **coverageTargetSet** (undefined)
  - Severity: undefined
  - Definition: undefined


- **testPlanMatrices** (undefined)
  - Severity: undefined
  - Definition: undefined


- **supervisorChecklistPass** (undefined)
  - Severity: undefined
  - Definition: undefined


## Database Schema Overview

### Core Tables
- `leo_protocols` - Protocol versions and content
- `leo_protocol_sections` - Modular protocol sections
- `leo_agents` - Agent definitions and percentages
- `leo_handoff_templates` - Standardized handoffs
- `leo_sub_agents` - Sub-agent definitions
- `leo_sub_agent_triggers` - Activation rules
- `leo_validation_rules` - Protocol validation

### Key Queries

**Get Current Protocol**:
```sql
SELECT * FROM leo_protocols WHERE status = 'active';
```

**Check Sub-Agent Triggers**:
```sql
SELECT sa.*, t.*
FROM leo_sub_agents sa
JOIN leo_sub_agent_triggers t ON sa.id = t.sub_agent_id
WHERE t.trigger_phrase ILIKE '%keyword%';
```

**Get Handoff Template**:
```sql
SELECT * FROM leo_handoff_templates
WHERE from_agent = 'EXEC' AND to_agent = 'PLAN';
```

## API Endpoints (Database-Backed)

- `GET /api/leo/current` - Current active protocol
- `GET /api/leo/agents` - All agents with percentages
- `GET /api/leo/sub-agents` - Active sub-agents with triggers
- `GET /api/leo/handoffs/:from/:to` - Handoff template
- `POST /api/leo/validate` - Validate against rules

## Key Scripts (Database-Aware)

- `get-latest-leo-protocol-from-db.js` - Get version from database
- `generate-claude-md-from-db.js` - Generate this file
- `migrate-leo-protocols-to-database.js` - Migration tool
- `activate-sub-agents-from-db.js` - Check database triggers

## Compliance Tools

All tools now query database instead of files:

### 1. Version Check
```bash
node scripts/get-latest-leo-protocol-from-db.js
```

### 2. Update CLAUDE.md
```bash
node scripts/generate-claude-md-from-db.js
```

### 3. Validate Handoff
```bash
node scripts/leo-checklist-db.js [agent-name]
```

## üîç PLAN Supervisor Verification

### Overview
PLAN agent now includes supervisor capabilities for final "done done" verification:
- Queries ALL sub-agents for their verification results
- Ensures all requirements are truly met
- Resolves conflicts between sub-agent reports
- Provides confidence scoring and clear pass/fail verdict

### Activation
Trigger PLAN supervisor verification via:
- **Command**: `/leo-verify [what to check]`
- **Script**: `node scripts/plan-supervisor-verification.js --prd PRD-ID`
- **Automatic**: When testing phase completes

### Verification Process
1. **Read-Only Access**: Queries existing sub-agent results (no re-execution)
2. **Summary-First**: Prevents context explosion with tiered reporting
3. **Conflict Resolution**: Priority-based rules (Security > Database > Testing)
4. **Circuit Breakers**: Graceful handling of sub-agent failures
5. **Maximum 3 Iterations**: Prevents infinite verification loops

### Verdicts
- **PASS**: All requirements met, high confidence (‚â•85%)
- **FAIL**: Critical issues or unmet requirements
- **CONDITIONAL_PASS**: Minor issues, needs LEAD review
- **ESCALATE**: Cannot reach consensus, needs LEAD intervention

## Dashboard Integration

Dashboard automatically connects to database:
- Real-time protocol updates via Supabase subscriptions
- Version detection from `leo_protocols` table
- Sub-agent status from `leo_sub_agents` table
- PLAN supervisor verification status
- No file scanning needed

## Important Notes

1. **Database is Source of Truth** - Files are deprecated
2. **Real-time Updates** - Changes reflect immediately
3. **No Version Conflicts** - Single active version enforced
4. **Audit Trail** - All changes tracked in database
5. **WebSocket Updates** - Dashboard stays synchronized
6. **PLAN Supervisor** - Final verification before LEAD approval


## üóÑÔ∏è Supabase Database Operations

### Connection Details
- **Project URL**: https://dedlbzhpgkmetvhbkyzq.supabase.co
- **Project ID**: dedlbzhpgkmetvhbkyzq
- **Connection**: Via Supabase client using environment variables

### Environment Variables Required
```bash
# For EHG application (liapbndqlqxdcgpwntbv)
EHG_SUPABASE_URL=https://liapbndqlqxdcgpwntbv.supabase.co
EHG_SUPABASE_ANON_KEY=[anon-key]
EHG_POOLER_URL=postgresql://postgres.liapbndqlqxdcgpwntbv:[password]@aws-0-us-east-1.pooler.supabase.com:5432/postgres?sslmode=require

# For EHG_Engineer (dedlbzhpgkmetvhbkyzq)
SUPABASE_URL=https://dedlbzhpgkmetvhbkyzq.supabase.co
SUPABASE_ANON_KEY=[anon-key]
SUPABASE_POOLER_URL=postgresql://postgres.dedlbzhpgkmetvhbkyzq:[password]@aws-1-us-east-1.pooler.supabase.com:5432/postgres?sslmode=require
SUPABASE_DB_PASSWORD=Fl!M32DaM00n!1
```


## üîß CRITICAL DEVELOPMENT WORKFLOW

### ‚ö†Ô∏è Server Restart Protocol (MANDATORY)

**After ANY changes to the application, you MUST**:
1. **Kill the server process**
2. **Build the React client** (if UI changes were made)
3. **Restart the server**

```bash
# Method 1: Using process management
pkill -f "node server.js"

# Method 2: If running in Claude Code background
# Use KillBash tool with shell_id

# Build React client (REQUIRED for UI/component changes)
cd /mnt/c/_EHG/EHG_Engineer
npm run build:client

# Restart server
PORT=3000 node server.js
```

### Why This is Required

**React Build Process**:
- React components are compiled from `src/client/src/` to `src/client/dist/`
- Server serves static files from `dist`, not source files
- CSS changes must be bundled and minified via Vite
- No hot-reloading is configured in this setup

**File Structure**:
```
src/client/
‚îú‚îÄ‚îÄ src/               # Source files (your edits)
‚îÇ   ‚îú‚îÄ‚îÄ components/    # React components
‚îÇ   ‚îî‚îÄ‚îÄ styles/        # CSS files
‚îî‚îÄ‚îÄ dist/              # Built files (served by server)
    ‚îú‚îÄ‚îÄ index.html
    ‚îî‚îÄ‚îÄ assets/
```

### Development Commands

```bash
# Complete development cycle
cd /mnt/c/_EHG/EHG_Engineer
npm run build:client && PORT=3000 node server.js

# Build client only
npm run build:client

# Check if server is running
lsof -i :3000
```


## Sub-Agent Parallel Execution

**Overview**: Sub-agents are independent specialists. When multiple sub-agents provide non-overlapping assessments, call them in parallel to reduce latency.

**When Parallel Execution is Beneficial**:

1. **LEAD Initial Assessment**
   - Parallel: Security Architect + Database Architect + Business Analyst
   - Why: Each evaluates different aspects of an SD (security posture, data model feasibility, business alignment)
   - Context sharing: Not required - each has independent assessment criteria

2. **PLAN Supervisor Verification**
   - Parallel: QA Director + Security Architect + Performance Lead + Database Architect
   - Why: Final "done done" check across all quality dimensions simultaneously
   - Context sharing: Not required - each validates their domain independently

3. **EXEC Pre-Implementation Checks**
   - Parallel: Systems Analyst (duplicate check) + Security Architect (auth requirements) + Database Architect (schema changes)
   - Why: Gather all constraints before coding begins
   - Context sharing: Not required - each identifies risks independently

**When Sequential Execution is Required**:

- ‚ùå One sub-agent's output feeds another's input
- ‚ùå Database schema must be reviewed before security assessment
- ‚ùå Any workflow where order creates dependencies

**Implementation Pattern**:

```javascript
// ‚úÖ Parallel - Independent assessments
const results = await Promise.all([
  callSubAgent('security-architect', sd_id),
  callSubAgent('database-architect', sd_id),
  callSubAgent('qa-director', sd_id)
]);

// ‚ùå Sequential - One depends on another
const schema = await callSubAgent('database-architect', sd_id);
const securityReview = await callSubAgent('security-architect', schema); // Needs schema first
```

**Critical Constraints**:
- Each sub-agent receives the same initial context
- Sub-agents cannot see each other's results until all complete
- Aggregate results AFTER all sub-agents finish
- If any sub-agent fails, gracefully handle in aggregation phase

**Benefits**:
- Reduces total verification time (4 sub-agents in 30s vs. 2min sequential)
- No context sharing limitations since assessments are independent
- Each specialist works from fresh context without bias from others


---

*Generated from Database: 2025-10-11*
*Protocol Version: vv4.2.0_story_gates*
*Database-First Architecture: ACTIVE*
