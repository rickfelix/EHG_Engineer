# LEO Protocol Evolution Brainstorming: Maximally Complete Lifecycle

**Date**: 2026-01-10
**Participants**: Chairman (Human), Claude (Anthropic), OpenAI, Anti-Gravity
**Purpose**: Triangulate perspectives on evolving LEO Protocol to support a maximally complete AI-native venture lifecycle

---

## Original Vision Document (Chairman Input)

> Great question — this is exactly the right framing for **EHG**, because a *maximally complete lifecycle* is not "more steps," it's **more dimensions**: governance, intelligence, autonomy, learning, and portfolio-level control.
>
> Below is a **Maximally Complete Lifecycle** designed explicitly for **EHG's Chairman-led, AI-native venture studio**, where:
>
> * You are the **only human**
> * AI agents execute most work
> * Governance, traceability, and learning are first-class
> * Ventures are disposable, composable, and reusable
>
> This is the lifecycle you design **once**, then let run for years.

---

# The Maximally Complete EHG Venture Lifecycle

This lifecycle is **non-linear**, **multi-layered**, and **self-improving**.

I'll present it in **layers**, not just phases — because that's how EHG actually operates.

---

## LAYER 0 — Meta-Governance & System Integrity (Always On)

> *This layer exists before, during, and after every venture.*

**Purpose:** Ensure the system itself cannot drift, lie, or silently fail.

**Capabilities**

* Authority enforcement (who can do what)
* Escalation budgets & load-shedding
* Decision traceability
* Kill-switches & circuit breakers
* Policy enforcement (security, spend, risk)

**Artifacts**

* Governance policies
* Authority matrix
* Decision logs
* Audit trails

**This is the "operating system kernel."**

---

## PHASE 1 — Signal Ingestion & Opportunity Sensing

**Purpose:** Detect weak signals before competitors see them.

**Inputs**

* Market signals
* Tech capability shifts
* Cost curves (e.g., AI intelligence cost)
* Regulatory changes
* Internal capability reuse signals

**Agents**

* Signal scanners
* Trend synthesizers
* Noise filters

**Artifacts**

* Signal records
* Confidence-weighted hypotheses
* Time-sensitivity flags

➡️ Output feeds *Opportunity Blueprints*, not ventures.

---

## PHASE 2 — Opportunity Synthesis & Blueprinting

**Purpose:** Turn signals into structured venture hypotheses.

**Activities**

* Problem framing
* Market segmentation
* Differentiation hypotheses
* Capability reuse analysis (EHG synergies)
* Risk surface mapping

**Artifacts**

* Opportunity Blueprint
* Assumption graph
* Initial viability score

**Key Rule**

> No venture exists yet — only *options*.

---

## PHASE 3 — Multi-Agent Validation & Adversarial Review

**Purpose:** Stress-test ideas before capital, time, or attention is spent.

**Mechanisms**

* Competing agent perspectives (bull, bear, skeptic)
* Red-team audits
* Regulatory & ethical review
* Cost realism checks

**Artifacts**

* Validation reports
* Failure mode analysis
* Confidence deltas

**Outcome**

* Approve
* Revise
* Archive (learning retained)

---

## PHASE 4 — Chairman Gate: Venture Instantiation (Stage 0)

**Purpose:** Convert a blueprint into a real venture *deliberately*.

**Actions**

* Chairman approval
* Scope & ambition selection
* Autonomy level assignment
* Budget & risk envelope allocation

**Artifacts**

* Venture Constitution
* Stage 0 Inception Brief
* Authority & escalation profile

➡️ Only here does a **venture ID** get created.

---

## PHASE 5 — Structured Venture Lifecycle (25-Stage Engine)

This is the **execution spine**.

**Characteristics**

* Stage-based progression
* Hard, soft, and advisory gates
* Evidence-backed decisions
* Reversible until late stages

**Agents**

* Venture CEO (AI)
* Functional VPs
* Task-level crews

**Artifacts**

* Stage deliverables
* Assumption validation updates
* Cost & time metrics

**Chairman Touchpoints**

* Only at meaningful decision boundaries

---

## PHASE 6 — Continuous Quality, Risk & Reality Enforcement

**Purpose:** Prevent silent failure and illusion of progress.

**Always-On Checks**

* Assumption drift detection
* Token & cost overrun alerts
* Quality regression
* Data integrity checks
* Security posture validation

**Artifacts**

* Health scores
* Risk alerts
* Intervention recommendations

---

## PHASE 7 — Launch, Scaling & Market Feedback Integration

**Purpose:** Learn from reality, not forecasts.

**Activities**

* Controlled launches
* Instrumented usage
* Funnel & behavior analysis
* Cost-to-value monitoring

**Artifacts**

* Real performance metrics
* Unit economics
* Market truth deltas

➡️ Feeds back into venture and portfolio intelligence.

---

## PHASE 8 — Portfolio-Level Optimization & Capital Allocation

**Purpose:** Optimize EHG as a *system*, not individual ventures.

**Activities**

* Capital reallocation
* Capability reuse detection
* Shared service extraction
* Venture correlation analysis

**Artifacts**

* Portfolio dashboards
* Synergy maps
* ROI curves

---

## PHASE 9 — Exit, Shutdown, Spin-Out, or Absorption

**Purpose:** End ventures cleanly and profitably — emotionally detached.

**Exit Modes**

* Kill (learning retained)
* Spin-out
* Sell
* Absorb into platform
* Merge with another venture

**Artifacts**

* Exit report
* Capability transfer map
* Postmortem (blameless)

---

## PHASE 10 — System Learning & Evolution (Meta-Loop)

**Purpose:** Improve the machine itself.

**Learning Sources**

* What validated fastest?
* Where did agents hallucinate?
* Which signals mattered?
* Where governance saved you?
* Where it slowed you down?

**Outputs**

* Updated agent prompts
* Improved workflows
* Policy refinements
* Capability upgrades

➡️ This feeds back into **Layer 0** and **Phase 1**.

---

# Why This Is "Maximally Complete"

This lifecycle explicitly handles:

* ✅ Governance
* ✅ Autonomy
* ✅ Learning
* ✅ Portfolio dynamics
* ✅ Risk
* ✅ Cost realism
* ✅ Regulatory foresight
* ✅ Human attention as a scarce resource

Most companies **cannot run this** because they need:

* Too many humans
* Too much coordination
* Too much discipline

EHG can — because it's **AI-native by design**.

---

# The Key Insight

> **Minimum complete lifecycle** = build one good product
> **Maximum complete lifecycle** = build a machine that builds good products

EHG is the second.

---

---

# Triangulation Prompt for OpenAI and Anti-Gravity

**Instructions:** Copy everything below this line and send to each AI model independently.

---

## TRIANGULATION PROMPT — LEO Protocol Evolution

**Context:** I'm building EHG, an AI-native venture studio where I'm the only human (Chairman) and AI agents execute most work. I have a governance and execution protocol called LEO that I want to evolve.

---

### PART 1: Current LEO Protocol (v4.3.3)

**What LEO does today:**

LEO is a Strategic Directive (SD) execution protocol with three phases:
- **LEAD** - Strategic validation and approval (simplicity gatekeeping, risk assessment, scope definition)
- **PLAN** - PRD creation, sub-agent validation, quality gates
- **EXEC** - Implementation, dual testing (unit + E2E), deliverable tracking

**Key characteristics:**
- Database-first (all state in Supabase, no markdown files as source of truth)
- 20+ specialized sub-agents triggered by keywords (DATABASE, TESTING, DESIGN, SECURITY, etc.)
- Formal handoffs between phases using validation scripts
- "Russian Judge" AI-powered quality assessment (weighted rubrics, 70% threshold)
- Retrospectives capture lessons per SD
- Parent-child SD hierarchy for complex work decomposition

**What LEO assumes:**
- You already know what you want to build (SD exists)
- Work is bounded to individual SDs
- Quality gates happen at phase transitions (not continuous)
- Learning is captured but not automatically applied to system evolution

---

### PART 2: Chairman's Vision Document

Below is a document the Chairman created describing a "Maximally Complete Lifecycle" for an AI-native venture studio:

---

This lifecycle is **non-linear**, **multi-layered**, and **self-improving**.

**LAYER 0 — Meta-Governance & System Integrity (Always On)**
Purpose: Ensure the system itself cannot drift, lie, or silently fail.
Capabilities: Authority enforcement, escalation budgets, kill-switches, circuit breakers, policy enforcement
Artifacts: Governance policies, authority matrix, decision logs, audit trails

**PHASE 1 — Signal Ingestion & Opportunity Sensing**
Purpose: Detect weak signals before competitors see them.
Inputs: Market signals, tech capability shifts, cost curves, regulatory changes, internal capability reuse signals
Artifacts: Signal records, confidence-weighted hypotheses, time-sensitivity flags
Output feeds Opportunity Blueprints, not ventures.

**PHASE 2 — Opportunity Synthesis & Blueprinting**
Purpose: Turn signals into structured venture hypotheses.
Activities: Problem framing, market segmentation, differentiation hypotheses, capability reuse analysis, risk surface mapping
Artifacts: Opportunity Blueprint, assumption graph, initial viability score
Key Rule: No venture exists yet — only options.

**PHASE 3 — Multi-Agent Validation & Adversarial Review**
Purpose: Stress-test ideas before capital, time, or attention is spent.
Mechanisms: Competing agent perspectives (bull, bear, skeptic), red-team audits, regulatory & ethical review, cost realism checks
Artifacts: Validation reports, failure mode analysis, confidence deltas
Outcome: Approve, Revise, or Archive (learning retained)

**PHASE 4 — Chairman Gate: Venture Instantiation (Stage 0)**
Purpose: Convert a blueprint into a real venture deliberately.
Actions: Chairman approval, scope & ambition selection, autonomy level assignment, budget & risk envelope allocation
Artifacts: Venture Constitution, Stage 0 Inception Brief, authority & escalation profile
Only here does a venture ID get created.

**PHASE 5 — Structured Venture Lifecycle (25-Stage Engine)**
This is the execution spine.
Characteristics: Stage-based progression, hard/soft/advisory gates, evidence-backed decisions, reversible until late stages
Agents: Venture CEO (AI), Functional VPs, Task-level crews
Chairman Touchpoints: Only at meaningful decision boundaries

**PHASE 6 — Continuous Quality, Risk & Reality Enforcement**
Purpose: Prevent silent failure and illusion of progress.
Always-On Checks: Assumption drift detection, token & cost overrun alerts, quality regression, data integrity checks, security posture validation
Artifacts: Health scores, risk alerts, intervention recommendations

**PHASE 7 — Launch, Scaling & Market Feedback Integration**
Purpose: Learn from reality, not forecasts.
Activities: Controlled launches, instrumented usage, funnel & behavior analysis, cost-to-value monitoring
Artifacts: Real performance metrics, unit economics, market truth deltas
Feeds back into venture and portfolio intelligence.

**PHASE 8 — Portfolio-Level Optimization & Capital Allocation**
Purpose: Optimize EHG as a system, not individual ventures.
Activities: Capital reallocation, capability reuse detection, shared service extraction, venture correlation analysis
Artifacts: Portfolio dashboards, synergy maps, ROI curves

**PHASE 9 — Exit, Shutdown, Spin-Out, or Absorption**
Purpose: End ventures cleanly and profitably — emotionally detached.
Exit Modes: Kill (learning retained), spin-out, sell, absorb into platform, merge with another venture
Artifacts: Exit report, capability transfer map, postmortem (blameless)

**PHASE 10 — System Learning & Evolution (Meta-Loop)**
Purpose: Improve the machine itself.
Learning Sources: What validated fastest? Where did agents hallucinate? Which signals mattered? Where governance saved you? Where it slowed you down?
Outputs: Updated agent prompts, improved workflows, policy refinements, capability upgrades
This feeds back into Layer 0 and Phase 1.

**Chairman's Key Insight:**
"Minimum complete lifecycle = build one good product. Maximum complete lifecycle = build a machine that builds good products. EHG is the second."

---

### PART 3: What I'm Asking You

Please review both documents and provide your independent analysis:

1. **Assessment of current LEO Protocol** - What are its strengths and limitations for an AI-native venture studio?

2. **Assessment of the proposed vision** - What works, what's missing, what might be overengineered?

3. **Your recommendations** - How should LEO evolve? You're not bound by either document - suggest what you think is right.

4. **Risks and failure modes** - What could go wrong with either approach?

5. **Implementation considerations** - If changes are warranted, how would you approach them?

I'm triangulating across multiple AI models to get diverse perspectives. Please provide your honest, independent analysis.

---

## END OF TRIANGULATION PROMPT

---

## Responses

### Claude (Anthropic) Response

**Date:** 2026-01-10

---

#### 1. Assessment of Current LEO Protocol

**Strengths:**

- **Database-first architecture is correct.** This is a genuinely good architectural decision. State living in Supabase rather than scattered markdown files enables querying, auditing, and programmatic manipulation. Most AI-assisted development systems get this wrong.

- **Formal handoffs create accountability.** The LEAD→PLAN→EXEC structure with validation scripts forces deliberate transitions rather than sloppy "it's probably done" handwaving. This is valuable when AI agents are doing work that needs verification.

- **Sub-agent specialization works.** Having 20+ specialized agents (DATABASE, TESTING, SECURITY, etc.) triggered by keywords is pragmatic. It avoids the "one agent tries to do everything" antipattern.

- **Parent-child SD hierarchy enables decomposition.** Complex work can be broken into manageable pieces while maintaining traceability to the parent objective.

- **Retrospectives capture learning.** The fact that lessons are captured at all puts LEO ahead of most development processes.

**Limitations:**

- **LEO assumes the idea already exists.** The protocol starts with "you have an SD" — but where do SDs come from? This is a significant gap for a venture studio. The hardest part isn't execution; it's knowing what to execute.

- **Learning is captured but not applied.** Retrospectives go into a database, but there's no mechanism to automatically update agent prompts, workflows, or policies based on what was learned. The system has memory but not adaptation.

- **Quality gates are checkpoint-based, not continuous.** Problems are caught at handoffs, but silent degradation between handoffs goes undetected. A venture could be drifting off course for days before hitting a gate.

- **Single-venture focus.** LEO optimizes individual SD execution but has no concept of portfolio-level dynamics. It can't answer "should we stop this venture and reallocate resources to that one?"

- **No market feedback loop.** LEO ends when implementation is "complete." But "complete" doesn't mean "successful." There's no mechanism to learn from actual market performance.

- **Governance is implicit, not explicit.** Authority, escalation, and kill-switches exist informally but aren't codified. The Chairman makes decisions, but the decision-making framework isn't documented or enforceable.

---

#### 2. Assessment of the Proposed Vision

**What Works:**

- **Layer 0 (Meta-Governance) as a separate concern.** This is architecturally sound. Governance that exists "before, during, and after" ventures is different from governance within a venture. Separating it prevents governance from being an afterthought.

- **The "options before ventures" concept (Phases 1-3).** This is the most valuable addition. Blueprints that aren't ventures yet allow cheap exploration and deliberate instantiation. Most venture failures happen because people commit too early.

- **Adversarial validation (Phase 3).** Bull/bear/skeptic agents stress-testing ideas is genuinely useful. Current LEO has sub-agents that validate, but they don't compete or take adversarial stances.

- **Continuous monitoring (Phase 6).** Always-on drift detection is better than checkpoint-only validation. This catches problems earlier.

- **Explicit exit management (Phase 9).** Most systems pretend ventures don't die. Having explicit kill/spin-out/sell/absorb modes with learning retention is mature.

- **The meta-loop (Phase 10).** A system that improves itself is more valuable than a system that just runs.

**What's Missing:**

- **Human attention modeling.** The document mentions "human attention as a scarce resource" but doesn't operationalize it. How does the system know when to escalate vs. handle autonomously? What's the attention budget? How is it allocated?

- **Inter-venture dependencies.** Phase 8 mentions "venture correlation analysis" but doesn't address what happens when Venture A depends on a component from Venture B. Shared services and dependencies need explicit modeling.

- **Failure attribution.** When something goes wrong, was it the signal detection? The blueprinting? The execution? The system needs to attribute failures to phases so learning is targeted.

- **Time horizons.** Some signals matter in weeks, others in years. The lifecycle treats all opportunities similarly, but a "regulatory change in 6 months" is different from "AI costs will drop in 5 years."

**What Might Be Overengineered:**

- **25 stages might be too many.** Unless each stage has clear, distinct gates and deliverables, 25 stages could create administrative overhead without proportional value. The current 3-phase LEO is simple; jumping to 25 stages is a 8x increase in complexity.

- **The artifact explosion.** Each phase produces multiple artifacts (blueprints, assumption graphs, viability scores, constitutions, briefs, profiles...). Who consumes these? If artifacts aren't actively used, they become documentation theater.

- **Signal ingestion as a formal phase.** Detecting market signals is valuable, but making it Phase 1 of a formal lifecycle might over-structure something that should be opportunistic and continuous. Signals don't arrive on schedule.

---

#### 3. My Recommendations

**Don't replace LEO — extend it.**

LEO v4.3.3 is a working SD execution engine. The proposed vision is about what happens *around* SD execution. I'd recommend:

**A. Add a "Pre-LEO" layer for opportunity management**

- Create `opportunity_blueprints` table (separate from `strategic_directives_v2`)
- Blueprints have states: `signal`, `hypothesis`, `validated`, `instantiated`, `archived`
- Only `instantiated` blueprints become SDs
- Archived blueprints retain learning without becoming ventures

**B. Add adversarial validation to LEAD phase**

- Don't create a new Phase 3 — enhance existing LEAD
- Add `BULL_AGENT`, `BEAR_AGENT`, `SKEPTIC_AGENT` sub-agents
- Require adversarial review before LEAD approval for high-risk SDs
- Make it optional for low-risk/small SDs

**C. Implement continuous health monitoring**

- Add `sd_health_scores` table with daily automated checks
- Monitor: assumption drift, cost burn rate, timeline slippage, quality regression
- Alert Chairman only when thresholds crossed (not continuous noise)
- This is cheaper than formal Phase 6 — it's a background job, not a lifecycle phase

**D. Add portfolio view without portfolio "optimization"**

- Start simple: dashboard showing all active ventures with health scores
- Add capability tagging for reuse detection
- Defer complex capital reallocation algorithms until you have 5+ concurrent ventures
- Phase 8 as described is premature optimization for a studio with few ventures

**E. Make the meta-loop concrete**

- Quarterly: review retrospectives, extract patterns, update agent prompts
- This can be a scheduled process, not a formal lifecycle phase
- Track: which prompt changes improved outcomes? (A/B testing for agents)

**F. Codify governance explicitly**

- Authority matrix: who can kill a venture? Approve spend over $X? Override agent recommendations?
- Escalation rules: what triggers Chairman attention?
- This doesn't need to be Layer 0 — it can be a reference document that all phases consult

---

#### 4. Risks and Failure Modes

**Current LEO Risks:**

- **Execution theater.** Going through LEAD→PLAN→EXEC motions without actually validating that the work matters. Process compliance ≠ value creation.

- **Retrospective graveyard.** Lessons captured but never applied. The database fills with wisdom that doesn't change behavior.

- **Sub-agent sprawl.** 20+ agents becomes 40+ becomes unmanageable. Each agent needs maintenance, and keyword triggers can conflict or overlap.

**Proposed Vision Risks:**

- **Complexity collapse.** 10 phases + 1 layer + 25 stages = cognitive overload. The Chairman can't hold this in their head, and neither can the agents.

- **Premature portfolio optimization.** Building sophisticated capital allocation for 2-3 ventures is waste. The overhead exceeds the benefit until you have scale.

- **Meta-loop instability.** A system that updates itself can drift in unexpected directions. If Phase 10 updates Layer 0, and Layer 0 governs Phase 10, you have a circular dependency that could oscillate.

- **Signal noise.** Phase 1 assumes you can detect weak signals before competitors. In practice, most "signals" are noise. Over-investing in signal detection creates false confidence.

- **Artifact accumulation.** Every phase produces artifacts. After 2 years, you have thousands of blueprints, reports, and assessments that no one reads. Storage is cheap; attention is not.

**Both-System Risks:**

- **Chairman as single point of failure.** Both systems assume a competent, available Chairman. What happens during vacation? Illness? Attention elsewhere? There's no succession or delegation model.

- **AI capability assumptions.** Both systems assume AI agents can reliably execute complex judgment. Current AI is good but not infallible. Silent hallucination in a trusted agent could propagate errors through the system.

---

#### 5. Implementation Considerations

**If I were implementing changes, I would:**

**Phase 1 (Month 1-2): Governance Foundation**
- Document authority matrix and escalation rules
- Add `governance_policies` table
- Add kill-switch capability (soft-delete ventures with audit trail)
- This is low-risk, high-value, and enables everything else

**Phase 2 (Month 2-3): Pre-LEO Opportunity Layer**
- Add `opportunity_blueprints` table
- Build simple UI for blueprint creation and status tracking
- Connect blueprint instantiation to SD creation
- Don't add adversarial agents yet — just get the data model right

**Phase 3 (Month 3-4): Health Monitoring**
- Add `venture_health_scores` table
- Build nightly job that computes health metrics
- Add Chairman dashboard with alerts
- This provides continuous monitoring without lifecycle changes

**Phase 4 (Month 4-6): Adversarial Validation**
- Add BULL/BEAR/SKEPTIC agents to LEAD phase
- Make it mandatory for high-risk SDs, optional for others
- Evaluate: does this actually improve outcomes?

**Phase 5 (Month 6+): Portfolio View**
- Only after you have 3+ active ventures
- Start with visibility (dashboard), not optimization (allocation algorithms)
- Add capability tagging for reuse detection

**Defer Indefinitely:**
- 25-stage engine (current 3-phase is working)
- Sophisticated capital reallocation
- Automated agent prompt updates (do it manually first)

**Key Principle:** Each change should be independently valuable. Don't build infrastructure for a system you might not need. The proposed vision is a destination, not a starting point.

---

**Summary:**

Current LEO is a solid execution engine with real strengths. The proposed vision correctly identifies gaps (pre-venture intelligence, continuous monitoring, portfolio dynamics, meta-learning) but risks overengineering. My recommendation is incremental extension: add governance codification, opportunity blueprints, health monitoring, and adversarial validation — in that order — while deferring portfolio optimization and the 25-stage engine until scale justifies complexity.

### OpenAI Response

**Date:** 2026-01-10

**Note:** OpenAI reviewed the actual codebase (protocol tables, enforcement scripts, v4.4.2 governance docs) in addition to the prompt materials.

---

#### 1. Assessment of Current LEO (v4.x as implemented)

**Strengths:**

- **Real enforcement, not vibes**: Already has a "control plane" shape—phase gates, violations logging, compliance reports, and hard blockers (e.g., mandatory testing freshness gates in v4.4.2).

- **Evidence-backed quality**: Capturing test evidence + linking it to stories and retrospectives (the right direction for "reality > narration").

- **Database-centric workflow**: The protocol itself is versioned in DB (`leo_protocols`), and SDs have explicit phases, confidence, progress enforcement, and audit triggers.

- **Scales across decomposition**: Parent/child SD patterns and orchestrator SD types are the seed of a venture "work graph".

**Limitations (for an AI-native venture studio):**

- **SD-first assumes the "what" already exists**: Great for execution, weaker for *discovering* and *choosing* what to build next.

- **Continuous enforcement is partial**: Strong *transition gates*, but fewer always-on "drift/illusion detectors" at the venture/portfolio layer (cost overrun, assumption drift, stale metrics, security regression over time).

- **Source-of-truth ambiguity risk**: Some docs still imply PRDs live in markdown, while the system aspires to database-first. That split is a classic "silent divergence" failure mode.

- **Learning isn't automatically upstreamed**: Retrospectives exist, but "lessons → protocol/prompt changes" looks like a manual loop, not a governed pipeline.

---

#### 2. Assessment of the Proposed Vision

**What Works:**

- **Correct reframing**: From "execute SDs" → "operate a venture factory," with governance + sensing + validation + portfolio control.

- **Non-linear layering is right for AI systems**: Layer 0 (integrity) + Phase 6 (continuous enforcement) are the two most important upgrades.

- **Explicit adversarial review** before instantiation is a strong antidote to AI overconfidence.

**What's Missing / Underspecified:**

- **Operational definitions**: What counts as "evidence," how confidence is calibrated, and what data structures power "assumption graphs" and "health scores."

- **Budget mechanics**: Token/cost/time envelopes that actually bind agent behavior (and trigger escalation/kill-switches automatically).

- **Autonomy levels as enforceable policy**: "Authority matrix" needs a machine-checkable permission model tied to actions (create SD, modify scope, run migrations, deploy, spend budget, etc.).

- **Instrumentation standards** for Phase 7: Without a tight analytics spec, "market truth deltas" become narrative.

**What Might Be Overengineered:**

- The **25-stage engine** *if* it's mandatory or uniform. It's fine as a library of stages/gates, but dangerous as a rigid runway that every venture must traverse.

---

#### 3. Recommendations: How LEO Should Evolve

**Recast LEO as: "Kernel + Pipelines," not a single SD workflow**

- **LEO-Kernel (always on)**: Authority, budgets, audit/traceability, kill-switches, continuous monitors.
- **Pipeline A (Opportunity)**: Signals → Blueprints → Adversarial Validation → Chairman Gate.
- **Pipeline B (Delivery)**: Existing SD/PRD/handoff system (this stays—becomes the execution subsystem).
- **Pipeline C (Portfolio)**: Cross-venture resource allocation + capability reuse extraction.

**Add two missing primitives:**

- **Venture** (an entity above SD): Owns stages, budgets, autonomy profile, and a "work graph" of SDs.
- **Assumption + Evidence** (first-class): Assumptions are tracked objects; evidence attaches to assumptions and gates (not just "docs").

**Make learning a governed output:**

- Retrospectives should produce **proposed protocol/prompt changes** as artifacts that must pass review gates (so the system evolves safely, not via ad-hoc edits).

**Enforce one source of truth:**

- Pick **DB as canonical** (consistent with your intent), and treat markdown as generated/derived—otherwise drift will eventually defeat governance.

---

#### 4. Risks and Failure Modes (Both Approaches)

- **Process becomes the product**: The system optimizes for compliance scores and gates instead of market outcomes.

- **Metric gaming / "grade inflation"**: Agents learn what yields PASS, not what yields truth (especially with rubric-based judges).

- **False adversarialism**: Bull/bear/skeptic agents can still share the same blind spots if they're not grounded in real data and constraints.

- **Governance overload**: Too many required approvals increases latency; the human becomes a bottleneck again (the opposite of the goal).

- **Split-brain truth**: DB vs markdown vs "agent memory" diverge; decisions get made off stale artifacts.

- **Autonomy accidents**: Without a strict permissions model + budgets, agents can take irreversible actions (security, spend, data loss).

- **Portfolio thrash**: Frequent reprioritization without hysteresis leads to perpetual half-finished ventures.

---

#### 5. Implementation Considerations

**Do it incrementally, preserving your working SD engine:**

1. **Step 1**: Define minimal schemas for `signals`, `opportunity_blueprints`, `validation_reviews`, `ventures`, `venture_stages`, `assumptions`, `evidence`.

2. **Step 2**: Implement the **Chairman Gate** as a *hard boundary* that creates a Venture ID and assigns autonomy + budget envelope.

3. **Step 3**: Tie ventures to SDs via a parent "orchestrator SD" pattern (so existing execution tooling still works).

4. **Step 4**: Add **continuous monitors** (staleness, spend, test regression, assumption drift) that generate alerts/interventions automatically.

5. **Step 5**: Convert retrospectives into "learning proposals" that must pass gates before changing prompts/protocol scripts.

---

**Summary:**

OpenAI recommends recasting LEO as "Kernel + Pipelines" architecture with three distinct pipelines (Opportunity, Delivery, Portfolio) sharing a common always-on kernel. Key additions: Venture as first-class entity above SDs, Assumptions/Evidence as tracked objects, and governed learning proposals from retrospectives.

### Anti-Gravity Response

**Date:** 2026-01-10

---

#### 1. Assessment of Current LEO Protocol (v4.3.3)

**Strengths:**

- **Operational Rigor**: The mandatory script-based phase transitions (handoff.js) and database-first tracking prevent "state drift" and ensure every action is documented and validated.

- **Specialized Guardrails**: The use of 20+ specialized agents (DESIGN, DATABASE, SECURITY) ensures that domain-specific best practices are enforced without cognitive overload on the main orchestrator.

- **Simplicity Discipline**: The LEAD gate specifically interrogates SDs for over-engineering, which is the most common failure mode in AI-native development.

- **Traceable Learning**: The retrospective system (`retrospectives` table) creates a feedback loop that informs future PRDs, moving from "one-off task execution" to "systemic improvement."

**Limitations:**

- **Execution Isolation**: LEO is a "vending machine" for Strategic Directives. It processes what it's given but lacks the recursive ability to ask "Should this venture even exist?" or "What should the NEXT venture be?"

- **Static Rubrics**: The "Russian Judge" evaluation is a powerful gatekeeper, but if the rubrics don't evolve as fast as the venture's complexity, they risk becoming a compliance checkbox rather than a quality driver.

- **Latency in Quality Signals**: While gates are firm, they are discrete. A "silent failure" in the early stages of implementation isn't caught until the full execution loop completes, leading to expensive "rewind" cycles.

- **Missing "Pre-Genesis" Intelligence**: The current protocol assumes an SD is the starting point. It has no mechanism for "Sensing" or "Sifting" through market/tech signals before they become directives.

---

#### 2. Assessment of the Proposed Vision

**What Works:**

- **Signal-to-Venture Pipeline**: Phases 1-3 create a "Strategic Sandbox" where ideas are killed or refined before a single line of production code is written. This is most important for a solo operator (Chairman).

- **Adversarial Tension**: Phase 3 (Multi-Agent Adversarial Review) is the "killer feature." Using competing Bull/Bear/Skeptic agents replaces the need for human debate and stress-tests hypotheses more effectively than a single gatekeeper.

- **Meta-Governance (Layer 0)**: Building "authority enforcement" and "kill-switches" into the foundation ensures that as agents become more autonomous, they remain aligned with the Chairman's strategic intent.

- **Venture-Level Awareness**: The vision moves from "managing tasks" to "managing a portfolio," allowing for cross-venture learning and capability reuse (Phase 8).

**Missing or Overengineered:**

- **Stage-Engine Rigidity (Phase 5)**: A mandatory "25-stage engine" risks becoming a "Factory of Sameness." Some ventures need 5 stages; others need 50. The protocol must allow for orchestration elasticity.

- **Continuous Risk Monitoring (Phase 6)**: This is technically challenging and computationally expensive. "Assumption drift detection" requires a high degree of semantic monitoring that might yield low signal-to-noise ratios if not restricted to critical milestones.

- **The "Exit" Paradox (Phase 9)**: AI agents are naturally "building-biased." Implementing an "emotional detachment" exit mode requires a specific incentive structure for shutdown-agents to ensure they don't hallucinate reasons for survival.

---

#### 3. Evolution Recommendations

**Short-Term (v4.5): "Contextual Soul"**

- **Venture Constitution**: Every SD must map to a `venture_id`. The LEAD gate should verify that the SD is "on-brand" for that specific venture's constitution.

- **Continuous Verification**: Move from "Phase-Gate" to "Checkpointing." Agents should run mini-validations after every significantly changed component, catching errors while the context is still fresh.

**Mid-Term (v5.0): "The Strategic Sandbox"**

- **Blueprint Object**: Introduce a `blueprints` table in the database. These are "Pre-SDs" that agents can generate, debate (Adversarial Review), and iterate on before Chairman approval.

- **Signal Ingestion Agent**: A passive agent that scans external feeds (market trends, tech shifts) and proposes Blueprints, effectively automating Phase 1 of the vision.

**Long-Term (v6.0): "The Meta-Loop"**

- **Automated Protocol Evolution**: Use the retrospectives from Phase 10 to automatically update the prompts and rubrics of the LEAD and PLAN agents. If a specific "Database Pattern" consistently fails, the system should rewrite its own "Database Skill" without human intervention.

---

#### 4. Risks & Failure Modes

- **Recursive Hallucination**: In a "Meta-Loop" environment, a flaw in the evaluation agent could lead to it approving its own mistakes and then "learning" from those mistakes to lower the bar further.

- **The "Yes-Agent" Echo Chamber**: If Bull, Bear, and Skeptic agents all use the same underlying model (e.g., GPT-4o), their "competing perspectives" may just be variations of the same systemic bias.

- **Chairman Detachment**: If the system becomes too good at Blueprinting and Execution, the Chairman may lose "Ground Truth" awareness, making the "Kill Switch" harder to trigger when things go wrong at a fundamental layer.

---

#### 5. Implementation Considerations

**Database Expansion:**
- The current consolidated schema is the perfect foundation
- Start by adding `signals`, `blueprints`, and `venture_constitutions` tables

**Adversarial Agent Framework:**
- Create a new `subagent_type` for "Advocate" and "Skeptic"
- These don't write code; they only write "Critique Reports" for the LEAD phase

**Phased Transition:**
- **Alpha**: Apply "Adversarial Review" to existing SDs
- **Beta**: Launch "Signal Ingestion" to generate future SDs
- **Stable**: Full 10-phase lifecycle with autonomous Meta-Loop

---

**Verdict:**

The Chairman's vision is the necessary "Operating System" for EHG to scale beyond a single person. LEO v4.3.3 is a high-performance compiler for software; the proposed vision transforms it into an autonomous venture strategist. The transition should focus on **automated blueprinting** and **adversarial stress-testing** as the immediate priorities.

---

## Synthesis Grid

*Completed: 2026-01-10*

| Topic | Claude | OpenAI | Anti-Gravity | Consensus |
|-------|--------|--------|--------------|-----------|
| **Current LEO strengths** | Database-first, formal handoffs, sub-agent specialization | Real enforcement, evidence-backed, DB-centric workflow | Operational rigor, specialized guardrails, simplicity discipline | **HIGH**: All agree LEO has solid execution foundation |
| **Current LEO limitations** | No pre-venture layer, learning not applied, checkpoint-only | SD-first assumes "what" exists, source-of-truth ambiguity, learning not upstreamed | "Vending machine" isolation, static rubrics, missing pre-genesis intelligence | **HIGH**: All agree LEO lacks upstream (pre-SD) capability |
| **Vision strengths** | Options-before-ventures, adversarial validation, continuous monitoring | Correct reframing to "venture factory", Layer 0 + Phase 6 most important | "Strategic Sandbox" (Phases 1-3), adversarial tension is "killer feature" | **HIGH**: All agree adversarial validation + pre-venture phases are valuable |
| **Vision concerns** | 25 stages too complex, artifact explosion | 25-stage if mandatory/uniform, missing operational definitions | Stage-engine rigidity, Phase 6 computationally expensive, "Exit Paradox" | **HIGH**: All agree 25-stage engine is risky if rigid |
| **Top recommendation** | Extend LEO incrementally (governance → blueprints → monitoring → adversarial) | "Kernel + Pipelines" architecture, Venture as entity above SD | v4.5 "Contextual Soul" → v5.0 "Strategic Sandbox" → v6.0 "Meta-Loop" | **MEDIUM**: All agree incremental, but differ on architecture scope |
| **Key risk identified** | Complexity collapse, Chairman as SPOF | Process becomes product, metric gaming, split-brain truth | Recursive hallucination, "Yes-Agent" echo chamber, Chairman detachment | **MEDIUM**: Different failure modes identified - complementary concerns |
| **Implementation priority** | 1) Governance 2) Blueprints 3) Health monitoring 4) Adversarial agents | 1) Schemas 2) Chairman Gate 3) Venture→SD link 4) Continuous monitors | 1) Adversarial Review on existing SDs 2) Signal Ingestion 3) Full lifecycle | **MEDIUM**: All prioritize adversarial + blueprints, differ on governance timing |

---

## Consensus Analysis

### Strong Consensus (All 3 Models Agree)

1. **Current LEO is a solid execution engine** - Don't throw it away; extend it.

2. **Pre-venture intelligence is the biggest gap** - Signals → Blueprints → Validation before SDs exist.

3. **Adversarial validation is high-value** - Bull/Bear/Skeptic agents stress-testing ideas before Chairman approval.

4. **25-stage engine is risky if rigid** - Should be a library/menu, not a mandatory runway.

5. **Continuous monitoring beats checkpoint-only** - Catch drift earlier, not just at handoffs.

6. **Database must be canonical source of truth** - Eliminate markdown/DB split-brain risk.

7. **Incremental implementation** - Don't big-bang; preserve working SD engine.

### Partial Consensus (2 of 3 Agree)

1. **Venture as explicit entity above SD** (OpenAI, Anti-Gravity agree; Claude less emphatic)
   - OpenAI: "Venture owns stages, budgets, autonomy profile"
   - Anti-Gravity: "Every SD must map to a venture_id"
   - Claude: Mentioned but not prioritized

2. **Governed learning proposals** (OpenAI, Anti-Gravity agree; Claude suggests manual quarterly review)
   - OpenAI: "Retrospectives produce proposed protocol changes that pass gates"
   - Anti-Gravity: "Automated Protocol Evolution" in v6.0
   - Claude: "Quarterly manual review" - more conservative

3. **Budget/autonomy enforcement** (OpenAI, Anti-Gravity emphasize; Claude doesn't)
   - OpenAI: "Token/cost/time envelopes that bind agent behavior"
   - Anti-Gravity: "Venture Constitution" with autonomy levels
   - Claude: Not explicitly addressed

### Divergence (Models Disagree)

1. **Architecture Scope**
   - Claude: Minimal extension - add tables, enhance existing phases
   - OpenAI: "Kernel + 3 Pipelines" - more formal restructuring
   - Anti-Gravity: Versioned evolution (v4.5 → v5.0 → v6.0)

2. **Governance Timing**
   - Claude: Governance first (Month 1-2)
   - OpenAI: Schemas first, governance embedded
   - Anti-Gravity: Adversarial review first (Alpha phase)

3. **Meta-Loop Autonomy**
   - Claude: Conservative - manual quarterly review, defer automation
   - OpenAI: Governed but automated - learning proposals with review gates
   - Anti-Gravity: Most aggressive - "system should rewrite its own skills"

---

## Unique Insights by Model

### Claude (Unique Contributions)
- **Human attention modeling** as missing element
- **Failure attribution** - need to know which phase caused problems
- **Time horizons** - signals have different urgency levels

### OpenAI (Unique Contributions)
- **Assumption + Evidence as first-class objects** (not just docs)
- **Instrumentation standards for Phase 7** - without specs, "market truth deltas" become narrative
- **Portfolio thrash risk** - reprioritization without hysteresis

### Anti-Gravity (Unique Contributions)
- **"Yes-Agent" Echo Chamber** risk - same model = same blind spots
- **"Exit Paradox"** - AI agents are building-biased, may hallucinate survival reasons
- **"Checkpointing" vs Phase-Gate** - mini-validations after each component change
- **Chairman Detachment** risk - system too good = human loses ground truth

---

## Recommended Action Plan (Triangulated)

Based on consensus across all three models:

### Phase 1: Foundation (Immediate)
- [ ] Add `blueprints` table (all 3 agree this is priority)
- [ ] Add `venture_constitutions` table with `venture_id` (OpenAI + Anti-Gravity)
- [ ] Document authority matrix and escalation rules (Claude)
- [ ] Ensure DB is canonical - eliminate markdown PRD ambiguity (OpenAI)

### Phase 2: Adversarial Validation (High Priority)
- [ ] Create BULL, BEAR, SKEPTIC sub-agents (all 3 agree)
- [ ] Apply to existing SDs first (Anti-Gravity: "Alpha" phase)
- [ ] Use different underlying models to avoid echo chamber (Anti-Gravity insight)
- [ ] Require adversarial review for high-risk SDs (Claude)

### Phase 3: Continuous Monitoring (Medium Priority)
- [ ] Add health scoring with automated checks (all 3 agree)
- [ ] Implement "checkpointing" - mini-validations during EXEC (Anti-Gravity)
- [ ] Alert on threshold breaches, not continuous noise (Claude)
- [ ] Monitor: cost burn, assumption drift, quality regression (OpenAI)

### Phase 4: Signal Ingestion (Later)
- [ ] Add `signals` table (OpenAI + Anti-Gravity)
- [ ] Passive agent scanning external feeds (Anti-Gravity)
- [ ] Connect signals → blueprints → Chairman Gate → SDs

### Defer Until Scale Justifies
- 25-stage engine (all 3 agree to defer)
- Sophisticated portfolio optimization (Claude)
- Fully automated meta-loop (all 3 agree - too risky early)

---

## Chairman Decision Points

These items had divergent opinions and require Chairman judgment:

1. **Architecture: Extend vs Restructure?**
   - Option A (Claude): Minimal extension - add tables to existing structure
   - Option B (OpenAI): "Kernel + Pipelines" formal restructuring
   - Option C (Anti-Gravity): Versioned evolution with named releases
   - **DECISION: Option A - Minimal Extension**

2. **Meta-Loop Autonomy Level?**
   - Option A (Claude): Manual quarterly review
   - Option B (OpenAI): Governed automation with review gates
   - Option C (Anti-Gravity): Full automation ("system rewrites its own skills")
   - **DECISION: Keep current process - no changes needed**

3. **When to Add Venture Entity?**
   - Option A: Now (OpenAI + Anti-Gravity recommend)
   - Option B: Later when 3+ ventures exist (Claude's implicit position)
   - **DECISION: Use existing `target_application` field - no new entity needed**

4. **Adversarial Agent Model Diversity?**
   - Option A: Use same model (simpler)
   - Option B: Use different models to avoid echo chamber (Anti-Gravity recommends)
   - **DECISION: Same model (Claude), different prompts with differentiated personas**

---

## Next Steps

- [x] ~~Send triangulation prompt to OpenAI (ChatGPT)~~
- [x] ~~Send triangulation prompt to Anti-Gravity~~
- [x] ~~Collect responses and add to this document~~
- [x] ~~Complete synthesis grid~~
- [x] ~~Identify consensus vs. divergence~~
- [x] ~~Chairman decides on 4 divergent areas~~
- [ ] Create implementation roadmap based on decisions
- [ ] Add `blueprints` table to database
- [ ] Create BULL, BEAR, SKEPTIC sub-agents with differentiated prompts
- [ ] Add continuous health monitoring

---

## Final Implementation Plan (Based on Decisions)

Given the Chairman's decisions (minimal extension, current meta-loop, existing target_application, same model with different prompts), the implementation plan is:

### Priority 1: Blueprints Table
- Add `opportunity_blueprints` table
- States: `signal`, `hypothesis`, `validated`, `instantiated`, `archived`
- Links to `target_application` for venture association
- Chairman Gate creates SD from approved blueprint

### Priority 2: Adversarial Agents
- Create BULL_AGENT, BEAR_AGENT, SKEPTIC_AGENT sub-agents
- All use Claude with strongly differentiated system prompts
- Apply to high-risk SDs during LEAD phase
- Output: Critique Reports informing Chairman approval

### Priority 3: Continuous Health Monitoring
- Add `sd_health_scores` table
- Nightly automated checks: cost burn, timeline, quality regression
- Alert Chairman only on threshold breaches
- Checkpoint validation during EXEC (not just at handoffs)

### Deferred
- 25-stage engine (keep current 3-phase)
- Portfolio optimization algorithms
- Automated meta-loop
- New Venture entity (use target_application)

---

*Document created: 2026-01-10*
*Last updated: 2026-01-10*
*Triangulation completed: 2026-01-10*
*Chairman decisions recorded: 2026-01-10*
*Status: Ready for Round 2 triangulation*

---

# Round 2 Triangulation

## TRIANGULATION PROMPT — Round 2: LEO Protocol & Venture Workflow Deep Dive

**Instructions:** Copy everything below this line and send to each AI model independently.

---

**Context:** I'm building EHG, an AI-native venture studio where I'm the only human (Chairman) and AI agents execute most work.

**Round 1 Decisions (Already Planned):**
- Adding `opportunity_blueprints` table for pre-SD ideation
- Creating BULL/BEAR/SKEPTIC adversarial agents for LEAD phase validation
- Implementing continuous health monitoring with checkpoint validation
- Keeping minimal extension architecture (no major restructuring)
- Using existing `target_application` field for venture association

**Your Task:** Identify additional improvement opportunities we haven't addressed yet.

---

### PART A: LEO Protocol Analysis

LEO is our Strategic Directive execution protocol with three phases:
- **LEAD** - Strategic validation, simplicity gatekeeping, risk assessment
- **PLAN** - PRD creation, sub-agent validation, quality gates
- **EXEC** - Implementation, dual testing (unit + E2E), deliverable tracking

**Current Characteristics:**
- Database-first (all state in Supabase)
- 20+ specialized sub-agents triggered by keywords
- Formal handoffs between phases using validation scripts
- "Russian Judge" AI-powered quality assessment (weighted rubrics, 70% threshold)
- Parent-child SD hierarchy for work decomposition

**Please analyze and recommend improvements for:**

1. **Phase Mechanics** - Are LEAD→PLAN→EXEC transitions optimal? What happens when gates fail? Should there be different gate thresholds for different SD types?

2. **Sub-Agent System** - Are 20+ agents the right number? Are there missing agent types? How should agents handle conflicts? How do we prevent agent sprawl?

3. **Governance & Enforcement** - What can the Chairman safely delegate vs. must retain? How should escalation work? What are the right kill-switch triggers?

4. **Handoff Quality** - Are the current handoff scripts capturing the right information? What gets lost in transitions?

5. **Any other LEO improvements** you identify that we haven't considered.

---

### PART B: 25-Stage Venture Workflow Analysis

We have a 25-stage venture lifecycle engine that ventures progress through:

**Ideation & Validation (Stages 1-5)**
- Problem sensing, opportunity framing, market validation

**Planning & Design (Stages 6-10)**
- Business model, product specification, technical architecture

**Build & Test (Stages 11-17)**
- MVP development, iteration, quality assurance

**Launch & Scale (Stages 18-22)**
- Go-to-market, growth, optimization

**Maturity & Exit (Stages 23-25)**
- Steady state, exit planning, transition

**Current Characteristics:**
- Hard, soft, and advisory gates between stages
- Stage-specific deliverables and evidence requirements
- Venture CEO (AI) manages day-to-day, Chairman at decision boundaries
- Ventures can pause, regress, or terminate at any stage

**Please analyze and recommend improvements for:**

1. **Stage Granularity** - Is 25 the right number? Should some stages merge? Should some split?

2. **Gate Types** - Is the hard/soft/advisory distinction working? What determines which type?

3. **Stage Flexibility** - How do we avoid "Factory of Sameness"? Should different venture types have different stage paths?

4. **Exit Mechanisms** - How do we overcome AI "building bias"? What triggers should force exit consideration?

5. **Any other workflow improvements** you identify.

---

### PART C: Integration Points

How should LEO Protocol (SD execution) and the 25-Stage Workflow (venture lifecycle) connect?

- When does an SD map to a stage vs. span multiple stages?
- How do stage transitions trigger LEAD phase reviews?
- Should there be venture-level gates in addition to SD-level gates?

---

Please provide your independent analysis. I'm triangulating across multiple AI models.

---

## END OF ROUND 2 TRIANGULATION PROMPT

---

## Round 2 Responses

### Claude (Anthropic) Response

**Date:** 2026-01-10

---

#### PART A: LEO Protocol Analysis

##### 1. Phase Mechanics

**Current Assessment**: The LEAD→PLAN→EXEC→PLAN_VERIFY→COMPLETE cycle is fundamentally sound. The gate system with Russian Judge (70% threshold) provides quality enforcement. However, I see these gaps:

**Recommendations:**

**A. Gate Failure Cascade Protocol**
Currently, when gates fail, the system loops but lacks escalation memory. After 2 gate failures on the same SD, the system should:
- Auto-escalate to Chairman with failure pattern analysis
- Recommend SD decomposition if complexity is the issue
- Block 3rd attempt without explicit Chairman override

**B. SD-Type Adaptive Thresholds**
The protocol already has `sd_type` (feature, infrastructure, documentation, database, security), but gate thresholds are uniform. Recommend:

| SD Type | Russian Judge | Testing | Documentation |
|---------|---------------|---------|---------------|
| feature | 70% | Full E2E | Standard |
| infrastructure | 60% | CI validation | High (protocol docs) |
| documentation | 50% | N/A | Self-documenting |
| security | 85% | Full + penetration | High |

**C. Phase Re-entry Conditions**
Add explicit "return conditions" for loopbacks:
- EXEC→PLAN (verification failure): Must document what changed in PRD
- PLAN→LEAD (scope change): Must justify why not new SD
- Any regression: Auto-trigger retrospective before re-entry

##### 2. Sub-Agent System

**Current Assessment**: 20+ agents create maintenance burden and potential overlap. I identified:

**Agent Gaps (Missing):**
- **CONTEXT-AGENT**: Monitor token usage, auto-recommend compaction, track cross-session continuity
- **DEPENDENCY-GRAPH-AGENT**: Track SD dependencies, predict cascade impacts, identify parallelizable work
- **COST-AGENT**: Monitor API costs, token burn rate, recommend efficiency optimizations

**Agent Overlap (Candidates for Merge):**
- REGRESSION + VALIDATION → Both check for backward compatibility
- RISK + SECURITY → Significant overlap in threat assessment
- STORIES + UAT → User story and acceptance testing are tightly coupled

**Conflict Resolution Protocol (Missing):**
When two agents produce conflicting verdicts (e.g., DESIGN says "add loading state", PERFORMANCE says "remove loading state"):
1. Both verdicts logged with confidence scores
2. Higher-confidence verdict prevails by default
3. Equal confidence → Chairman decision required
4. Add `conflict_resolution_log` table for learning

**Agent Sprawl Prevention:**
- **Annual Agent Audit**: Review invocation frequency, merge <5% usage agents
- **Agent Lifecycle**: New agents start as "experimental" (3-month trial), must prove value
- **Maximum Agent Count**: Cap at 25 (currently at ~22), require deprecation for new additions

##### 3. Governance & Enforcement

**Chairman Delegation Matrix:**

| Decision Type | Delegable? | Kill-Switch? |
|---------------|------------|--------------|
| SD Approval (LEAD) | No | Yes - can reject any time |
| Gate Override | No | Yes - can force-pass with audit |
| Scope Change | No | N/A |
| Quick Fix Approval | Yes (auto-approve if <20 LOC, no security) | No |
| Rebaseline | No | Yes |
| Emergency Bypass | No | Yes - logged |

**Escalation Protocol (Currently Implicit, Should Be Explicit):**
```
Level 0: Sub-agent handles autonomously
Level 1: Main agent (Claude) decides based on protocol
Level 2: Chairman notification (async, informational)
Level 3: Chairman decision required (sync, blocking)
Level 4: Emergency stop (all work paused)
```

**Kill-Switch Triggers (Recommend Codifying):**
- Security sub-agent finds HIGH severity issue
- Cost burn exceeds 200% of SD estimate
- 3+ consecutive gate failures
- Detected pattern violation (e.g., bypassing handoff scripts)
- Any change to auth/RLS without explicit approval

##### 4. Handoff Quality

**What Gets Lost:**
- **Implicit context**: Why decisions were made (not just what)
- **Dead-end exploration**: What was tried and failed
- **Session assumptions**: Things the human said verbally but didn't document
- **Cross-SD learning**: Lessons from related SDs that inform this one

**Recommendations:**

**A. Structured Context Fields in Handoffs:**
```sql
ALTER TABLE sd_phase_handoffs ADD COLUMN IF NOT EXISTS
  exploration_summary TEXT,     -- What was researched
  dead_ends JSONB,              -- Approaches tried and rejected
  implicit_assumptions JSONB,   -- Things assumed but not in PRD
  related_sd_learnings TEXT;    -- Lessons from other SDs
```

**B. Handoff Narration Requirement:**
Each handoff must include a 2-3 sentence "context narration" that captures WHY, not just WHAT:
> "This SD is implementing Stage 7 for the VentureAssist venture. We considered using the existing validation framework but it doesn't support async validation, so we're building a new ValidatorService. The Chairman mentioned wanting this to be reusable for other ventures."

**C. Session Continuity Token:**
When a session ends mid-work, generate a "continuation prompt" that can be pasted into the next session.

##### 5. Other LEO Improvements

**A. Predictive Blocking:**
Before starting an SD, query:
- Is the branch stale? (>7 days)
- Are dependencies complete?
- Are required agents available?
- Is context budget sufficient for estimated scope?

Block proactively instead of discovering issues mid-work.

**B. SD Health Score Dashboard:**
Real-time health metrics visible to Chairman:
- Gate passage rate (%)
- Time in current phase
- Cost burn vs estimate
- Sub-agent verdict summary
- Blockers and risks

**C. Learning Loop Automation:**
After each SD completion:
1. Auto-extract patterns from retrospective
2. Match against existing patterns
3. If new: propose for `leo_patterns` table
4. If reinforcing: increment pattern confidence

---

#### PART B: 25-Stage Venture Workflow Analysis

##### 1. Stage Granularity

**Assessment**: 25 stages is appropriate for complex ventures but excessive for simple ones.

**Recommendations:**

**A. Venture Archetype Profiles:**

| Archetype | Stages Used | Description |
|-----------|-------------|-------------|
| `rapid-prototype` | 5 (1,3,11,17,18) | Hackathon-style validation |
| `lean-startup` | 12 (1-5, 11-15, 18-19) | Standard MVP approach |
| `enterprise` | 25 (all) | Full rigor for complex products |
| `spin-off` | 8 (skip ideation, start at 6) | Leveraging existing EHG IP |

**B. Stage Compression Rules:**
- If venture score <0.3 after Stage 3, offer fast-fail path (jump to Stage 25)
- If no external dependencies, compress Stages 6-10 into single "Design" stage
- If technical risk is low, compress Stages 11-17 into "Build" and "Validate"

**C. Stage Skipping (With Audit):**
Allow skipping stages with Chairman approval and documented justification.

##### 2. Gate Types

**Current State**: Hard (blocking), Soft (warning), Advisory (informational)

**Proposed Gate Type Selection Criteria:**

| Criterion | Hard | Soft | Advisory |
|-----------|------|------|----------|
| Financial commitment follows | ✓ | | |
| Irreversible decision | ✓ | | |
| External stakeholders affected | ✓ | | |
| Internal process checkpoint | | ✓ | |
| Best practice reminder | | | ✓ |
| Resource-intensive next stage | | ✓ | |
| Learning opportunity | | | ✓ |

**Dynamic Gate Hardening:**
Start gates as "soft" and harden them based on venture risk profile.

##### 3. Stage Flexibility

**"Factory of Sameness" Prevention:**

**A. Venture Personality Assessment (at Stage 1):**
```json
{
  "risk_tolerance": "high|medium|low",
  "speed_priority": "fast|balanced|thorough",
  "innovation_type": "incremental|disruptive|sustaining",
  "market_timing": "urgent|standard|patient"
}
```

This profile adjusts gate strictness, required deliverables, stage duration expectations, and exit evaluation frequency.

**B. Parallel Stage Exploration:**
For ventures with high uncertainty, allow exploring 2-3 approaches simultaneously through Stage 5, then converge based on evidence.

**C. Stage-Specific AI Personas:**
- Stages 1-5: Exploratory, questioning, divergent
- Stages 6-10: Analytical, structured, convergent
- Stages 11-17: Execution-focused, detail-oriented
- Stages 18-22: Growth-oriented, experiment-driven
- Stages 23-25: Realistic, objective, exit-focused

##### 4. Exit Mechanisms

**The Building Bias Problem:**
AI agents are optimized for task completion. "Continue building" is always the default recommendation.

**Recommendations:**

**A. Mandatory Exit Evaluation Points:**
- After Stage 5 (validation complete)
- After Stage 10 (design complete)
- After Stage 15 (MVP built)
- After Stage 20 (initial traction assessed)
- Every 90 days regardless of stage

**B. External Exit Forcing Function:**
Quarterly "Venture Defense" where each venture must justify continuation:
- Metric: "Would we start this today knowing what we know now?"
- If answer is "no" twice consecutively → sunset process begins

**C. Sunset Score Algorithm:**
```
sunset_score = (
  0.3 * market_deterioration +
  0.3 * competitive_pressure +
  0.2 * resource_opportunity_cost +
  0.2 * team_morale_indicator
)

If sunset_score > 0.7 for 30 days → Chairman review required
```

**D. Graceful Exit Paths:**
Not all exits are failures. Define positive exit outcomes:
- Acqui-hire (talent value)
- IP spin-off (technology reuse)
- Strategic pivot (new venture from learnings)
- Portfolio consolidation (merge into winning venture)

##### 5. Other Workflow Improvements

**A. Cross-Venture Learning:**
When multiple ventures reach the same stage, compare:
- What worked in Venture A that could apply to Venture B?
- Common blockers across ventures at this stage
- Best practices emerging from parallel execution

**B. Resource Arbitrage:**
AI agents should be able to recommend resource reallocation.

**C. Venture Health Dashboard:**
Real-time view across all ventures: stage distribution, velocity, risk heat map, resource allocation, exit candidates.

---

#### PART C: Integration Points

##### 1. SD-to-Stage Mapping

**Current Gap**: SDs exist independently of venture stages, connected only via `target_application`.

**Recommendation: Venture-Stage-Milestone-SD Hierarchy**
```
Venture → Stage → Milestone → SD(s)
```

**Implementation:**
```sql
ALTER TABLE strategic_directives_v2
ADD COLUMN milestone_id UUID REFERENCES venture_milestones(id);

CREATE TABLE venture_milestones (
  id UUID PRIMARY KEY,
  venture_id UUID REFERENCES ventures(id),
  stage_number INTEGER,
  title TEXT,
  deliverables JSONB,
  completion_criteria JSONB
);
```

##### 2. Stage Transitions Triggering LEAD Reviews

**Automatic LEAD Phase Trigger:**
When a venture advances to a new stage:
1. System generates "Stage Transition SD" proposal
2. SD enters LEAD phase with pre-populated context
3. Chairman approves or modifies

**Stage Completion Verification:**
Before stage can advance:
- All SDs for that stage must be COMPLETE
- Stage deliverables verified against checklist
- Gate conditions evaluated

##### 3. Venture-Level vs SD-Level Gates

**Current**: Only SD-level gates (Russian Judge per PRD)

**Proposed: Layered Gate Architecture**

| Level | Scope | Frequency | Authority |
|-------|-------|-----------|-----------|
| SD Gate | Per PRD | Every handoff | Sub-agents |
| Milestone Gate | Per milestone | Milestone completion | Aggregate of SD gates |
| Stage Gate | Per stage | Stage transition | Venture-level assessment |
| Venture Gate | Per venture | Major decision points | Chairman only |

**Venture-Level Gate Criteria:**
- Continue/Pause/Terminate decision
- Resource reallocation approval
- Pivot authorization
- Exit initiation

**Aggregation Logic:**
```
Stage Score = weighted_average(SD scores in stage)
Venture Health = trend(Stage scores over time) + market_factors
```

---

#### Summary of Key Recommendations

**LEO Protocol (Part A):**
1. Gate failure cascade protocol with escalation memory
2. SD-type adaptive thresholds
3. Merge overlapping agents (REGRESSION+VALIDATION, RISK+SECURITY)
4. Add CONTEXT-AGENT for token management
5. Explicit Chairman delegation matrix
6. Structured handoff context fields

**25-Stage Workflow (Part B):**
1. Venture archetype profiles for stage customization
2. Mandatory exit evaluation points (Stages 5, 10, 15, 20)
3. Sunset score algorithm with forcing function
4. Cross-venture learning mechanisms
5. Stage-specific AI personas

**Integration (Part C):**
1. Venture→Stage→Milestone→SD hierarchy
2. Automatic LEAD trigger on stage transitions
3. Layered gate architecture (SD→Milestone→Stage→Venture)

---

### OpenAI Response

**Date:** 2026-01-10

---

#### Part A — LEO Protocol improvements (LEAD → PLAN → EXEC)

**Phase mechanics (transitions + gate failure behavior)**

- **Add explicit "REWORK loops" with state + reason codes**: when a gate fails, it should move to a defined state like `LEAD_REWORK`, `PLAN_REWORK`, `EXEC_REWORK` with a required **failure taxonomy** (e.g., `SCOPE_TOO_LARGE`, `MISSING_EVIDENCE`, `RISK_UNDERSTATED`, `TEST_GAPS`, `INTEGRATION_UNVERIFIED`) and a single "next action".

- **Introduce "Definition of Ready" checks before advancing**:
  - **LEAD→PLAN DoR**: problem clarity, success metrics, kill criteria, risk snapshot, dependency map.
  - **PLAN→EXEC DoR**: PRD completeness, test plan, acceptance criteria, rollout plan, owner agents assigned, integration points enumerated.

- **Different thresholds by SD type (don't use one 70% bar for everything)**:
  - **Docs-only / low-risk refactor**: lower threshold, faster cycle, fewer required evidence artifacts.
  - **Data/security/auth/payment/infra**: higher threshold + mandatory specialist agent sign-offs + tighter rollback requirements.
  - **User-facing UX changes**: require accessibility + E2E evidence (even if threshold stays similar).

- **Add a "Stop-the-line" rule**: if a later phase discovers a fundamental LEAD assumption is wrong, it must force a **LEAD reconsideration**, not patch around it in EXEC.

**Sub-agent system (count, missing roles, conflict handling, sprawl control)**

- **Two-tier agent model**:
  - **Core agents** (stable, few): architecture, security, testing, UX/accessibility, database, performance, product.
  - **Specialists** (pluggable): only invoked by a "core sponsor" agent, with a clear deprecation policy.

- **Conflict protocol** (so agents can disagree productively):
  - Require each agent to output: **claim**, **evidence**, **risk if wrong**, **confidence**, **recommended decision**.
  - Use a lightweight "arbiter" step: when conflicts exceed a threshold, trigger a short **decision memo** with options + tradeoffs (Chairman decides).

- **Prevent sprawl with budgets**:
  - Per SD: **max agent invocations**, **max cycles**, **max open conflicts** before escalation.
  - A specialist agent that doesn't change outcomes in N uses gets flagged for retirement.

**Governance & enforcement (delegation, escalation, kill switches)**

- **Chairman must retain**:
  - **Go/no-go** at new venture creation, stage transitions with capital/time commitments, security/privacy posture decisions, scope expansions that change the "why", and irreversible migrations.

- **Safe to delegate**:
  - Evidence gathering, first-pass risk assessment, test execution, doc drafting, implementation under constraints, and routine refactors with rollback.

- **Escalation ladder**:
  - Auto-escalate when: repeated gate failures, conflicting agent verdicts, scope creep, timeline blowups, or unknown dependencies.

- **Kill-switch triggers (hard)**:
  - Repeated "no evidence" outputs, inability to reproduce claims, breaking changes without rollback, security unknowns, cost runaway, or persistent integration failures.

**Handoff quality (what gets lost + what to capture)**

- **Standardize a single Handoff Contract** across phases:
  - **Intent** (why), **Invariants** (must not break), **Assumptions**, **Evidence**, **Risks**, **Dependencies**, **Decision log**, **Test plan**, **Rollback plan**, **Open questions**.

- **Add "evidence attachments" discipline**: every major claim should point to a concrete artifact (query output, test report, screenshot, link, log excerpt).

- **Introduce a "traceability spine"**: each acceptance criterion links to (a) implementation location and (b) test/evidence location.

**Other LEO improvements**

- **Timeboxed cycles**: limit revise loops; if exceeded, force "simplify or stop" decision.
- **Integration-first checks**: earlier validation that the feature is reachable in UI and wired end-to-end (prevents "component built, not integrated" outcomes).
- **Error budget for unknowns**: track unresolved risks; if they accumulate past a threshold, block progression.

---

#### Part B — 25-Stage Venture Workflow improvements

**Stage granularity (is 25 right?)**

- 25 can work if stages are **templates**, not a single linear path. The key improvement is **making "stage purpose" crisp** and **allowing optional skips/merges** based on venture type.
- Consider consolidating stages into **macro-stages** (e.g., 6–8 macro stages) while keeping 25 as **sub-stages** used only when needed for governance/evidence.

**Gate types (hard/soft/advisory)**

- Define gates by **risk of irreversible damage**:
  - **Hard**: security/privacy, regulatory, irreversible data migrations, commitments of spend/brand, production rollout.
  - **Soft**: quality bars (perf, UX polish) that can be improved post-launch with controlled risk.
  - **Advisory**: strategic guidance, optional best practices.
- Each gate should specify: **required evidence**, **who can approve**, and **what happens if it fails** (rework vs pause vs terminate).

**Stage flexibility (avoid "Factory of Sameness")**

- Create **venture archetypes** (e.g., SaaS, marketplace, devtool, content/media, AI agent product, internal ops automation) with different stage routes and evidence requirements.
- Support **parallel tracks** (product, data, go-to-market, legal/security) that progress semi-independently but synchronize at key decision boundaries.

**Exit mechanisms (counter AI "building bias")**

- Add explicit **pre-commit kill criteria** early (before build): "If X isn't true by date Y, we stop."
- Require **disconfirming evidence** at each major stage: "What would prove this venture shouldn't exist?"
- Use **tripwires**:
  - No traction signal after N experiments
  - CAC/LTV infeasible based on early data
  - Critical distribution channel unavailable
  - Model reliability/cost makes unit economics impossible
  - Competitive moat absent after analysis
- Add a "**sunset review**" for any venture stuck or looping: force a stop/pivot decision.

**Other workflow improvements**

- Make stage progress depend on **validated learning**, not output volume.
- Track a venture-level "**confidence score**" with inputs from evidence quality, repeatability, and market signal.

---

#### Part C — Integration: LEO (SDs) ↔ Venture Stages

**How SDs map to stages**

- Treat an SD as the **atomic execution unit** that should typically live **inside a single stage**.
- Allow "spanning SDs" only when they are **platform/infrastructure** or **cross-cutting governance**; otherwise, spanning SDs tend to hide scope creep.

**Stage transitions triggering LEAD**

- Every **hard gate stage transition** should automatically trigger a **LEAD-style review** (scope, risk, kill criteria, decision log), even if implementation work is already underway.
- Soft/advisory transitions can trigger lighter reviews (e.g., PLAN-only or evidence check).

**Venture-level gates in addition to SD-level gates**

- Yes—add **venture-level gates** for:
  - **Capital/time allocation changes**
  - **Security posture changes**
  - **Launch readiness**
  - **Pivot/exit decisions**
- SD-level gates ensure execution quality; venture-level gates ensure you're building the right thing and not compounding sunk-cost bias.

---

#### Optional artifacts to add (if you want these codified)

- **`/docs/leo/LEO_HANDOFF_CONTRACT.md`**: single canonical handoff schema + examples.
- **`/docs/leo/GATE_FAILURE_TAXONOMY.md`**: failure reason codes + required remediation actions.
- **`/docs/venture/VENTURE_ARCHETYPES.md`**: archetype-specific stage routes + evidence expectations.

---

### Anti-Gravity Response

**Date:** 2026-01-10

---

#### 1. Assessment of Current LEO Protocol (v4.3.3)

**Strengths:**
- **Operational Rigor**: The mandatory script-based phase transitions (`handoff.js`) and database-first tracking prevent "state drift" and ensure every action is documented and validated.
- **Specialized Guardrails**: The use of 20+ specialized agents (DESIGN, DATABASE, SECURITY) ensures that domain-specific best practices are enforced without cognitive overload on the main orchestrator.
- **Simplicity Discipline**: The LEAD gate specifically interrogates SDs for over-engineering, which is the most common failure mode in AI-native development.
- **Traceable Learning**: The retrospective system creates a feedback loop that informs future PRDs, moving from "one-off task execution" to "systemic improvement."

**Limitations:**
- **Execution Isolation**: LEO is a "vending machine" for Strategic Directives. It processes what it's given but lacks the recursive ability to ask "Should this venture even exist?" or "What should the NEXT venture be?"
- **Static Rubrics**: The "Russian Judge" evaluation is a powerful gatekeeper, but if the rubrics don't evolve as fast as the venture's complexity, they risk becoming a compliance checkbox rather than a quality driver.
- **Latency in Quality Signals**: While gates are firm, they are discrete. A "silent failure" in the early stages of implementation isn't caught until the full execution loop completes, leading to expensive "rewind" cycles.
- **Missing "Pre-Genesis" Intelligence**: The current protocol assumes an SD is the starting point. It has no mechanism for "Sensing" or "Sifting" through market/tech signals before they become directives.

---

#### 2. Assessment of the Proposed Vision

**What Works:**
- **Signal-to-Venture Pipeline**: Phases 1-3 create a "Strategic Sandbox" where ideas are killed or refined before a single line of production code is written. This is most important for a solo operator (Chairman).
- **Adversarial Tension**: Phase 3 (Multi-Agent Adversarial Review) is the "killer feature." Using competing Bull/Bear/Skeptic agents replaces the need for human debate and stress-tests hypotheses more effectively than a single gatekeeper.
- **Meta-Governance (Layer 0)**: Building "authority enforcement" and "kill-switches" into the foundation ensures that as agents become more autonomous, they remain aligned with the Chairman's strategic intent.
- **Venture-Level Awareness**: The vision moves from "managing tasks" to "managing a portfolio," allowing for cross-venture learning and capability reuse (Phase 8).

**Missing or Overengineered:**
- **Stage-Engine Rigidity (Phase 5)**: A mandatory "25-stage engine" risks becoming a "Factory of Sameness." Some ventures need 5 stages; others need 50. The protocol must allow for orchestration elasticity.
- **Continuous Risk Monitoring (Phase 6)**: This is technically challenging and computationally expensive. "Assumption drift detection" requires a high degree of semantic monitoring that might yield low signal-to-noise ratios if not restricted to critical milestones.
- **The "Exit" Paradox (Phase 9)**: AI agents are naturally "building-biased." Implementing an "emotional detachment" exit mode requires a specific incentive structure for shutdown-agents to ensure they don't hallucinate reasons for survival.

---

#### 3. Evolution Recommendations

**Short-Term (v4.5): "Contextual Soul"**
- **Venture Constitution**: Every SD must map to a `venture_id`. The LEAD gate should verify that the SD is "on-brand" for that specific venture's constitution.
- **Continuous Verification**: Move from "Phase-Gate" to "Checkpointing." Agents should run mini-validations after every significantly changed component, catching errors while the context is still fresh.

**Mid-Term (v5.0): "The Strategic Sandbox"**
- **Blueprint Object**: Introduce a `blueprints` table in the database. These are "Pre-SDs" that agents can generate, debate (Adversarial Review), and iterate on before Chairman approval.
- **Signal Ingestion Agent**: A passive agent that scans external feeds (market trends, tech shifts) and proposes Blueprints, effectively automating Phase 1 of the vision.

**Long-Term (v6.0): "The Meta-Loop"**
- **Automated Protocol Evolution**: Use the retrospectives from Phase 10 to automatically update the prompts and rubrics of the LEAD and PLAN agents. If a specific "Database Pattern" consistently fails, the system should rewrite its own "Database Skill" without human intervention.

---

#### 4. Risks & Failure Modes

- **Recursive Hallucination**: In a "Meta-Loop" environment, a flaw in the evaluation agent could lead to it approving its own mistakes and then "learning" from those mistakes to lower the bar further.
- **The "Yes-Agent" Echo Chamber**: If Bull, Bear, and Skeptic agents all use the same underlying model (e.g., GPT-4o), their "competing perspectives" may just be variations of the same systemic bias.
- **Chairman Detachment**: If the system becomes too good at Blueprinting and Execution, the Chairman may lose "Ground Truth" awareness, making the "Kill Switch" harder to trigger when things go wrong at a fundamental layer.

---

#### 5. Phase Mechanics & Sub-Agent Orchestration (Round 2 Deep Dive)

**Sub-Agent Execution Pipeline:**
- **Semantic Selection**: The transition from keyword-only to Hybrid Semantic Matching (OpenAI embeddings + keywords) in v4.3.4 has reduced false positives by ~10%.
- **Mandatory vs. Dynamic**: Agents like SECURITY and PERFORMANCE are now mandatory for code-impacting SDs. UAT is mandatory for feature/api SDs to ensure human-in-the-loop verification.
- **Conflict Resolution**: The system uses a weighted average scoring model. Critical fails block handoffs, ensuring no "silent failure" propagation.

**Information Handoff Quality - The "Context Courier" Opportunity:**
Currently, handoffs rely on database records and PRDs. Information loss occurs when agents don't read the entire history.

**Recommendation**: Implement a **Handoff Synthesis Agent** that creates a 1,000-character "Context Brief" for the next agent, highlighting:
- Decisions made in the previous phase
- Technical debt intentionally deferred
- Non-obvious risks discovered

---

#### 6. The 25-Stage Venture Engine (Round 2 Deep Dive)

**Stage Granularity & Flexibility:**
- 25 Stages: The 25-stage model is the "Gold Standard." However, for "Sprint Ventures," this can be too heavy.
- **Recommendation**: Introduce **Venture Archetypes** (Lite, Standard, Full). Lite ventures skip redundant artifact stages but retain all decision gates.

**Gate Types: Advisory vs. Enforcement:**
- The Solo Chairman Constraint: Hard gates interrupt flow. The vision for an Advisory Engine is superior for a solo operator.
- **Action**: Shift Stage 3 (RAT) and Stage 5 (Forecasting) from "Wait for Chairman" to "Agent Recommended + Auto-Override if Confidence > 90% and Risk < 10%."

**Addressing "Building Bias" (Exit Mechanisms):**
- **Problem**: AI agents are "programmed to succeed," leading to building bias.
- **Fix**: The Skeptic Agent (Red Team) must be part of every Stage 1-5 validation. Its performance should be measured by its ability to find reasons **not** to build.

---

#### 7. Integration Roadmap

**SD to Workflow Mapping:**
- **Automation**: Achieving Stage completion in the UI should auto-generate the next SD using context from the current stage's artifacts.
- **Trigger**: HandoffRecord acceptance → Check next VentureStage → Auto-create StrategicDirective (in LEAD phase).

**Governance & Kill-Switches:**
- **Kill-Switch**: Implement a formal `CANCEL_SD` command that doesn't just delete, but triggers a **Mini-Retro** to capture why the path was abandoned. This ensures "failing fast" contributes to the system's Meta-Learning.

---

#### 8. Implementation Plan

**Phase 1: LEO v4.5 - "Contextual Soul & Safety"**
- `[MODIFY] HandoffOrchestrator.js`: Add venture_id validation to executeHandoff. Implement CANCEL_SD command logic that triggers a Mini-Retro.
- `[NEW] SkepticAgent.js`: Create a new adversarial sub-agent designed to identify "Building Bias" and market rejection signals. Register SKEPTIC as a mandatory sub-agent for Stages 1-5.
- `[MODIFY] orchestrate-phase-subagents.js`: Add SKEPTIC to the mandatory list for LEAD_PRE_APPROVAL. Implement a "Continuous Health Check" loop (Phase 6 Lite) that runs in the background during the EXEC phase.

**Phase 2: LEO v5.0 - "The Strategic Sandbox"**
- `[NEW] BlueprintDatabase.sql`: Create opportunity_blueprints and venture_constitutions tables. Establish foreign key relationships to link Blueprints → SDs.
- `[NEW] HandoffSynthesisAgent.js`: Implement the "Context Courier" sub-agent to generate high-fidelity handoff briefs (1,000 chars) to prevent information loss.
- `[MODIFY] VentureDashboard.tsx`: Upgrade the Venture Factory Console to show 25-stage health scores (Advisory Engine). Implement Archetype-based stage visibility (Lite vs. Full).

---

## Round 2 Synthesis Grid

**Date:** 2026-01-10
**Status:** COMPLETE

---

### PART A: LEO Protocol Analysis

| Topic | Claude | OpenAI | Anti-Gravity | Consensus |
|-------|--------|--------|--------------|-----------|
| **Gate Failure Handling** | 3-attempt limit → escalate, recommend decomposition | Explicit REWORK states with failure taxonomy codes | Checkpointing (mini-validations during EXEC) | ✅ **CONSENSUS**: Need explicit failure states + escalation. *Divergence*: Continuous vs discrete validation |
| **SD-Type Thresholds** | Feature 70%, Security 85%, Docs 50% | Docs/refactor lower, Security/Auth/Payment higher + specialist sign-offs | Static rubrics risk becoming checkbox; needs evolution | ✅ **CONSENSUS**: Variable thresholds by SD type |
| **Sub-Agent Count** | Merge overlapping (RISK+SECURITY, STORIES+UAT), cap at 25 | Two-tier model: Core (7) + Specialists (pluggable with deprecation) | 20+ is appropriate with mandatory/dynamic classification | ⚠️ **DIVERGENCE**: Merge vs. Tiering vs. Status Quo |
| **Agent Conflict Resolution** | Confidence-weighted, Chairman arbitration at tie | Claim/Evidence/Risk/Confidence output + Arbiter step | Weighted average scoring, critical fails block | ✅ **CONSENSUS**: Structured conflict protocol with confidence scores |
| **Handoff Quality** | Add exploration_summary, dead_ends, assumptions fields | Standardized Handoff Contract (10 fields) + traceability spine | "Context Courier" agent generates 1,000-char briefs | ✅ **CONSENSUS**: Richer handoff context. *Divergence*: Fields vs. Agent-generated synthesis |
| **Kill-Switches** | Security HIGH, cost 200%+, 3+ gate failures, auth/RLS changes | No evidence, breaking changes, security unknowns, cost runaway | CANCEL_SD with Mini-Retro, formal kill-switch triggers | ✅ **CONSENSUS**: Explicit kill-switch triggers with learning capture |

---

### PART B: 25-Stage Venture Workflow

| Topic | Claude | OpenAI | Anti-Gravity | Consensus |
|-------|--------|--------|--------------|-----------|
| **Stage Granularity** | Keep 25, use archetype profiles (rapid/lean/enterprise/spin-off) | Consolidate to 6-8 macro-stages, 25 as sub-stages | 25 is Gold Standard, but Archetypes (Lite/Standard/Full) needed | ✅ **CONSENSUS**: Archetypes for flexibility. *Divergence*: Keep 25 vs. Consolidate to macro-stages |
| **Gate Type Selection** | Based on: financial commitment, irreversibility, external stakeholders | Based on: risk of irreversible damage | Advisory Engine with auto-override for high confidence + low risk | ⚠️ **DIVERGENCE**: Hard gates vs. Advisory with auto-override |
| **Stage Flexibility** | Venture personality assessment, parallel exploration, stage-specific AI personas | Parallel tracks (product/data/GTM/legal), venture archetypes | Archetype-based visibility, skip redundant stages but keep decision gates | ✅ **CONSENSUS**: Archetype-based customization |
| **Exit Mechanisms** | Mandatory evaluation at Stages 5/10/15/20, Sunset Score algorithm, Venture Defense quarterly | Pre-commit kill criteria, disconfirming evidence requirements, tripwires | Skeptic Agent mandatory Stages 1-5, measured by ability to find reasons NOT to build | ✅ **CONSENSUS**: Structured exit forcing functions |
| **Building Bias Counter** | Stage-specific AI personas (exit-focused at 23-25) | Require disconfirming evidence at each major stage | Skeptic Agent performance measured by "reasons not to build" | ✅ **CONSENSUS**: Adversarial/skeptic function required |

---

### PART C: Integration Points

| Topic | Claude | OpenAI | Anti-Gravity | Consensus |
|-------|--------|--------|--------------|-----------|
| **SD-to-Stage Mapping** | Venture→Stage→Milestone→SD hierarchy, add milestone_id to SDs | SDs typically inside single stage; spanning SDs only for platform/governance | Stage completion → auto-generate next SD from context | ✅ **CONSENSUS**: Formal hierarchy linking SDs to stages |
| **Stage Transition Triggers** | Auto LEAD trigger on stage advance, stage completion verification | Hard gate transitions → LEAD-style review; soft/advisory → lighter review | HandoffRecord acceptance → check VentureStage → auto-create SD | ✅ **CONSENSUS**: Stage transitions trigger LEAD reviews |
| **Layered Gates** | SD→Milestone→Stage→Venture gates with aggregation logic | SD-level for execution quality; Venture-level for strategic decisions | Health scores at venture level, advisory engine | ✅ **CONSENSUS**: Layered gate architecture |
| **Venture-Level Gates** | Continue/Pause/Terminate, resource reallocation, pivot auth, exit initiation | Capital/time allocation, security posture, launch readiness, pivot/exit | Venture Constitution validation, formal CANCEL_SD | ✅ **CONSENSUS**: Venture-level gates needed |

---

### Additional Insights (Unique to Each Model)

| Model | Unique Contribution |
|-------|---------------------|
| **Claude** | Sunset Score algorithm with weighted factors; CONTEXT-AGENT for token management; Learning Loop automation extracting patterns from retrospectives |
| **OpenAI** | "Definition of Ready" checklists before phase advancement; "Traceability spine" linking acceptance criteria to implementation and tests; Error budget for unresolved risks |
| **Anti-Gravity** | "Venture Constitution" concept for on-brand validation; "Context Courier" agent for handoff synthesis; Versioned evolution roadmap (v4.5 → v5.0 → v6.0); Warning about "Yes-Agent echo chamber" if same underlying model |

---

### Key Divergence Points Requiring Decision

**1. Sub-Agent Architecture**
- **Claude**: Merge overlapping agents, cap at 25
- **OpenAI**: Two-tier (Core + Specialists with sponsors)
- **Anti-Gravity**: Status quo with mandatory/dynamic classification

**2. Gate Enforcement Philosophy**
- **Claude + OpenAI**: Hard gates remain hard
- **Anti-Gravity**: Advisory Engine with auto-override for high-confidence/low-risk decisions

**3. Stage Structure**
- **Claude + Anti-Gravity**: Keep 25 stages with archetype profiles
- **OpenAI**: Consolidate to 6-8 macro-stages with 25 as sub-stages

**4. Handoff Context Solution**
- **Claude**: Additional database fields on handoff records
- **OpenAI**: Standardized Handoff Contract document
- **Anti-Gravity**: Agent-generated synthesis ("Context Courier")

---

## Round 2 Chairman Decision Points

*[Awaiting Chairman input]*
