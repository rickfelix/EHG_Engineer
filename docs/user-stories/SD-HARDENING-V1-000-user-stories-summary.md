# User Stories Summary: SD-HARDENING-V1-000

**Strategic Directive**: Hardening V1: Post-Assessment Security & Stability
**Type**: PARENT Orchestrator SD
**Total Stories**: 18 (2-3 per child SD)
**Total Story Points**: 73
**Created**: 2025-12-17
**Generated By**: STORIES Agent v2.0.0

## Executive Summary

This PARENT orchestrator SD coordinates 6 child SDs focused on critical security, stability, and quality improvements following the system assessment. The work addresses RLS vulnerabilities in both repositories, decision data split-brain issues, function naming inconsistencies, N+1 query performance problems, and type safety gaps.

## Child SDs Overview

| Child SD | Priority | Story Count | Story Points | Focus Area |
|----------|----------|-------------|--------------|------------|
| SD-HARDENING-V1-001 | CRITICAL | 3 | 13 | RLS Hardening - ehg repo |
| SD-HARDENING-V1-002 | CRITICAL | 3 | 13 | RLS Hardening - EHG_Engineer repo |
| SD-HARDENING-V1-003 | HIGH | 3 | 13 | Decision Split-Brain Resolution |
| SD-HARDENING-V1-004 | MEDIUM | 3 | 11 | Function Naming Standardization |
| SD-HARDENING-V1-005 | HIGH | 3 | 13 | N+1 Query Elimination |
| SD-HARDENING-V1-006 | MEDIUM | 3 | 10 | Type Safety Improvements |

---

## SD-HARDENING-V1-001: RLS Hardening - ehg repo (CRITICAL)

### Story Points: 13

### US-001-1: Audit all RLS policies in ehg repo (5 points)
**Priority**: Critical | **Type**: Security Audit

**User Story**:
> As a System, I want a complete audit of all Row Level Security (RLS) policies in the ehg repository database, so that I can identify missing policies, overly permissive policies, and security vulnerabilities before they are exploited.

**Key Deliverables**:
- Script: `scripts/security/audit-rls-policies-ehg.js`
- Report: RLS policy coverage by table
- Identified vulnerabilities: Missing policies, public access without auth
- Severity classification: Critical, High, Medium, Low
- Recommended fixes for each finding

**Acceptance Criteria** (5 scenarios):

1. **Policy Coverage Check**
   - **Given**: ehg database with 50+ tables
   - **When**: RLS audit script executes
   - **Then**: Report shows policy count per table, identifies tables with 0 policies (vulnerability), categorizes as CRITICAL if table contains sensitive data (users, companies, ventures, decisions)

2. **Public Access Audit**
   - **Given**: Tables with SELECT policies for `anon` role
   - **When**: Audit checks policy conditions
   - **Then**: Report flags policies with `USING (true)` for anon role as HIGH risk, recommends adding WHERE conditions (e.g., `is_public = true`, `status = 'published'`)

3. **Write Operation Audit**
   - **Given**: Tables with INSERT/UPDATE/DELETE policies
   - **When**: Audit checks write permissions
   - **Then**: Report identifies policies allowing unauthenticated writes (CRITICAL), policies missing user ownership checks (HIGH), policies with broad `WITH CHECK (true)` conditions (MEDIUM)

4. **Service Role Key Usage Detection**
   - **Given**: Application code in `src/`, `pages/api/`
   - **When**: Code scan for SUPABASE_SERVICE_ROLE_KEY usage
   - **Then**: Report lists all service role usages, classifies as CRITICAL if used to bypass RLS in regular operations (vs migration scripts), provides refactoring recommendations

5. **Audit Report Generation**
   - **Given**: Audit findings from steps 1-4
   - **When**: Final report generated
   - **Then**: Markdown report with executive summary (total findings by severity), table-by-table analysis, prioritized remediation roadmap, estimated effort per fix

**Testing**:
- Unit tests: Table enumeration, policy parsing
- Integration test: Full audit against test database
- Path: `tests/security/US-001-1-rls-audit-ehg.spec.ts`

**Implementation Context**:
```javascript
// Example vulnerability detection
const tablesWithoutRLS = await client.query(`
  SELECT t.tablename
  FROM pg_tables t
  LEFT JOIN pg_policies p ON p.tablename = t.tablename
  WHERE t.schemaname = 'public'
  GROUP BY t.tablename
  HAVING COUNT(p.policyname) = 0
  ORDER BY t.tablename
`);

// Classify by sensitivity
const criticalTables = ['users', 'companies', 'ventures', 'venture_decisions'];
const highRiskTables = tablesWithoutRLS.rows
  .filter(t => criticalTables.includes(t.tablename));
```

---

### US-001-2: Implement missing RLS policies for critical tables (5 points)
**Priority**: Critical | **Type**: Security Implementation

**User Story**:
> As a System, I want comprehensive RLS policies on all critical tables (users, companies, ventures, venture_decisions), so that unauthorized users cannot read, modify, or delete data they don't own.

**Key Deliverables**:
- Migration: `database/migrations/YYYYMMDD_rls_hardening_ehg_critical_tables.sql`
- RLS policies: 4-6 policies per critical table (SELECT, INSERT, UPDATE, DELETE for authenticated users)
- Policy patterns: User ownership (`auth.uid() = user_id`), team membership, public read with conditions
- Documentation: RLS policy design patterns and rationale
- Rollback script: Safe policy removal if issues detected

**Acceptance Criteria** (5 scenarios):

1. **User-Owned Resources Pattern**
   - **Given**: `ventures` table with `chairman_id` column
   - **When**: CREATE POLICY for authenticated users
   - **Then**: Policy created with `USING (auth.uid() = chairman_id)` for SELECT/UPDATE/DELETE, `WITH CHECK (auth.uid() = chairman_id)` for INSERT, prevents users from reading/modifying other users' ventures

2. **Public Read, Authenticated Write Pattern**
   - **Given**: `companies` table with public company profiles
   - **When**: CREATE POLICY for public and authenticated users
   - **Then**: SELECT policy for `anon` role with `USING (is_public = true)`, INSERT/UPDATE/DELETE policies for `authenticated` with ownership check, public users can view published companies but not edit

3. **Team Membership Pattern**
   - **Given**: `venture_decisions` table with decisions visible to venture team
   - **When**: CREATE POLICY for team access
   - **Then**: Policy uses JOIN or EXISTS to check team membership (`EXISTS (SELECT 1 FROM venture_team_members WHERE venture_id = venture_decisions.venture_id AND user_id = auth.uid())`), team members can view venture decisions, non-members blocked

4. **Migration Execution Safety**
   - **Given**: Migration script with 20+ CREATE POLICY statements
   - **When**: Migration applied to production database
   - **Then**: Transaction boundary wraps all statements, rollback on any error, verification query confirms policy count matches expected, no data access disruption during deployment

5. **Policy Effectiveness Validation**
   - **Given**: New RLS policies deployed
   - **When**: Test suite executes with different user contexts
   - **Then**: Authenticated user A can only see/edit their own ventures (verified), authenticated user B cannot access user A's data (verified), unauthenticated requests blocked for non-public data (verified), test coverage includes all CRUD operations

**Testing**:
- Unit tests: Policy SQL syntax validation
- Integration tests: Policy enforcement with different user contexts
- E2E tests: Full user workflows (create venture, edit own venture, attempt to edit other's venture)
- Path: `tests/e2e/security/US-001-2-rls-policy-enforcement.spec.ts`

**Architecture References**:
- Similar patterns: `database/migrations/*_rls_policies.sql` (existing RLS migrations)
- RLS patterns documentation: `docs/reference/database-agent-patterns.md` (Section: RLS Policy Handling)
- Supabase RLS guide: https://supabase.com/docs/guides/auth/row-level-security

---

### US-001-3: Remove SERVICE_ROLE_KEY bypasses from application code (3 points)
**Priority**: Critical | **Type**: Security Refactoring

**User Story**:
> As a Security Engineer, I want all SERVICE_ROLE_KEY usages removed from application code and replaced with proper RLS policies, so that we maintain audit trails, follow principle of least privilege, and eliminate security vulnerabilities.

**Key Deliverables**:
- Code refactoring: Replace service role client with ANON_KEY client
- RLS policy adjustments: Ensure policies allow legitimate operations
- Documentation: When to use service role (migrations only) vs ANON_KEY (application)
- Migration script: If data fixes needed to support RLS (e.g., add missing `user_id` columns)
- Verification: No service role key in `src/`, `pages/api/` (allowed in `scripts/migrations/`)

**Acceptance Criteria** (3 scenarios):

1. **Code Scan and Refactor**
   - **Given**: Application code with `SUPABASE_SERVICE_ROLE_KEY` imports
   - **When**: Code scan identifies all usages
   - **Then**: Each usage classified (legitimate migration script vs improper bypass), improper bypasses refactored to use ANON_KEY client, RLS policies adjusted to allow operations, service role only used in `scripts/migrations/` directory

2. **Policy Adjustment for Legitimate Operations**
   - **Given**: Service role previously used for admin operations (e.g., bulk inserts)
   - **When**: Refactoring to ANON_KEY client
   - **Then**: New RLS policy created for admin role (`auth.jwt() ->> 'role' = 'admin'`), admin users can perform bulk operations via RLS policy (not bypass), audit trail maintained (user_id tracked)

3. **Verification and Documentation**
   - **Given**: Refactoring complete
   - **When**: Verification script runs
   - **Then**: Zero service role usages in `src/`, `pages/api/` directories (scan passes), documentation updated with service role usage policy (migrations only), team training document created explaining when/why to use proper RLS

**Testing**:
- Unit tests: Verify ANON_KEY client used in refactored code
- Integration tests: Verify operations still work with RLS enforcement
- Security scan: Automated check for service role in application code
- Path: `tests/security/US-001-3-service-role-removal.spec.ts`

**Edge Cases**:
- Migration scripts: Service role allowed and expected
- Seeding scripts: Service role allowed for initial data setup
- Cron jobs: Evaluate if service account pattern needed (user_id = 'system')

---

## SD-HARDENING-V1-002: RLS Hardening - EHG_Engineer repo (CRITICAL)

### Story Points: 13

### US-002-1: Audit all RLS policies in EHG_Engineer repo (5 points)
**Priority**: Critical | **Type**: Security Audit

**User Story**:
> As a System, I want a complete audit of all Row Level Security (RLS) policies in the EHG_Engineer repository database, so that I can identify missing policies, overly permissive policies, and security vulnerabilities in the LEO protocol infrastructure.

**Key Deliverables**:
- Script: `scripts/security/audit-rls-policies-engineer.js`
- Report: RLS policy coverage for LEO tables (strategic_directives, product_requirements, handoffs, retrospectives)
- Identified vulnerabilities: Tables without RLS, policies allowing cross-user access
- Critical findings: LEO protocol data leakage between projects/users
- Recommended fixes with LEO protocol context

**Acceptance Criteria** (5 scenarios):

1. **LEO Protocol Table Coverage**
   - **Given**: LEO protocol tables (strategic_directives_v2, product_requirements_v2, handoffs, retrospectives, agent_registry)
   - **When**: RLS audit script executes
   - **Then**: Report shows policy count per LEO table, identifies tables with 0 policies (CRITICAL for protocol integrity), flags multi-tenant tables without user filtering (HIGH risk)

2. **Cross-Project Data Leakage Check**
   - **Given**: Multiple SDs/PRDs/retrospectives in database from different users/projects
   - **When**: Audit checks isolation policies
   - **Then**: Report identifies tables where user A could access user B's SDs/PRDs (CRITICAL), checks for `created_by` or `owner_id` filtering in policies, flags any tables with global SELECT access

3. **Agent Registry Security**
   - **Given**: `agent_registry` table with agent activities
   - **When**: Audit checks agent access policies
   - **Then**: Policy ensures agents only see their own context (SD-scoped), no cross-SD agent data leakage, service accounts (LEAD, PLAN, EXEC) properly isolated

4. **Handoff Data Protection**
   - **Given**: `handoffs` table with phase transition data
   - **When**: Audit checks handoff access
   - **Then**: Policies ensure handoffs only visible to SD owner, no cross-user handoff visibility, LEAD approvals not visible to other users' SDs

5. **Audit Report with LEO Context**
   - **Given**: Audit findings across LEO protocol tables
   - **When**: Final report generated
   - **Then**: Report includes LEO protocol impact assessment (how vulnerabilities affect LEAD→PLAN→EXEC flow), prioritized by protocol phase criticality (LEAD approval leakage = CRITICAL, metrics leakage = MEDIUM), remediation roadmap with protocol-aware fixes

**Testing**:
- Unit tests: LEO table enumeration, policy parsing
- Integration test: Multi-user audit scenario
- Path: `tests/security/US-002-1-rls-audit-engineer.spec.ts`

**Implementation Context**:
```javascript
// LEO protocol tables to audit
const leoProtocolTables = [
  'strategic_directives_v2',
  'product_requirements_v2',
  'handoffs',
  'retrospectives',
  'agent_registry',
  'agent_messages',
  'issue_patterns',
  'lessons_learned'
];

// Check for multi-tenant isolation
for (const table of leoProtocolTables) {
  const hasUserFilter = await checkPolicyForColumn(table, ['created_by', 'owner_id', 'user_id']);
  if (!hasUserFilter) {
    findings.push({
      severity: 'CRITICAL',
      table,
      issue: 'No user isolation in RLS policy - cross-user data leakage possible'
    });
  }
}
```

---

### US-002-2: Implement RLS policies for LEO protocol tables (5 points)
**Priority**: Critical | **Type**: Security Implementation

**User Story**:
> As a System, I want comprehensive RLS policies on all LEO protocol tables (strategic_directives, product_requirements, handoffs, retrospectives, agent_registry), so that users can only access their own SDs, PRDs, and agent activities.

**Key Deliverables**:
- Migration: `database/migrations/YYYYMMDD_rls_hardening_engineer_leo_tables.sql`
- RLS policies: Owner-based access for all LEO tables
- Policy pattern: `auth.uid() = created_by` or `auth.uid() = owner_id`
- Multi-tenant isolation: Prevent cross-user SD/PRD visibility
- Phase-aware policies: LEAD agents see pending approvals, PLAN/EXEC see assigned work only

**Acceptance Criteria** (5 scenarios):

1. **Strategic Directives Isolation**
   - **Given**: `strategic_directives_v2` table with multiple users' SDs
   - **When**: CREATE POLICY for authenticated users
   - **Then**: Policy with `USING (auth.uid() = created_by)` for SELECT/UPDATE/DELETE, user A can only see/edit their own SDs, user B's SDs completely hidden from user A, test with 2+ user contexts confirms isolation

2. **Product Requirements Protection**
   - **Given**: `product_requirements_v2` table linked to SDs
   - **When**: CREATE POLICY for PRD access
   - **Then**: Policy joins to `strategic_directives_v2` to check ownership (`EXISTS (SELECT 1 FROM strategic_directives_v2 WHERE id = directive_id AND created_by = auth.uid())`), users can only see PRDs for their own SDs, foreign PRDs blocked

3. **Handoff Data Isolation**
   - **Given**: `handoffs` table with LEAD→PLAN→EXEC transitions
   - **When**: CREATE POLICY for handoff access
   - **Then**: Policy checks SD ownership via foreign key, only SD owner can view handoff records, handoff history scoped to user's SDs, no visibility into other users' approval workflows

4. **Agent Registry Scoping**
   - **Given**: `agent_registry` with LEAD, PLAN, EXEC agent activities
   - **When**: CREATE POLICY for agent data
   - **Then**: Policy scopes agent messages to SD ownership, agents can only access context for user's SDs, no cross-user agent message leakage, service account pattern (user_id = 'system') allowed for global agents

5. **Retrospectives Privacy**
   - **Given**: `retrospectives` table with quality scores and lessons
   - **When**: CREATE POLICY for retrospective access
   - **Then**: Policy ensures users only see their own retrospectives, quality scores private to SD owner, lessons_learned aggregation uses public pattern (if sharing enabled), default is private

**Testing**:
- Unit tests: Policy SQL syntax validation for LEO tables
- Integration tests: Multi-user scenarios (user A creates SD, user B attempts access, verify block)
- E2E tests: Full LEO workflow with RLS enforcement (LEAD→PLAN→EXEC)
- Path: `tests/e2e/security/US-002-2-leo-rls-enforcement.spec.ts`

**Architecture References**:
- LEO protocol tables: `database/schema/007_leo_protocol_schema_fixed.sql`
- Multi-tenant patterns: `docs/reference/database-agent-patterns.md`
- Similar isolation: Supabase multi-tenant RLS guide

---

### US-002-3: Verify no RLS bypasses in LEO scripts (3 points)
**Priority**: Critical | **Type**: Security Verification

**User Story**:
> As a Security Engineer, I want all LEO protocol scripts (handoff.js, add-prd-to-database.js, orchestrate-phase-subagents.js) verified to use proper RLS-enforcing clients, so that LEO operations don't accidentally bypass security policies.

**Key Deliverables**:
- Code audit: Scan all `scripts/` for service role usage
- Client standardization: Ensure scripts use `createDatabaseClient` with user context
- User context passing: Scripts accept `--user-id` flag for RLS enforcement
- Documentation: LEO script security guidelines
- Automated check: Pre-commit hook prevents service role in LEO scripts

**Acceptance Criteria** (3 scenarios):

1. **LEO Script Service Role Audit**
   - **Given**: All scripts in `scripts/` directory (handoff.js, add-prd-to-database.js, etc.)
   - **When**: Code scan for SUPABASE_SERVICE_ROLE_KEY usage
   - **Then**: Report lists any scripts using service role, classifies usage (legitimate migration vs improper bypass), migration scripts allowed (`scripts/migrations/`), LEO workflow scripts flagged if using service role

2. **User Context Enforcement**
   - **Given**: LEO scripts that create SDs, PRDs, handoffs
   - **When**: Scripts refactored to accept `--user-id` parameter
   - **Then**: Scripts use `createDatabaseClient` with user context, RLS policies enforced during script execution, created records have `created_by = user_id`, test script execution with different user IDs confirms isolation

3. **Automated Security Check**
   - **Given**: Pre-commit hook configuration
   - **When**: Developer commits code with service role in `scripts/` (excluding `scripts/migrations/`)
   - **Then**: Commit blocked with error message, developer directed to use `createDatabaseClient` pattern, commit allowed only after refactoring, documentation reference provided

**Testing**:
- Unit tests: Script argument parsing for `--user-id`
- Integration tests: Script execution with RLS enforcement
- Pre-commit test: Verify hook blocks service role in scripts
- Path: `tests/security/US-002-3-leo-script-rls-verification.spec.ts`

**Edge Cases**:
- Admin scripts: May need elevated access, document usage and require explicit `--admin` flag
- Migration scripts: Service role allowed and expected
- Testing scripts: May use service role for test data setup, isolate to `tests/` directory

---

## SD-HARDENING-V1-003: Decision Split-Brain Resolution (HIGH)

### Story Points: 13

### US-003-1: Audit decision data sources and identify inconsistencies (5 points)
**Priority**: High | **Type**: Data Quality Audit

**User Story**:
> As a Chairman, I want a comprehensive audit of all decision data sources (venture_decisions, venture_decision_log, PRD tables, UI state) to identify inconsistencies, so that I can trust that decision data is accurate and there is a single source of truth.

**Key Deliverables**:
- Script: `scripts/data-quality/audit-decision-split-brain.js`
- Audit report: Identified data sources storing decisions
- Inconsistency detection: Decisions in one source but not another, conflicting statuses
- Impact assessment: Which decisions are affected, severity of inconsistencies
- Root cause analysis: Why split-brain occurred (multiple writes, no single source)

**Acceptance Criteria** (5 scenarios):

1. **Data Source Enumeration**
   - **Given**: Database with multiple decision-related tables
   - **When**: Audit script scans schema
   - **Then**: Report lists all tables with decision data (venture_decisions, venture_decision_log, related PRD fields), identifies which tables store decision status, identifies which tables store decision history, creates dependency map

2. **Inconsistency Detection - Missing Decisions**
   - **Given**: Decision in `venture_decisions` but not in `venture_decision_log`
   - **When**: Audit compares tables
   - **Then**: Report flags orphan decisions (in one table but not another), calculates inconsistency rate (X% of decisions missing from log), severity: HIGH if recent decisions affected, provides list of affected decision IDs

3. **Inconsistency Detection - Status Conflicts**
   - **Given**: Decision with status="approved" in `venture_decisions` but status="pending" in UI state/cache
   - **When**: Audit checks status across sources
   - **Then**: Report identifies conflicting statuses for same decision_id, shows status divergence timeline (when did they diverge?), severity: CRITICAL if chairman sees wrong status, lists affected ventures

4. **Root Cause Analysis**
   - **Given**: Inconsistencies identified in steps 2-3
   - **When**: Audit analyzes write patterns
   - **Then**: Report shows which code paths write to each table, identifies race conditions (simultaneous writes), identifies missing triggers (updates not propagated), identifies cache invalidation issues, recommends single source of truth

5. **Impact Assessment**
   - **Given**: All audit findings
   - **When**: Final report generated
   - **Then**: Executive summary with total inconsistencies by severity, business impact (chairman made decisions with wrong data? decisions lost?), affected ventures list, prioritized remediation plan, estimated effort

**Testing**:
- Unit tests: Data source scanning, inconsistency detection logic
- Integration test: Audit against database with known inconsistencies
- Path: `tests/data-quality/US-003-1-decision-audit.spec.ts`

**Implementation Context**:
```javascript
// Example inconsistency detection
const orphanDecisions = await client.query(`
  SELECT vd.id, vd.title, vd.status, vd.created_at
  FROM venture_decisions vd
  LEFT JOIN venture_decision_log vdl ON vd.id = vdl.decision_id
  WHERE vdl.decision_id IS NULL
  ORDER BY vd.created_at DESC
`);

const statusConflicts = await client.query(`
  SELECT vd.id, vd.status as vd_status, vdl.status as vdl_status
  FROM venture_decisions vd
  JOIN venture_decision_log vdl ON vd.id = vdl.decision_id
  WHERE vd.status != vdl.status
`);
```

---

### US-003-2: Implement single source of truth for decision data (5 points)
**Priority**: High | **Type**: Data Architecture Refactoring

**User Story**:
> As a System, I want a single source of truth for decision data (venture_decisions table), so that all decision queries, updates, and displays use the same authoritative data source and split-brain inconsistencies are eliminated.

**Key Deliverables**:
- Database schema: Designate `venture_decisions` as single source of truth
- Triggers: Auto-sync to `venture_decision_log` (history/audit only)
- Code refactoring: All decision writes go to `venture_decisions`, all decision reads from `venture_decisions`
- Migration: Reconcile existing inconsistencies, backfill missing data
- Documentation: Decision data architecture and write/read patterns

**Acceptance Criteria** (5 scenarios):

1. **Single Source Designation**
   - **Given**: Multiple tables currently storing decision data
   - **When**: Architecture refactored
   - **Then**: `venture_decisions` designated as authoritative source (single source of truth), `venture_decision_log` repurposed as audit trail only (writes via trigger, not direct), PRD tables store decision_id foreign key (no duplicate status fields), UI state uses cached `venture_decisions` data (no separate state)

2. **Auto-Sync Trigger Implementation**
   - **Given**: Decision updated in `venture_decisions`
   - **When**: Trigger fires on INSERT/UPDATE/DELETE
   - **Then**: Trigger function appends entry to `venture_decision_log` with action (created/updated/deleted), old_status, new_status, changed_by (auth.uid()), timestamp, no direct writes to `venture_decision_log` allowed (RLS policy blocks), audit trail automatically maintained

3. **Code Refactoring - All Writes**
   - **Given**: Application code with decision updates
   - **When**: Code audit identifies all decision write operations
   - **Then**: All writes refactored to target `venture_decisions` only, no direct writes to `venture_decision_log`, no status stored in PRD tables (use foreign key), cache invalidated on decision update, 100% of writes go through single source

4. **Code Refactoring - All Reads**
   - **Given**: Application code with decision queries
   - **When**: Code audit identifies all decision read operations
   - **Then**: All reads refactored to query `venture_decisions`, UI components use `venture_decisions` data, decision history queried from `venture_decision_log` (audit trail), no reading from PRD tables for decision status, 100% of reads come from single source

5. **Data Reconciliation Migration**
   - **Given**: Existing inconsistencies identified in US-003-1
   - **When**: Migration script executes
   - **Then**: Orphan decisions backfilled to missing tables (if needed), status conflicts resolved (venture_decisions wins as source of truth), `venture_decision_log` rebuilt from `venture_decisions` current state, verification query confirms 0 inconsistencies post-migration, rollback plan documented if issues arise

**Testing**:
- Unit tests: Trigger function correctness
- Integration tests: Write to `venture_decisions`, verify auto-sync to log
- E2E tests: Full decision workflow (create, approve, reject) with verification
- Path: `tests/e2e/decisions/US-003-2-single-source-enforcement.spec.ts`

**Architecture References**:
- Trigger patterns: `database/migrations/*_triggers.sql`
- Single source pattern: Event sourcing with audit log
- Similar refactoring: SD-VISION-V2-009 (unified decision data)

---

### US-003-3: Add data integrity checks and monitoring (3 points)
**Priority**: High | **Type**: Quality Assurance

**User Story**:
> As a System, I want automated data integrity checks and monitoring for decision data, so that any future inconsistencies are detected immediately and can be resolved before impacting users.

**Key Deliverables**:
- Cron job: Daily integrity check for decision data
- Monitoring: Alert if inconsistencies detected
- Validation queries: Check venture_decisions ↔ venture_decision_log sync
- Dashboard: Data quality metrics for decisions
- Alerting: Slack/email notification on integrity violations

**Acceptance Criteria** (3 scenarios):

1. **Daily Integrity Check**
   - **Given**: Cron job scheduled to run at 2 AM daily
   - **When**: Job executes
   - **Then**: Query checks for orphan decisions (in `venture_decisions` but not logged), query checks for status mismatches (shouldn't exist with trigger, but verify), query checks for missing foreign keys (decision_id references), report generated with pass/fail status, failures trigger alert

2. **Real-Time Monitoring**
   - **Given**: Application with decision operations
   - **When**: Decision created/updated
   - **Then**: Post-write validation query confirms sync to `venture_decision_log`, validation query confirms status consistency, if validation fails, error logged and alert sent, user operation still succeeds (validation is non-blocking check)

3. **Data Quality Dashboard**
   - **Given**: Decision data integrity metrics
   - **When**: Chairman views data quality dashboard
   - **Then**: Dashboard shows total decisions, sync rate (% decisions properly logged), consistency score (0 conflicts = 100%), trend chart (integrity over time), alert history (when were inconsistencies detected), link to detailed integrity report

**Testing**:
- Unit tests: Integrity check query logic
- Integration tests: Simulate inconsistency, verify detection
- E2E test: Dashboard displays integrity metrics
- Path: `tests/e2e/monitoring/US-003-3-decision-integrity-monitoring.spec.ts`

**Implementation Context**:
```javascript
// Example integrity check
const integrityCheck = await client.query(`
  -- Check 1: Orphan decisions
  SELECT COUNT(*) as orphan_count
  FROM venture_decisions vd
  LEFT JOIN venture_decision_log vdl ON vd.id = vdl.decision_id
  WHERE vdl.decision_id IS NULL;

  -- Check 2: Log entries without parent
  SELECT COUNT(*) as orphan_log_count
  FROM venture_decision_log vdl
  LEFT JOIN venture_decisions vd ON vdl.decision_id = vd.id
  WHERE vd.id IS NULL;

  -- Check 3: Missing foreign keys
  SELECT COUNT(*) as invalid_fk_count
  FROM venture_decision_log vdl
  WHERE decision_id IS NOT NULL
    AND NOT EXISTS (SELECT 1 FROM venture_decisions WHERE id = vdl.decision_id);
`);

if (integrityCheck.rows[0].orphan_count > 0) {
  sendAlert('CRITICAL: Decision split-brain detected');
}
```

---

## SD-HARDENING-V1-004: Function Naming Standardization (MEDIUM)

### Story Points: 11

### US-004-1: Audit function naming patterns and inconsistencies (3 points)
**Priority**: Medium | **Type**: Code Quality Audit

**User Story**:
> As a Developer, I want an audit of all function naming patterns across the codebase to identify inconsistencies (camelCase vs snake_case, get vs fetch, create vs add), so that I can prioritize which functions need renaming for better code maintainability.

**Key Deliverables**:
- Script: `scripts/code-quality/audit-function-naming.js`
- Report: Function naming patterns by file/module
- Inconsistency detection: Mixed patterns in same file, unclear naming (fetch vs get)
- Standardization recommendations: Preferred patterns per function type
- Refactoring effort estimation: Number of functions, files affected

**Acceptance Criteria** (3 scenarios):

1. **Pattern Detection**
   - **Given**: Codebase with mixed naming patterns
   - **When**: Audit script scans all TypeScript/JavaScript files
   - **Then**: Report shows function count by pattern (camelCase: X, snake_case: Y), detects mixed patterns in same file (inconsistency), categorizes by function type (getters, setters, handlers, utilities), identifies outliers (functions not following any clear pattern)

2. **Inconsistency Prioritization**
   - **Given**: Audit findings showing inconsistencies
   - **When**: Analysis determines impact
   - **Then**: HIGH priority: Public API functions with inconsistent naming (user-facing), MEDIUM priority: Internal utilities with mixed patterns (developer confusion), LOW priority: Private functions with minor variations, estimated refactoring effort per priority level

3. **Standardization Recommendations**
   - **Given**: Audit results analyzed
   - **When**: Recommendations generated
   - **Then**: Recommended standard: camelCase for TypeScript/JavaScript (industry standard), snake_case for database functions (PostgreSQL convention), get prefix for retrievals (non-mutating), create/add prefix for insertions (mutating), handle prefix for event handlers, recommended linter rules to enforce patterns

**Testing**:
- Unit tests: Pattern detection regex
- Integration test: Audit against sample codebase with known patterns
- Path: `tests/code-quality/US-004-1-naming-audit.spec.ts`

---

### US-004-2: Implement naming standards and refactor high-priority functions (5 points)
**Priority**: Medium | **Type**: Code Quality Refactoring

**User Story**:
> As a Developer, I want all high-priority functions (public APIs, service layer) refactored to follow consistent naming standards (camelCase, clear prefixes), so that code is more readable and maintainable.

**Key Deliverables**:
- Naming standards document: `docs/FUNCTION_NAMING_STANDARDS.md`
- Refactoring: High-priority functions renamed (public APIs, services)
- Code changes: Update all call sites for renamed functions
- Type definitions: Update TypeScript interfaces/types
- Migration guide: For external API consumers (if any)

**Acceptance Criteria** (5 scenarios):

1. **Standards Documentation**
   - **Given**: Naming standards created
   - **When**: Developer reads documentation
   - **Then**: Clear rules for camelCase vs snake_case (TypeScript vs SQL), prefix guidelines (get/fetch/create/update/delete/handle), examples for each pattern, anti-patterns to avoid (ambiguous names like `data()`, `process()`)

2. **Public API Refactoring**
   - **Given**: Public API functions with inconsistent naming
   - **When**: Refactoring applied
   - **Then**: All public APIs use camelCase, clear prefixes (getUserById, createVenture, updateDecisionStatus), no abbreviations unless industry standard (id, db, api), TypeScript types updated to match, breaking changes documented

3. **Service Layer Refactoring**
   - **Given**: Service layer functions (evaInsightService, ventureService)
   - **When**: Refactoring applied
   - **Then**: Consistent naming within each service, method names match CRUD operations (create, read, update, delete, list), helper methods use clear names (validateInput, formatOutput), no name collisions within service

4. **Call Site Updates**
   - **Given**: Functions renamed in steps 2-3
   - **When**: Code search for old function names
   - **Then**: All call sites updated to use new names, TypeScript compiler confirms no missing references, tests updated to use new names, 100% of references migrated (verified via grep/search)

5. **Backward Compatibility (If Needed)**
   - **Given**: External API consumers (if any)
   - **When**: Breaking changes introduced
   - **Then**: Deprecated functions created as wrappers (call new functions), deprecation warnings logged, migration guide provided to consumers, deprecation timeline announced (remove in 3 months), minimal disruption to external users

**Testing**:
- Unit tests: Renamed functions work correctly
- Integration tests: Service layer functions with new names
- E2E tests: Full workflows with refactored functions
- Path: `tests/code-quality/US-004-2-naming-refactoring.spec.ts`

**Architecture References**:
- Similar refactoring: Service layer standardization patterns
- TypeScript best practices: Microsoft TypeScript handbook
- Naming conventions: Airbnb JavaScript style guide

---

### US-004-3: Add linter rules to enforce naming standards (3 points)
**Priority**: Medium | **Type**: Code Quality Automation

**User Story**:
> As a Developer, I want ESLint rules that enforce function naming standards, so that future code automatically follows conventions and doesn't reintroduce inconsistencies.

**Key Deliverables**:
- ESLint rules: Custom rules for function naming patterns
- ESLint config: `.eslintrc.js` updated with naming rules
- Pre-commit hook: Automatically runs linter before commit
- Documentation: How to configure linter, how to handle exceptions
- Gradual rollout: Warn mode first, then error mode

**Acceptance Criteria** (3 scenarios):

1. **ESLint Rule Implementation**
   - **Given**: Custom ESLint rule for function naming
   - **When**: Developer writes function with invalid name (e.g., `function Fetch_data()`)
   - **Then**: Linter flags error with message ("Function names must use camelCase"), suggests correct name ("fetchData"), provides auto-fix if possible, rule applies to all function types (declarations, expressions, arrow functions)

2. **Pre-Commit Hook Integration**
   - **Given**: Linter rules configured
   - **When**: Developer commits code with naming violations
   - **Then**: Pre-commit hook runs ESLint, commit blocked if errors found, developer shown error messages and suggested fixes, commit allowed only after fixes applied, hook skippable with `--no-verify` flag (discouraged)

3. **Gradual Enforcement**
   - **Given**: Linter rules enabled
   - **When**: Initial rollout to existing codebase
   - **Then**: Rules set to "warn" mode (doesn't block commits), developers see warnings in IDE and terminal, 2-week grace period to fix warnings, after grace period, rules upgraded to "error" mode (blocks commits), legacy code exempted via `.eslintignore` or inline comments (minimize exceptions)

**Testing**:
- Unit tests: ESLint rule correctness (valid names pass, invalid names fail)
- Integration test: Pre-commit hook blocks violations
- Developer experience: Test in real IDE (VSCode) with ESLint extension
- Path: `tests/code-quality/US-004-3-linter-enforcement.spec.ts`

**Implementation Context**:
```javascript
// Example ESLint rule
{
  "rules": {
    "camelcase": ["error", {
      "properties": "never",
      "ignoreDestructuring": false,
      "allow": ["^UNSAFE_"]  // React lifecycle methods
    }],
    "custom/function-naming": ["error", {
      "prefixes": ["get", "fetch", "create", "update", "delete", "handle"],
      "allowedExceptions": ["setup", "teardown"]  // Test helpers
    }]
  }
}
```

---

## SD-HARDENING-V1-005: N+1 Query Elimination (HIGH)

### Story Points: 13

### US-005-1: Identify N+1 query patterns in codebase (5 points)
**Priority**: High | **Type**: Performance Audit

**User Story**:
> As a System, I want all N+1 query patterns identified in the codebase (loops with queries, missing eager loading), so that I can prioritize which queries to optimize for better API performance.

**Key Deliverables**:
- Script: `scripts/performance/detect-n+1-queries.js`
- Static analysis: Detect loops with database queries inside
- Runtime analysis: Log query counts per request (development mode)
- Performance report: Endpoints with N+1 issues, query counts, impact
- Optimization recommendations: Use JOIN, eager loading, caching

**Acceptance Criteria** (5 scenarios):

1. **Static Code Analysis**
   - **Given**: Codebase with potential N+1 patterns
   - **When**: Static analysis script scans code
   - **Then**: Script detects loops (for, forEach, map) with database queries inside (`supabase.from()`, `client.query()`), flags queries inside `Promise.all()` (may indicate N+1), categorizes by severity (tight loop = HIGH, one-time loop = MEDIUM), generates report with file path, line number, code snippet

2. **Runtime Query Logging**
   - **Given**: Development mode with query logging enabled
   - **When**: API endpoint executes (e.g., GET /api/ventures)
   - **Then**: Middleware logs all database queries per request, calculates total query count per endpoint, flags endpoints with >10 queries as N+1 candidates, logs query execution time (total and per query), report shows endpoints ranked by query count

3. **N+1 Pattern Detection - Specific Example**
   - **Given**: Code fetching ventures then looping to get financials
   - **When**: Analysis detects pattern
   - **Then**: Report shows: "N+1 detected: GET /api/ventures fetches ventures (1 query), then loops to fetch financials (N queries where N = venture count)", calculates impact (100 ventures = 101 queries), severity: HIGH (user-facing endpoint), recommended fix: JOIN or eager loading

4. **Performance Impact Assessment**
   - **Given**: N+1 patterns identified
   - **When**: Impact calculated
   - **Then**: Report shows query count increase per N (1 venture = 2 queries, 100 ventures = 101 queries), estimated response time impact (linear growth with N), user-facing vs internal endpoints (prioritize user-facing), estimated improvement with fix (101 queries → 1 query = 100x reduction)

5. **Prioritized Remediation Plan**
   - **Given**: All N+1 findings
   - **When**: Final report generated
   - **Then**: Findings prioritized by severity (HIGH: user-facing, >50 queries; MEDIUM: internal, >20 queries; LOW: admin, <10 queries), estimated effort per fix (simple JOIN = 1 hour, complex eager loading = 4 hours), recommended fixes with code examples, total estimated effort for all fixes

**Testing**:
- Unit tests: Static analysis pattern detection
- Integration test: Runtime logging captures queries
- Performance test: Measure query count before/after
- Path: `tests/performance/US-005-1-n+1-detection.spec.ts`

**Implementation Context**:
```javascript
// Example N+1 detection
const ventures = await supabase.from('ventures').select('id, name');

// N+1 DETECTED: Loop with query inside
for (const venture of ventures.data) {
  const financials = await supabase
    .from('venture_financials')
    .select('*')
    .eq('venture_id', venture.id);
  venture.financials = financials.data;
}

// RECOMMENDATION: Use JOIN or eager loading
const venturesWithFinancials = await supabase
  .from('ventures')
  .select('*, venture_financials(*)')  // Eager load
  .order('name');
```

---

### US-005-2: Optimize high-priority N+1 queries with JOIN/eager loading (5 points)
**Priority**: High | **Type**: Performance Optimization

**User Story**:
> As a User, I want API endpoints optimized to eliminate N+1 queries (using JOIN, eager loading, or caching), so that page load times are faster and the system is more responsive.

**Key Deliverables**:
- Code refactoring: Replace N+1 loops with JOIN queries
- Eager loading: Use Supabase nested select syntax
- Caching: Add Redis cache for frequently accessed data (if needed)
- Performance tests: Verify query count reduction
- Documentation: Query optimization patterns

**Acceptance Criteria** (5 scenarios):

1. **JOIN Optimization - Ventures + Financials**
   - **Given**: GET /api/ventures endpoint with N+1 (ventures + financials)
   - **When**: Refactored to use JOIN
   - **Then**: Single query with JOIN or nested select (`ventures.select('*, venture_financials(*)')`), query count reduced from N+1 to 1, response time reduced by >50% (measured), test with 100 ventures confirms 1 query (not 101)

2. **Eager Loading - SDs + PRDs**
   - **Given**: GET /api/strategic-directives with N+1 (SDs + PRDs)
   - **When**: Refactored to use eager loading
   - **Then**: Supabase nested select loads PRDs in single query (`strategic_directives.select('*, product_requirements(*)')`), query count reduced from N+1 to 1, PRDs automatically attached to SD objects, test verifies correct data structure

3. **Batch Query Optimization - Agent Messages**
   - **Given**: Agent dashboard loading messages for multiple agents (N+1)
   - **When**: Refactored to batch query
   - **Then**: Single query with WHERE IN clause (`agent_id IN [...]`), messages fetched in one query then grouped by agent_id in code, query count reduced from N to 1, test with 10 agents confirms 1 query (not 10)

4. **Caching Strategy - Frequently Accessed Data**
   - **Given**: Lookup tables queried repeatedly (categories, tags, statuses)
   - **When**: Redis cache implemented
   - **Then**: First request queries database and caches result (TTL: 1 hour), subsequent requests served from cache (0 database queries), cache invalidation on data update, query count reduced to 0 for cached data, test verifies cache hit/miss behavior

5. **Performance Verification**
   - **Given**: All N+1 optimizations applied
   - **When**: Performance tests run
   - **Then**: Query count per endpoint reduced by >90% (101 → 1), response time reduced by >50% (measured with k6 or similar), no correctness regressions (data matches original), test with varying data sizes (10, 100, 1000 records) confirms linear improvement

**Testing**:
- Unit tests: Optimized query logic
- Integration tests: Query count verification (mock Supabase client)
- Performance tests: Load test with k6 or Artillery
- Path: `tests/e2e/performance/US-005-2-n+1-optimization.spec.ts`

**Architecture References**:
- Supabase nested select: https://supabase.com/docs/guides/database/joins-and-nested-tables
- Query optimization patterns: Database query best practices
- Caching strategies: Redis caching guide

---

### US-005-3: Add query performance monitoring (3 points)
**Priority**: High | **Type**: Performance Monitoring

**User Story**:
> As a Developer, I want query performance monitoring that alerts when N+1 patterns reappear or query counts exceed thresholds, so that performance regressions are caught before reaching production.

**Key Deliverables**:
- Middleware: Query count tracking per request
- Alerting: Slack/email alert if query count >threshold
- Dashboard: Query performance metrics (query count, execution time)
- CI/CD integration: Performance test fails if query count regression
- Documentation: How to interpret metrics and respond to alerts

**Acceptance Criteria** (3 scenarios):

1. **Runtime Query Monitoring**
   - **Given**: Middleware tracking database queries
   - **When**: API request executes
   - **Then**: Middleware counts queries per request, logs query count and total execution time, if query count >threshold (e.g., 10), warning logged, if query count >critical threshold (e.g., 50), alert sent to Slack, metrics stored for dashboard

2. **CI/CD Performance Gate**
   - **Given**: Performance tests in CI/CD pipeline
   - **When**: PR submitted with code changes
   - **Then**: Automated test runs key endpoints (e.g., GET /api/ventures), test asserts query count ≤ expected (e.g., 1 query), if query count exceeds threshold, CI fails with error, PR blocked until performance regression fixed, developer notified of specific endpoint and query count

3. **Performance Dashboard**
   - **Given**: Query metrics collected over time
   - **When**: Developer views performance dashboard
   - **Then**: Dashboard shows query count per endpoint (average, p95, p99), shows query execution time trends (chart over 7 days), shows N+1 alerts history (when and which endpoints), highlights endpoints with recent regressions, provides drill-down to individual request traces

**Testing**:
- Unit tests: Middleware query counting logic
- Integration tests: Alert triggered when threshold exceeded
- E2E test: Dashboard displays metrics
- Path: `tests/e2e/monitoring/US-005-3-query-monitoring.spec.ts`

**Implementation Context**:
```javascript
// Example middleware
app.use(async (req, res, next) => {
  const queryCountBefore = getQueryCount();  // Track Supabase queries
  const startTime = Date.now();

  res.on('finish', () => {
    const queryCount = getQueryCount() - queryCountBefore;
    const duration = Date.now() - startTime;

    logMetric({
      endpoint: req.path,
      queryCount,
      duration,
      timestamp: new Date()
    });

    if (queryCount > 10) {
      console.warn(`⚠️ High query count: ${req.path} (${queryCount} queries)`);
    }

    if (queryCount > 50) {
      sendAlert(`CRITICAL: N+1 detected on ${req.path} (${queryCount} queries)`);
    }
  });

  next();
});
```

---

## SD-HARDENING-V1-006: Type Safety Improvements (MEDIUM)

### Story Points: 10

### US-006-1: Audit TypeScript type coverage and identify `any` usages (3 points)
**Priority**: Medium | **Type**: Code Quality Audit

**User Story**:
> As a Developer, I want an audit of TypeScript type coverage to identify all `any` types, missing type definitions, and implicit `any` usages, so that I can improve type safety and catch bugs at compile time.

**Key Deliverables**:
- Script: `scripts/code-quality/audit-typescript-types.js`
- Report: `any` usage count by file/module
- Missing types: Functions without return types, parameters without types
- Implicit `any`: Variables inferred as `any`
- Type coverage percentage: Overall and per module

**Acceptance Criteria** (3 scenarios):

1. **Explicit `any` Detection**
   - **Given**: Codebase with explicit `any` types
   - **When**: Audit script scans TypeScript files
   - **Then**: Report lists all explicit `any` usages (line number, file path), categorizes by context (function parameter, return type, variable), calculates `any` density (usages per 1000 lines), prioritizes files with highest `any` count for refactoring

2. **Implicit `any` Detection**
   - **Given**: TypeScript compiler with strict mode disabled
   - **When**: Audit enables `noImplicitAny` flag temporarily
   - **Then**: Compiler reports all implicit `any` cases, audit captures compiler output, report shows functions without return types, parameters without types, calculates implicit `any` count, recommends enabling `noImplicitAny` in tsconfig.json

3. **Type Coverage Calculation**
   - **Given**: Total lines of TypeScript code
   - **When**: Type coverage calculated
   - **Then**: Report shows overall type coverage (typed lines / total lines), per-module coverage (src/services: 90%, src/pages: 70%), trend analysis (coverage improving or degrading?), target recommendation (aim for >90% coverage)

**Testing**:
- Unit tests: AST parsing for `any` detection
- Integration test: Audit against sample codebase
- Path: `tests/code-quality/US-006-1-type-audit.spec.ts`

---

### US-006-2: Replace `any` with proper types and add missing type definitions (5 points)
**Priority**: Medium | **Type**: Code Quality Refactoring

**User Story**:
> As a Developer, I want all `any` types replaced with proper TypeScript types (interfaces, generics, union types), so that the codebase has strong type safety and catches bugs at compile time instead of runtime.

**Key Deliverables**:
- Type definitions: Create interfaces for complex types
- Refactoring: Replace `any` with specific types
- Generic types: Use `T extends` for flexible typing
- Union types: Use `string | number` instead of `any` where applicable
- Type guards: Add runtime type checking for unknowns

**Acceptance Criteria** (5 scenarios):

1. **Interface Creation for Complex Types**
   - **Given**: Function parameters typed as `any` for complex objects
   - **When**: Refactored to use interfaces
   - **Then**: Interface created with proper fields (e.g., `interface VentureData { id: string; name: string; ... }`), function parameter updated to use interface, TypeScript compiler validates object structure, auto-completion works in IDE

2. **Generic Types for Reusable Functions**
   - **Given**: Utility function with `any` for flexibility
   - **When**: Refactored to use generics
   - **Then**: Generic type parameter added (e.g., `function getData<T>(id: string): Promise<T>`), caller specifies type (e.g., `getData<Venture>('v123')`), return type properly inferred, type safety maintained across function boundary

3. **Union Types for Multiple Possibilities**
   - **Given**: Variable that could be string or number, typed as `any`
   - **When**: Refactored to use union type
   - **Then**: Type changed to `string | number`, TypeScript compiler enforces type checking, code uses type guards to narrow type (`if (typeof value === 'string')`), no runtime errors from type assumptions

4. **`unknown` Type for Truly Unknown Data**
   - **Given**: External API data typed as `any`
   - **When**: Refactored to use `unknown`
   - **Then**: Type changed to `unknown` (safer than `any`), type guards added to validate data before use, Zod or similar library used for runtime validation, TypeScript forces explicit type checking before using data

5. **Type Coverage Improvement**
   - **Given**: Refactoring complete
   - **When**: Type coverage re-calculated
   - **Then**: Type coverage increased by >20% (e.g., 70% → 90%), `any` usage reduced by >80%, TypeScript compiler catches more errors (verified by introducing intentional type errors), developer confidence improved (surveyed)

**Testing**:
- Unit tests: Type guards work correctly
- Compile-time tests: Invalid types rejected by compiler
- Integration tests: Refactored code with new types
- Path: `tests/code-quality/US-006-2-type-refactoring.spec.ts`

**Architecture References**:
- TypeScript generics: TypeScript handbook
- Type guards: TypeScript narrowing guide
- Zod validation: https://zod.dev/

---

### US-006-3: Enable strict TypeScript compiler options (2 points)
**Priority**: Medium | **Type**: Code Quality Configuration

**User Story**:
> As a Developer, I want strict TypeScript compiler options enabled (`noImplicitAny`, `strictNullChecks`, `strictFunctionTypes`), so that future code is forced to have proper types and we don't regress on type safety.

**Key Deliverables**:
- tsconfig.json: Enable strict mode options
- Compiler error fixes: Resolve all errors from strict mode
- CI/CD integration: TypeScript compiler runs in CI, fails on type errors
- Documentation: Strict mode guidelines for developers
- Gradual rollout: Enable options one at a time if needed

**Acceptance Criteria** (2 scenarios):

1. **Strict Mode Enablement**
   - **Given**: tsconfig.json with strict mode disabled
   - **When**: Strict options enabled (`strict: true` or individual flags)
   - **Then**: Compiler errors revealed for all type issues, errors categorized by type (`noImplicitAny`, `strictNullChecks`, etc.), all errors resolved before merging to main, TypeScript compilation succeeds with strict mode, CI/CD enforces strict mode (build fails if disabled)

2. **CI/CD Type Checking**
   - **Given**: TypeScript compiler in CI/CD pipeline
   - **When**: PR submitted
   - **Then**: CI runs `tsc --noEmit` (type check without compilation), PR blocked if type errors detected, developer shown specific errors and line numbers, PR passes only with 0 type errors, prevents type safety regressions

**Testing**:
- Compile-time: All strict mode errors resolved
- CI/CD test: Verify type check runs and fails on errors
- Developer experience: Test in IDE with strict mode
- Path: `tests/code-quality/US-006-3-strict-mode.spec.ts`

**Implementation Context**:
```json
// tsconfig.json
{
  "compilerOptions": {
    "strict": true,  // Enables all strict options
    // Or enable individually:
    "noImplicitAny": true,
    "strictNullChecks": true,
    "strictFunctionTypes": true,
    "strictBindCallApply": true,
    "strictPropertyInitialization": true,
    "noImplicitThis": true,
    "alwaysStrict": true
  }
}
```

---

## INVEST Criteria Validation

All 18 user stories follow INVEST principles:

### Independent
- Each story can be developed independently within its child SD
- Clear boundaries between stories (audit → implement → monitor pattern)
- Child SDs can be executed in parallel (001 and 002 both RLS, 003-006 different domains)
- Stories within child SD have dependencies (audit before implement) but across SDs are independent

### Negotiable
- Story points are estimates (can be adjusted based on complexity)
- Audit reports can focus on subset of findings (negotiate scope)
- Refactoring can be phased (high-priority first, low-priority later)
- Monitoring/alerting thresholds are configurable

### Valuable
- Each story delivers tangible business value:
  - **RLS stories**: Security vulnerability elimination
  - **Decision stories**: Data integrity and trust
  - **Naming stories**: Code maintainability and developer efficiency
  - **N+1 stories**: Performance improvement and user experience
  - **Type safety stories**: Bug prevention and developer confidence

### Estimable
- Clear acceptance criteria (3-5 per story)
- Story points assigned (2-5 range, most 3-5 points)
- Total: 73 points (~4-5 sprints for full hardening)
- Estimates based on similar past work (RLS migrations, query optimization)

### Small
- All stories ≤5 points
- Most stories 3-5 points (audit, implement, monitor pattern)
- Largest stories (5 points) can be split if needed (e.g., US-005-2 split by endpoint)
- Average story size: 4.1 points

### Testable
- Every story has Given-When-Then acceptance criteria
- Test paths defined for all stories (unit, integration, e2e)
- Specific test scenarios (RLS enforcement, query count reduction, type coverage)
- Measurable outcomes (query count ≤1, type coverage >90%, 0 security vulnerabilities)

---

## Implementation Plan

### Phase 1: Security Hardening (CRITICAL - 26 points)
**Goal**: Eliminate RLS vulnerabilities in both repositories

**Parallel Execution**:
1. **SD-HARDENING-V1-001**: RLS Hardening - ehg repo (13 points)
   - US-001-1: Audit RLS policies
   - US-001-2: Implement missing policies
   - US-001-3: Remove service role bypasses

2. **SD-HARDENING-V1-002**: RLS Hardening - EHG_Engineer repo (13 points)
   - US-002-1: Audit RLS policies
   - US-002-2: Implement LEO table policies
   - US-002-3: Verify LEO script RLS compliance

**Deliverable**: Zero RLS vulnerabilities, 100% policy coverage on critical tables, audit trail for all data access

---

### Phase 2: Data Integrity & Performance (HIGH - 26 points)
**Goal**: Fix decision split-brain and eliminate N+1 queries

**Parallel Execution**:
3. **SD-HARDENING-V1-003**: Decision Split-Brain Resolution (13 points)
   - US-003-1: Audit decision data sources
   - US-003-2: Implement single source of truth
   - US-003-3: Add integrity checks

4. **SD-HARDENING-V1-005**: N+1 Query Elimination (13 points)
   - US-005-1: Identify N+1 patterns
   - US-005-2: Optimize with JOIN/eager loading
   - US-005-3: Add query monitoring

**Deliverable**: 0 decision inconsistencies, >90% query count reduction, performance monitoring in place

---

### Phase 3: Code Quality (MEDIUM - 21 points)
**Goal**: Standardize naming and improve type safety

**Parallel Execution**:
5. **SD-HARDENING-V1-004**: Function Naming Standardization (11 points)
   - US-004-1: Audit naming patterns
   - US-004-2: Refactor high-priority functions
   - US-004-3: Add linter rules

6. **SD-HARDENING-V1-006**: Type Safety Improvements (10 points)
   - US-006-1: Audit type coverage
   - US-006-2: Replace `any` with proper types
   - US-006-3: Enable strict TypeScript mode

**Deliverable**: Consistent naming standards, >90% type coverage, strict TypeScript enforced

---

## Success Metrics

### Security Metrics (Phase 1)
- **RLS Coverage**: 100% on critical tables (users, companies, ventures, decisions, SDs, PRDs)
- **Service Role Bypasses**: 0 in application code (allowed only in migrations)
- **Security Vulnerabilities**: 0 critical, 0 high
- **Audit Trail**: 100% of data access logged via RLS policies

### Data Quality Metrics (Phase 2)
- **Decision Consistency**: 0 split-brain inconsistencies
- **Data Integrity**: 100% pass rate on integrity checks
- **Query Performance**: >90% reduction in N+1 query count
- **Response Time**: >50% improvement on high-traffic endpoints

### Code Quality Metrics (Phase 3)
- **Naming Consistency**: >95% functions follow standard
- **Type Coverage**: >90% of codebase properly typed
- **`any` Usage**: <5% of type annotations
- **Strict Mode**: Enabled with 0 compiler errors

---

## Dependencies

### Prerequisites
- **Database Access**: Both ehg and EHG_Engineer databases
- **Migration Scripts**: Working migration execution pipeline
- **Testing Infrastructure**: Unit, integration, E2E test frameworks
- **CI/CD Pipeline**: TypeScript compiler, linter, test execution

### Child SD Dependencies
- All child SDs are independent and can run in parallel
- PARENT SD (SD-HARDENING-V1-000) orchestrates and tracks overall progress
- No sequential dependencies between child SDs (001 doesn't block 002, etc.)

---

## Risk Mitigation

### Risk 1: RLS Policies Break Existing Functionality
**Mitigation**: Thorough testing with multiple user contexts, gradual rollout with monitoring

**Approach**:
1. Test RLS policies in development environment with 3+ test users
2. Verify all CRUD operations work with RLS enforcement
3. Deploy to staging environment first (1 week monitoring)
4. Production deployment with rollback plan ready

### Risk 2: Query Optimization Introduces Bugs
**Mitigation**: Comprehensive test coverage, performance verification

**Approach**:
1. Write tests for existing behavior before refactoring
2. Verify query results match original (data correctness)
3. Performance tests confirm query count reduction
4. Gradual rollout per endpoint (10% → 50% → 100% traffic)

### Risk 3: Type Safety Changes Reveal Hidden Bugs
**Mitigation**: This is actually a benefit - expose bugs now vs runtime

**Approach**:
1. Enable strict mode gradually (one option at a time)
2. Fix all compiler errors before enabling next option
3. Add runtime type guards for external data
4. Increase test coverage where types reveal gaps

---

## Next Steps

1. **PARENT SD Creation** (Now)
   - Create SD-HARDENING-V1-000 in database as PARENT orchestrator
   - Link child SDs (001-006) to parent

2. **Validate Stories** (Now)
   ```bash
   npm run stories:validate SD-HARDENING-V1-000
   ```

3. **Create Child SD PRDs** (PLAN Phase)
   - Each child SD gets a PRD (6 PRDs total)
   - PRDs reference user stories from this summary

4. **Begin Implementation** (EXEC Phase)
   - **Phase 1**: Start with security (001, 002) - CRITICAL priority
   - **Phase 2**: Data integrity and performance (003, 005) - HIGH priority
   - **Phase 3**: Code quality (004, 006) - MEDIUM priority

5. **Orchestration Monitoring**
   ```bash
   npm run sd:status SD-HARDENING-V1-000  # Track parent progress
   npm run sd:children SD-HARDENING-V1-000  # Track child progress
   ```

---

## Version History

- **v1.0** (2025-12-17): Initial user stories generated by STORIES Agent v2.0.0
  - 18 user stories across 6 child SDs
  - 73 story points total
  - INVEST criteria validated
  - 72 acceptance criteria scenarios (3-5 per story)
  - 3-phase implementation plan
  - PARENT orchestrator structure

---

**Generated by**: STORIES Agent v2.0.0 (Lessons Learned Edition)
**Model**: Claude Sonnet 4.5 (claude-sonnet-4-5-20250929)
**SD**: SD-HARDENING-V1-000 (Hardening V1: Post-Assessment Security & Stability)
**Type**: PARENT Orchestrator SD with 6 child SDs
**Status**: Ready for PLAN phase
