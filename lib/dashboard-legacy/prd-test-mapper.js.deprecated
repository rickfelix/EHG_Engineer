#!/usr/bin/env node

/**
 * LEO Protocol v4.2 - PRD Test Result Mapper
 * Maps Playwright test execution results back to PRD requirements
 * Enables complete traceability from requirement to verification
 */

import { createClient } from '@supabase/supabase-js';
import fsModule from 'fs';
const fs = fsModule.promises;
import path from 'path';
import dotenv from "dotenv";
dotenv.config();

const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL || process.env.SUPABASE_URL;
const supabaseKey = process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY || process.env.SUPABASE_ANON_KEY;

class PRDTestMapper {
  constructor(config = {}) {
    this.config = {
      resultsDir: config.resultsDir || 'test-results',
      reportDir: config.reportDir || 'test-results/reports',
      ...config
    };
    
    this.supabase = createClient(supabaseUrl, supabaseKey);
  }

  /**
   * Map test results to PRD requirements
   */
  async mapTestResults(testRunId, prdId) {
    console.log(`🗺️ Mapping test results for PRD: ${prdId}`);
    
    try {
      // 1. Load test results
      const testResults = await this.loadTestResults(testRunId);
      
      // 2. Fetch PRD requirements
      const requirements = await this.fetchPRDRequirements(prdId);
      
      // 3. Fetch test scenarios
      const scenarios = await this.fetchTestScenarios(prdId);
      
      // 4. Map results to requirements
      const mappedResults = await this.performMapping(testResults, requirements, scenarios);
      
      // 5. Update database with mappings
      await this.updateMappings(mappedResults);
      
      // 6. Calculate coverage metrics
      const coverage = await this.calculateCoverage(prdId, mappedResults);
      
      // 7. Generate verification report
      const report = await this.generateVerificationReport(prdId, mappedResults, coverage);
      
      // 8. Update PRD test status
      await this.updatePRDTestStatus(prdId, coverage);
      
      return {
        success: true,
        prdId,
        testRunId,
        mappedResults: mappedResults.length,
        coverage,
        reportPath: report
      };
      
    } catch (error) {
      console.error('❌ Test mapping failed:', error);
      throw error;
    }
  }

  /**
   * Load test results from Playwright output
   */
  async loadTestResults(testRunId) {
    const resultsPath = path.join(this.config.resultsDir, `results-${testRunId}.json`);
    
    try {
      const content = await fs.readFile(resultsPath, 'utf-8');
      const results = JSON.parse(content);
      
      // Extract individual test results
      const testResults = [];
      
      for (const suite of (results.suites || [])) {
        for (const spec of (suite.specs || [])) {
          for (const test of (spec.tests || [])) {
            testResults.push({
              testId: test.testId,
              title: test.title,
              status: test.status, // passed, failed, skipped
              duration: test.duration,
              error: test.error,
              retry: test.retry || 0,
              annotations: test.annotations || [],
              attachments: test.attachments || [],
              steps: test.steps || []
            });
          }
        }
      }
      
      return testResults;
      
    } catch (error) {
      console.error('Error loading test results:', error);
      
      // Try alternative format (from test-results.json)
      const altPath = path.join(this.config.resultsDir, 'results.json');
      const altContent = await fs.readFile(altPath, 'utf-8');
      return JSON.parse(altContent);
    }
  }

  /**
   * Fetch PRD requirements from database
   */
  async fetchPRDRequirements(prdId) {
    const { data, error } = await this.supabase
      .from('product_requirements_v2')
      .select('functional_requirements, non_functional_requirements, acceptance_criteria')
      .eq('id', prdId)
      .single();
    
    if (error) {
      console.error('Error fetching PRD:', error);
      return [];
    }
    
    return data.functional_requirements || [];
  }

  /**
   * Fetch test scenarios for PRD
   */
  async fetchTestScenarios(prdId) {
    const { data, error } = await this.supabase
      .from('prd_playwright_scenarios')
      .select('*')
      .eq('prd_id', prdId);
    
    if (error) {
      console.error('Error fetching scenarios:', error);
      return [];
    }
    
    return data || [];
  }

  /**
   * Perform the actual mapping of results to requirements
   */
  async performMapping(testResults, requirements, scenarios) {
    const mappedResults = [];
    
    for (const result of testResults) {
      // Try to match test to scenario
      const scenario = this.matchTestToScenario(result, scenarios);
      
      if (scenario) {
        // Find the corresponding requirement
        const requirement = requirements.find(req => req.id === scenario.requirement_id);
        
        mappedResults.push({
          prd_id: scenario.prd_id,
          requirement_id: scenario.requirement_id,
          scenario_id: scenario.scenario_id,
          test_title: result.title,
          test_status: result.status,
          test_duration_ms: result.duration,
          test_error: result.error,
          test_retry_count: result.retry,
          requirement_name: requirement?.name,
          requirement_description: requirement?.description,
          scenario_name: scenario.scenario_name,
          scenario_priority: scenario.priority,
          attachments: result.attachments,
          timestamp: new Date().toISOString()
        });
      } else {
        // Unmapped test - try fuzzy matching
        const fuzzyMatch = this.fuzzyMatchRequirement(result.title, requirements);
        
        if (fuzzyMatch) {
          mappedResults.push({
            prd_id: scenarios[0]?.prd_id,
            requirement_id: fuzzyMatch.id,
            scenario_id: null,
            test_title: result.title,
            test_status: result.status,
            test_duration_ms: result.duration,
            test_error: result.error,
            requirement_name: fuzzyMatch.name,
            fuzzy_matched: true,
            timestamp: new Date().toISOString()
          });
        }
      }
    }
    
    return mappedResults;
  }

  /**
   * Match test result to scenario
   */
  matchTestToScenario(testResult, scenarios) {
    // First try exact match by scenario name
    let scenario = scenarios.find(s => 
      testResult.title.includes(s.scenario_name) ||
      testResult.title.includes(s.scenario_id)
    );
    
    if (scenario) return scenario;
    
    // Try matching by requirement ID in test title
    const reqIdMatch = testResult.title.match(/([A-Z]+-\d+)/);
    if (reqIdMatch) {
      scenario = scenarios.find(s => s.requirement_id === reqIdMatch[1]);
    }
    
    return scenario;
  }

  /**
   * Fuzzy match test to requirement
   */
  fuzzyMatchRequirement(testTitle, requirements) {
    const testWords = testTitle.toLowerCase().split(/\s+/);
    let bestMatch = null;
    let bestScore = 0;
    
    for (const req of requirements) {
      const reqWords = (req.name + ' ' + req.description).toLowerCase().split(/\s+/);
      const commonWords = testWords.filter(word => reqWords.includes(word));
      const score = commonWords.length / Math.max(testWords.length, reqWords.length);
      
      if (score > bestScore && score > 0.3) { // 30% threshold
        bestScore = score;
        bestMatch = req;
      }
    }
    
    return bestMatch;
  }

  /**
   * Update database with test mappings
   */
  async updateMappings(mappedResults) {
    for (const mapping of mappedResults) {
      if (mapping.scenario_id) {
        // Update scenario with test result
        await this.supabase
          .from('prd_playwright_scenarios')
          .update({
            last_executed: new Date().toISOString(),
            last_result: mapping.test_status,
            execution_count: this.supabase.raw('execution_count + 1'),
            average_duration_ms: mapping.test_duration_ms
          })
          .eq('scenario_id', mapping.scenario_id);
        
        // Update verification mapping
        await this.supabase
          .from('prd_test_verification_mapping')
          .upsert({
            prd_id: mapping.prd_id,
            requirement_id: mapping.requirement_id,
            scenario_id: mapping.scenario_id,
            verification_status: mapping.test_status === 'passed' ? 'passed' : 'failed',
            last_run_date: new Date().toISOString(),
            last_run_status: mapping.test_status,
            last_run_duration_ms: mapping.test_duration_ms,
            failure_reason: mapping.test_error,
            screenshot_paths: mapping.attachments
              ?.filter(a => a.name?.includes('screenshot'))
              ?.map(a => a.path) || []
          }, {
            onConflict: 'prd_id,requirement_id,scenario_id'
          });
      }
    }
    
    // Store complete mapping results
    const { error } = await this.supabase
      .from('test_failures')
      .insert(
        mappedResults
          .filter(m => m.test_status === 'failed')
          .map(m => ({
            test_id: `${m.scenario_id || m.requirement_id}-${Date.now()}`,
            test_run_id: new Date().toISOString(),
            target_component: m.requirement_name,
            error_type: 'TEST_FAILURE',
            error_message: m.test_error || 'Test failed',
            prd_id: m.prd_id,
            severity: m.scenario_priority === 'critical' ? 'HIGH' : 'MEDIUM'
          }))
      );
    
    if (error) {
      console.error('Error storing test failures:', error);
    }
  }

  /**
   * Calculate test coverage metrics
   */
  async calculateCoverage(prdId, mappedResults) {
    // Fetch all requirements
    const { data: prd } = await this.supabase
      .from('product_requirements_v2')
      .select('functional_requirements')
      .eq('id', prdId)
      .single();
    
    const allRequirements = prd?.functional_requirements || [];
    const totalRequirements = allRequirements.length;
    
    // Calculate coverage
    const testedRequirements = new Set(mappedResults.map(m => m.requirement_id));
    const passedRequirements = new Set(
      mappedResults
        .filter(m => m.test_status === 'passed')
        .map(m => m.requirement_id)
    );
    
    const coverage = {
      totalRequirements,
      testedRequirements: testedRequirements.size,
      passedRequirements: passedRequirements.size,
      failedRequirements: testedRequirements.size - passedRequirements.size,
      untestedRequirements: totalRequirements - testedRequirements.size,
      coveragePercent: totalRequirements > 0 
        ? Math.round((testedRequirements.size / totalRequirements) * 100)
        : 0,
      passRate: testedRequirements.size > 0
        ? Math.round((passedRequirements.size / testedRequirements.size) * 100)
        : 0,
      requirementDetails: allRequirements.map(req => ({
        id: req.id,
        name: req.name,
        tested: testedRequirements.has(req.id),
        passed: passedRequirements.has(req.id),
        testCount: mappedResults.filter(m => m.requirement_id === req.id).length
      }))
    };
    
    // Store coverage in database
    await this.supabase
      .from('ui_validation_results')
      .insert({
        prd_id: prdId,
        test_run_id: `coverage-${Date.now()}`,
        test_type: 'prd_validation',
        total_tests: mappedResults.length,
        passed_tests: mappedResults.filter(m => m.test_status === 'passed').length,
        failed_tests: mappedResults.filter(m => m.test_status === 'failed').length,
        success_rate: coverage.passRate,
        validation_status: coverage.coveragePercent >= 80 ? 'passed' : 'partial',
        gaps_detected: coverage.requirementDetails
          .filter(r => !r.tested)
          .map(r => ({ requirement: r.id, name: r.name }))
      });
    
    return coverage;
  }

  /**
   * Generate verification report
   */
  async generateVerificationReport(prdId, mappedResults, coverage) {
    const reportPath = path.join(this.config.reportDir, `verification-${prdId}-${Date.now()}.md`);
    
    const report = `# PRD Test Verification Report

## PRD ID: ${prdId}
## Generated: ${new Date().toISOString()}

## Coverage Summary

- **Total Requirements**: ${coverage.totalRequirements}
- **Tested Requirements**: ${coverage.testedRequirements} (${coverage.coveragePercent}%)
- **Passed Requirements**: ${coverage.passedRequirements}
- **Failed Requirements**: ${coverage.failedRequirements}
- **Untested Requirements**: ${coverage.untestedRequirements}
- **Overall Pass Rate**: ${coverage.passRate}%

## Test Execution Summary

Total Tests Executed: ${mappedResults.length}

### By Status
- ✅ Passed: ${mappedResults.filter(m => m.test_status === 'passed').length}
- ❌ Failed: ${mappedResults.filter(m => m.test_status === 'failed').length}
- ⏭️ Skipped: ${mappedResults.filter(m => m.test_status === 'skipped').length}

### By Priority
- 🔴 Critical: ${mappedResults.filter(m => m.scenario_priority === 'critical').length}
- 🟠 High: ${mappedResults.filter(m => m.scenario_priority === 'high').length}
- 🟡 Medium: ${mappedResults.filter(m => m.scenario_priority === 'medium').length}
- 🟢 Low: ${mappedResults.filter(m => m.scenario_priority === 'low').length}

## Requirement Coverage Details

| Requirement ID | Name | Status | Tests | Pass Rate |
|----------------|------|--------|-------|-----------|
${coverage.requirementDetails.map(req => {
  const reqTests = mappedResults.filter(m => m.requirement_id === req.id);
  const passed = reqTests.filter(t => t.test_status === 'passed').length;
  const total = reqTests.length;
  const passRate = total > 0 ? Math.round((passed / total) * 100) : 0;
  
  return `| ${req.id} | ${req.name} | ${req.tested ? (req.passed ? '✅ Passed' : '❌ Failed') : '⚠️ Not Tested'} | ${total} | ${passRate}% |`;
}).join('\n')}

## Failed Tests

${mappedResults.filter(m => m.test_status === 'failed').map(m => `
### ${m.test_title}
- **Requirement**: ${m.requirement_id} - ${m.requirement_name}
- **Scenario**: ${m.scenario_name || 'N/A'}
- **Error**: ${m.test_error || 'Unknown error'}
- **Duration**: ${m.test_duration_ms}ms
`).join('\n')}

## Untested Requirements

${coverage.requirementDetails.filter(r => !r.tested).map(r => `
- **${r.id}**: ${r.name}
`).join('')}

## Recommendations

${this.generateRecommendations(coverage, mappedResults)}

## Artifacts

- Test Results: \`test-results/\`
- Screenshots: \`test-results/screenshots/\`
- Videos: \`test-results/videos/\`
- Traces: \`test-results/traces/\`

## Next Steps

1. Review failed tests and fix identified issues
2. Create tests for untested requirements
3. Re-run test suite after fixes
4. Achieve minimum 80% coverage for production readiness

---
*Generated by PRD Test Mapper v1.0*
*LEO Protocol v4.2*
`;
    
    await fs.mkdir(path.dirname(reportPath), { recursive: true });
    await fs.writeFile(reportPath, report);
    
    console.log(`📊 Verification report generated: ${reportPath}`);
    
    return reportPath;
  }

  /**
   * Generate recommendations based on coverage
   */
  generateRecommendations(coverage, mappedResults) {
    const recommendations = [];
    
    if (coverage.coveragePercent < 80) {
      recommendations.push(`⚠️ **Low Coverage**: Current coverage is ${coverage.coveragePercent}%. Target is 80%. Create tests for ${coverage.untestedRequirements} untested requirements.`);
    }
    
    if (coverage.passRate < 90) {
      recommendations.push(`⚠️ **Low Pass Rate**: Current pass rate is ${coverage.passRate}%. Investigate and fix failing tests.`);
    }
    
    const criticalFailures = mappedResults.filter(m => 
      m.test_status === 'failed' && m.scenario_priority === 'critical'
    );
    
    if (criticalFailures.length > 0) {
      recommendations.push(`🔴 **Critical Failures**: ${criticalFailures.length} critical tests are failing. These must be fixed immediately.`);
    }
    
    const fuzzyMatched = mappedResults.filter(m => m.fuzzy_matched);
    if (fuzzyMatched.length > 0) {
      recommendations.push(`📝 **Test Naming**: ${fuzzyMatched.length} tests were fuzzy matched. Consider updating test names to match scenario IDs.`);
    }
    
    if (recommendations.length === 0) {
      recommendations.push('✅ **Excellent Coverage**: All requirements are tested with high pass rate. Continue monitoring for regressions.');
    }
    
    return recommendations.join('\n\n');
  }

  /**
   * Update PRD test status
   */
  async updatePRDTestStatus(prdId, coverage) {
    const status = coverage.coveragePercent >= 80 && coverage.passRate >= 90
      ? 'testing_complete'
      : coverage.coveragePercent >= 60
      ? 'testing_partial'
      : 'testing_required';
    
    const { error } = await this.supabase
      .from('product_requirements_v2')
      .update({
        validation_checklist: {
          test_coverage: coverage.coveragePercent,
          test_pass_rate: coverage.passRate,
          last_test_run: new Date().toISOString(),
          test_status: status
        }
      })
      .eq('id', prdId);
    
    if (error) {
      console.error('Error updating PRD test status:', error);
    }
  }

  /**
   * Real-time test monitoring
   */
  async monitorTestExecution(prdId) {
    console.log(`👁️ Monitoring test execution for PRD: ${prdId}`);
    
    // Subscribe to test result updates
    const subscription = this.supabase
      .channel(`test-monitoring-${prdId}`)
      .on('postgres_changes', {
        event: '*',
        schema: 'public',
        table: 'prd_playwright_scenarios',
        filter: `prd_id=eq.${prdId}`
      }, payload => {
        console.log(`📊 Test update:`, payload);
        this.handleTestUpdate(payload);
      })
      .subscribe();
    
    return subscription;
  }

  /**
   * Handle real-time test updates
   */
  async handleTestUpdate(payload) {
    const { new: scenario } = payload;
    
    if (scenario.last_result === 'failed') {
      console.log(`❌ Test failed: ${scenario.scenario_name}`);
      
      // Trigger debugging sub-agent if available
      await this.triggerDebuggingAgent(scenario);
    } else if (scenario.last_result === 'passed') {
      console.log(`✅ Test passed: ${scenario.scenario_name}`);
    }
  }

  /**
   * Trigger debugging agent for failed tests
   */
  async triggerDebuggingAgent(scenario) {
    // Check if debugging sub-agent is available
    const { data: debugAgent } = await this.supabase
      .from('leo_sub_agents')
      .select('*')
      .eq('code', 'DEBUGGING')
      .single();
    
    if (debugAgent) {
      console.log(`🔍 Triggering Debugging Sub-Agent for failed test`);
      
      // Create handoff to debugging agent
      await this.supabase
        .from('fix_handoffs')
        .insert({
          handoff_id: `HANDOFF-${Date.now()}`,
          from_agent: 'Testing Sub-Agent',
          to_agent: 'Debugging Sub-Agent',
          test_run_id: scenario.last_executed,
          total_fixes_required: 1,
          overall_priority: scenario.priority,
          status: 'pending'
        });
    }
  }

  /**
   * Generate test execution command
   */
  generateTestCommand(prdId, requirements = []) {
    const commands = [];
    
    // Base command
    commands.push('npm run test:e2e');
    
    // Add specific test files if requirements specified
    if (requirements.length > 0) {
      const testFiles = requirements.map(req => `${req.toLowerCase()}.spec.js`);
      commands.push(`-- ${testFiles.join(' ')}`);
    } else {
      // Run all tests for PRD
      commands.push(`-- --grep "${prdId}"`);
    }
    
    // Add reporting
    commands.push('--reporter=json');
    commands.push(`--reporter-options=outputFile=test-results/results-${prdId}.json`);
    
    return commands.join(' ');
  }
}

// CLI execution
if (import.meta.url === `file://${process.argv[1]}`) {
  const args = process.argv.slice(2);
  const command = args[0];
  const prdId = args[1];
  const testRunId = args[2] || Date.now().toString();
  
  if (!command || !prdId) {
    console.error('Usage: node prd-test-mapper.js <command> <PRD_ID> [TEST_RUN_ID]');
    console.error('Commands: map, monitor, generate-command');
    process.exit(1);
  }
  
  const mapper = new PRDTestMapper();
  
  switch (command) {
    case 'map':
      mapper.mapTestResults(testRunId, prdId)
        .then(result => {
          console.log('\n✅ Test mapping completed!');
          console.log(`📊 Coverage: ${result.coverage.coveragePercent}%`);
          console.log(`✅ Pass Rate: ${result.coverage.passRate}%`);
          console.log(`📄 Report: ${result.reportPath}`);
        })
        .catch(error => {
          console.error('❌ Mapping failed:', error);
          process.exit(1);
        });
      break;
    
    case 'monitor':
      mapper.monitorTestExecution(prdId)
        .then(() => {
          console.log('👁️ Monitoring started. Press Ctrl+C to stop.');
        });
      break;
    
    case 'generate-command':
      const requirements = args.slice(3);
      const command = mapper.generateTestCommand(prdId, requirements);
      console.log('📝 Test command:');
      console.log(command);
      break;
    
    default:
      console.error(`Unknown command: ${command}`);
      process.exit(1);
  }
}

export default PRDTestMapper;